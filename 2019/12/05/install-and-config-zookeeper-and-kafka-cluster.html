<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">
  <link rel="manifest" href="/images/site.webmanifest">
  <meta name="msapplication-config" content="/images/browserconfig.xml">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|consolas:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"mac"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":true,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="此Kafka集群为了日志收集这一套，之前安装了Elasticsearch Cluster：参考：安装Elasticsearch 7.4集群（开启集群Auth + Transport SSL）以及 Kibana &amp;amp; Keystore。FELK： F    Filebeat | Fluentd | Fluent-bit E    Elasticsearch L    Logstash K">
<meta name="keywords" content="Knner Wang">
<meta property="og:type" content="article">
<meta property="og:title" content="安装配置Zookeeper和Kafka集群">
<meta property="og:url" content="https:&#x2F;&#x2F;knner.wang&#x2F;2019&#x2F;12&#x2F;05&#x2F;install-and-config-zookeeper-and-kafka-cluster.html">
<meta property="og:site_name" content="Knner.Wang&#39;s Blog">
<meta property="og:description" content="此Kafka集群为了日志收集这一套，之前安装了Elasticsearch Cluster：参考：安装Elasticsearch 7.4集群（开启集群Auth + Transport SSL）以及 Kibana &amp;amp; Keystore。FELK： F    Filebeat | Fluentd | Fluent-bit E    Elasticsearch L    Logstash K">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https:&#x2F;&#x2F;imgs.knner.wang&#x2F;images&#x2F;logos&#x2F;kafka.png">
<meta property="og:image" content="https:&#x2F;&#x2F;imgs.knner.wang&#x2F;images&#x2F;logos&#x2F;zookeeper.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;imgs.knner.wang&#x2F;images&#x2F;install-and-config-zookeeper-and-kafka-cluster&#x2F;kafka-apis.png">
<meta property="og:image" content="https:&#x2F;&#x2F;imgs.knner.wang&#x2F;images&#x2F;install-and-config-zookeeper-and-kafka-cluster&#x2F;log_anatomy.png">
<meta property="og:image" content="https:&#x2F;&#x2F;imgs.knner.wang&#x2F;images&#x2F;install-and-config-zookeeper-and-kafka-cluster&#x2F;log_consumer.png">
<meta property="og:image" content="https:&#x2F;&#x2F;imgs.knner.wang&#x2F;images&#x2F;install-and-config-zookeeper-and-kafka-cluster&#x2F;consumer-groups.png">
<meta property="og:image" content="https:&#x2F;&#x2F;imgs.knner.wang&#x2F;images&#x2F;knner&#x2F;wx.jpg">
<meta property="og:updated_time" content="2019-12-05T09:02:09.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;imgs.knner.wang&#x2F;images&#x2F;logos&#x2F;kafka.png">

<link rel="canonical" href="https://knner.wang/2019/12/05/install-and-config-zookeeper-and-kafka-cluster.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>安装配置Zookeeper和Kafka集群 | Knner.Wang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Knner.Wang's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">20</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">25</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://knner.wang/2019/12/05/install-and-config-zookeeper-and-kafka-cluster.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Knner.Wang">
      <meta itemprop="description" content="Knner.Wang's Blog,for sharing IT technology and also my study notes.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Knner.Wang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          安装配置Zookeeper和Kafka集群
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-12-05 17:02:09" itemprop="dateCreated datePublished" datetime="2019-12-05T17:02:09+08:00">2019-12-05</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2019/12/05/install-and-config-zookeeper-and-kafka-cluster.html#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/05/install-and-config-zookeeper-and-kafka-cluster.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>此Kafka集群为了日志收集这一套，之前安装了Elasticsearch Cluster：参考：<a href="/2019/11/26/install-elasticsearch-cluster-7-4.html" title="安装Elasticsearch 7.4集群（开启集群Auth + Transport SSL）以及 Kibana &amp; Keystore">安装Elasticsearch 7.4集群（开启集群Auth + Transport SSL）以及 Kibana &amp; Keystore</a>。</p><p>FELK：</p><ul>
<li>F    Filebeat | Fluentd | Fluent-bit</li>
<li>E    Elasticsearch</li>
<li>L    Logstash</li>
<li>K    Kibana</li>
</ul><p><img src="https://imgs.knner.wang/images/logos/kafka.png" alt></p><p>FELK兼容性：</p><ul>
<li>Filebeat 6：兼容kafka：between 0.11 and 2.0.0 <a href="https://www.elastic.co/guide/en/beats/filebeat/6.8/kafka-output.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/beats/filebeat/6.8/kafka-output.html</a></li>
<li>Filebeat 7：兼容kafka：between 0.11 and 2.1.0 <a href="https://www.elastic.co/guide/en/beats/filebeat/current/kafka-output.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/beats/filebeat/current/kafka-output.html</a></li>
<li>Logstash 6-7：使用Kafka client 2.1.0 <a href="https://www.elastic.co/guide/en/logstash/6.8/plugins-outputs-kafka.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/logstash/6.8/plugins-outputs-kafka.html</a></li>
</ul><a id="more"></a>





<p>环境说明：</p>
<ul>
<li>一台EC2主机Amazon Linux 2 AMI，本身是基于CentOS 7</li>
<li>Zookeeper 3.5.6</li>
<li>Kafka 2.0.0</li>
</ul>
<p>在安装Kafka之前需要安装Zookeeper，如果你只是想测试用Kafka，那么在kafka的包里边已经有了zookeeper了，直接启动即可，如下：</p>
<p>启动Zookeeper：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ bin/zookeeper-server-start.sh config/zookeeper.properties</span><br></pre></td></tr></table></figure>

<p>然后再启动Kafka:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure>

<p>OK，一个测试用的Kafka和Zookeeper就安装完成了，但是我们要创建的是Kafka集群+Zookeeper集群，用于生产环境的，所以我们先部署Zookeeper集群：</p>
<h2 id="Zookeeper："><a href="#Zookeeper：" class="headerlink" title="Zookeeper："></a>Zookeeper：</h2><p><img src="https://imgs.knner.wang/images/logos/zookeeper.gif" alt></p>
<p>官网：</p>
<p><a href="https://zookeeper.apache.org/index.html" target="_blank" rel="noopener">https://zookeeper.apache.org/index.html</a></p>
<p>文档：</p>
<p><a href="https://zookeeper.apache.org/doc/r3.5.6/" target="_blank" rel="noopener">https://zookeeper.apache.org/doc/r3.5.6/</a></p>
<p>下载：</p>
<p><a href="http://zookeeper.apache.org/releases.html" target="_blank" rel="noopener">http://zookeeper.apache.org/releases.html</a></p>
<p>Zookeeper集群中ZK节点数一般为基数，当超过半数的ZK节点存活时，那么这个ZK集群就是可提供服务的。对于三台ZK节点组成的集群来说，如果有两台ZK存活，那么就可以提供服务，也就是说三台zk集群只允许宕机一台，五台zk组成的集群最多宕机两台。假定你想要容忍F台机器宕机，而集群仍需要提供服务的话，你需要部署2F+1台服务。</p>
<h3 id="安装："><a href="#安装：" class="headerlink" title="安装："></a>安装：</h3><p>说明：我这里只有一台EC2主机，所以采用不同端口的方式安装Zookeeper。</p>
<table>
<thead>
<tr>
<th>IP</th>
<th>Port Client</th>
<th>Port Connect to Leader</th>
<th>Port leader election</th>
<th>Port AdminServer</th>
<th>hosts 名</th>
</tr>
</thead>
<tbody><tr>
<td>172.17.0.87</td>
<td>2181</td>
<td>2888</td>
<td>3888</td>
<td>8081</td>
<td>zk01</td>
</tr>
<tr>
<td>172.17.0.87</td>
<td>2182</td>
<td>2889</td>
<td>3889</td>
<td>8082</td>
<td>zk02</td>
</tr>
<tr>
<td>172.17.0.87</td>
<td>2183</td>
<td>2890</td>
<td>3890</td>
<td>8083</td>
<td>zk03</td>
</tr>
</tbody></table>
<p>配置<code>/etc/hosts</code>，用于集群间通信，所有的zookeeper节点都需要配置。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat &gt;&gt; /etc/hosts &lt;&lt;EOF</span><br><span class="line">172.17.0.87 zk01 zk02 zk03</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>



<h4 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ wget -c <span class="string">"https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.5.6/apache-zookeeper-3.5.6-bin.tar.gz"</span></span><br><span class="line">$ <span class="built_in">cd</span> /data/knner &amp;&amp; tar xf /opt/softs/apache-zookeeper-3.5.6-bin.tar.gz</span><br><span class="line">$ ln -s apache-zookeeper-3.5.6-bin zookeeper</span><br></pre></td></tr></table></figure>

<h4 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ tree zookeeper/ -L 2</span><br><span class="line">zookeeper/</span><br><span class="line">├── bin</span><br><span class="line">│   ├── README.txt</span><br><span class="line">│   ├── zkCleanup.sh</span><br><span class="line">│   ├── zkCli.cmd</span><br><span class="line">│   ├── zkCli.sh</span><br><span class="line">│   ├── zkEnv.cmd</span><br><span class="line">│   ├── zkEnv.sh</span><br><span class="line">│   ├── zkServer.cmd</span><br><span class="line">│   ├── zkServer-initialize.sh</span><br><span class="line">│   ├── zkServer.sh</span><br><span class="line">│   ├── zkTxnLogToolkit.cmd</span><br><span class="line">│   └── zkTxnLogToolkit.sh</span><br><span class="line">├── conf</span><br><span class="line">│   ├── configuration.xsl</span><br><span class="line">│   ├── log4j.properties</span><br><span class="line">│   └── zoo_sample.cfg</span><br><span class="line">├── docs</span><br><span class="line">│   ├── apidocs</span><br><span class="line">│   ├── images</span><br><span class="line">│   ├── index.html</span><br><span class="line">│   ├── javaExample.html</span><br><span class="line">│   ├── recipes.html</span><br><span class="line">│   ├── releasenotes.html</span><br><span class="line">│   ├── skin</span><br><span class="line">│   ├── zookeeperAdmin.html</span><br><span class="line">│   ├── zookeeperHierarchicalQuorums.html</span><br><span class="line">│   ├── zookeeperInternals.html</span><br><span class="line">│   ├── zookeeperJMX.html</span><br><span class="line">│   ├── zookeeperObservers.html</span><br><span class="line">│   ├── zookeeperOtherInfo.html</span><br><span class="line">│   ├── zookeeperOver.html</span><br><span class="line">│   ├── zookeeperProgrammers.html</span><br><span class="line">│   ├── zookeeperQuotas.html</span><br><span class="line">│   ├── zookeeperReconfig.html</span><br><span class="line">│   ├── zookeeperStarted.html</span><br><span class="line">│   └── zookeeperTutorial.html</span><br><span class="line">├── lib</span><br><span class="line">│   ├── audience-annotations-0.5.0.jar</span><br><span class="line">│   ├── commons-cli-1.2.jar</span><br><span class="line">│   ├── jackson-annotations-2.9.10.jar</span><br><span class="line">│   ├── jackson-core-2.9.10.jar</span><br><span class="line">│   ├── jackson-databind-2.9.10.jar</span><br><span class="line">│   ├── javax.servlet-api-3.1.0.jar</span><br><span class="line">│   ├── jetty-http-9.4.17.v20190418.jar</span><br><span class="line">│   ├── jetty-io-9.4.17.v20190418.jar</span><br><span class="line">│   ├── jetty-security-9.4.17.v20190418.jar</span><br><span class="line">│   ├── jetty-server-9.4.17.v20190418.jar</span><br><span class="line">│   ├── jetty-servlet-9.4.17.v20190418.jar</span><br><span class="line">│   ├── jetty-util-9.4.17.v20190418.jar</span><br><span class="line">│   ├── jline-2.11.jar</span><br><span class="line">│   ├── jline-2.11.LICENSE.txt</span><br><span class="line">│   ├── json-simple-1.1.1.jar</span><br><span class="line">│   ├── json-simple-1.1.1.LICENSE.txt</span><br><span class="line">│   ├── log4j-1.2.17.jar</span><br><span class="line">│   ├── log4j-1.2.17.LICENSE.txt</span><br><span class="line">│   ├── netty-buffer-4.1.42.Final.jar</span><br><span class="line">│   ├── netty-buffer-4.1.42.Final.LICENSE.txt</span><br><span class="line">│   ├── netty-codec-4.1.42.Final.jar</span><br><span class="line">│   ├── netty-codec-4.1.42.Final.LICENSE.txt</span><br><span class="line">│   ├── netty-common-4.1.42.Final.jar</span><br><span class="line">│   ├── netty-common-4.1.42.Final.LICENSE.txt</span><br><span class="line">│   ├── netty-handler-4.1.42.Final.jar</span><br><span class="line">│   ├── netty-handler-4.1.42.Final.LICENSE.txt</span><br><span class="line">│   ├── netty-resolver-4.1.42.Final.jar</span><br><span class="line">│   ├── netty-resolver-4.1.42.Final.LICENSE.txt</span><br><span class="line">│   ├── netty-transport-4.1.42.Final.jar</span><br><span class="line">│   ├── netty-transport-4.1.42.Final.LICENSE.txt</span><br><span class="line">│   ├── netty-transport-native-epoll-4.1.42.Final.jar</span><br><span class="line">│   ├── netty-transport-native-epoll-4.1.42.Final.LICENSE.txt</span><br><span class="line">│   ├── netty-transport-native-unix-common-4.1.42.Final.jar</span><br><span class="line">│   ├── netty-transport-native-unix-common-4.1.42.Final.LICENSE.txt</span><br><span class="line">│   ├── slf4j-1.7.25.LICENSE.txt</span><br><span class="line">│   ├── slf4j-api-1.7.25.jar</span><br><span class="line">│   ├── slf4j-log4j12-1.7.25.jar</span><br><span class="line">│   ├── zookeeper-3.5.6.jar</span><br><span class="line">│   └── zookeeper-jute-3.5.6.jar</span><br><span class="line">├── LICENSE.txt</span><br><span class="line">├── NOTICE.txt</span><br><span class="line">├── README.md</span><br><span class="line">└── README_packaging.txt</span><br><span class="line"></span><br><span class="line">7 directories, 73 files</span><br></pre></td></tr></table></figure>



<h3 id="Zookeeper-配置详解"><a href="#Zookeeper-配置详解" class="headerlink" title="Zookeeper 配置详解"></a>Zookeeper 配置详解</h3><p>摘自：<a href="https://zookeeper.apache.org/doc/r3.5.6/zookeeperAdmin.html" target="_blank" rel="noopener">https://zookeeper.apache.org/doc/r3.5.6/zookeeperAdmin.html</a></p>
<p>Zookeeper 新版本中还支持动态的配置，请参考：<a href="https://zookeeper.apache.org/doc/r3.5.6/zookeeperReconfig.html" target="_blank" rel="noopener">https://zookeeper.apache.org/doc/r3.5.6/zookeeperReconfig.html</a></p>
<p>这里首先列出常用的配置以及说明，然后给出集群配置。</p>
<h4 id="最低配置："><a href="#最低配置：" class="headerlink" title="最低配置："></a>最低配置：</h4><ul>
<li><p><code>tickTime</code> 基本的时间单位，毫秒，2000=2秒；心跳时间和最小的session过期时间通常是2倍的tickTime。</p>
</li>
<li><p><code>dataDir</code> 数据存放目录，包括database snapshots;</p>
</li>
<li><p><code>clientPort</code> 用于和客户端建立连接的端口；</p>
</li>
<li><p><code>secureClientPort</code>：使用SSL侦听安全客户端连接的端口。clientPort指定用于纯文本连接的端口，而secureClientPort指定用于SSL连接的端口。同时指定两者都将启用混合模式，而省略其中任何一个将禁用该模式。请注意，当用户将Zookeeper.serverCnxnFactory，zookeeper.clientCnxnSocket插入为Netty时，将启用SSL功能。</p>
</li>
</ul>
<h4 id="进阶配置："><a href="#进阶配置：" class="headerlink" title="进阶配置："></a>进阶配置：</h4><p>本节中的配置设置是可选的。您可以使用它们进一步调整ZooKeeper服务器的行为。也可以使用Java系统属性来设置某些属性，通常形式为<em>zookeeper.keyword</em>。可用时，确切的系统属性在下面列出。</p>
<ul>
<li><p><code>dataLogDir</code> 来指定transaction log的存放位置，如果不设定，那么transaction log将存放到<code>dataDir</code> 中，生产中建议这两者分开存储在不同的磁盘上以提高zk的吞吐量；</p>
</li>
<li><p><code>globalOutstandingLimit</code>：（Java系统属性：zookeeper.globalOutstandingLimit。）客户端可以比ZooKeeper更快地提交请求，尤其是在有很多客户端的情况下。为了防止ZooKeeper由于排队的请求而耗尽内存，ZooKeeper会限制客户端，以便系统中的未完成请求不超过globalOutstandingLimit。默认限制为1,000。</p>
</li>
<li><p><code>preAllocSize</code>：（Java系统属性：zookeeper.preAllocSize）为了避免查找，ZooKeeper以preAllocSize千字节的块为单位在事务日志文件中分配空间。默认块大小为64M。更改块大小的原因之一是如果更频繁地拍摄快照，则减小块大小。（另请参见snapCount）。</p>
</li>
<li><p><code>snapCount</code>：（Java系统属性：zookeeper.snapCount）ZooKeeper使用快照和事务日志（请考虑预写日志）记录其事务。可以在拍摄快照之前记录在事务日志中的事务数（并且事务日志已滚动） ）由snapCount确定。为了防止仲裁中的所有机器同时拍摄快照，当交易日志中的交易数量达到运行时生成的[snapCount / 2 + 1]随机值时，每个ZooKeeper服务器都将拍摄快照。 ，snapCount]范围。默认的snapCount是100,000。</p>
</li>
<li><p><code>maxClientCnxns</code>：（非Java系统属性）将单个客户端（由IP地址标识）可以与ZooKeeper集成中的单个成员建立的并发连接数（在套接字级别）限制为多少。这用于防止某些类的DoS攻击，包括文件描述符耗尽。默认值为60。将其设置为0将完全消除并发连接的限制。</p>
</li>
<li><p><code>clientPortAddress</code>：3.3.0中的新功能：用于侦听客户端连接的地址（ipv4，ipv6或主机名）；即客户端尝试连接的地址。这是可选的，默认情况下，我们以这种方式绑定，即可以接受服务器上任何地址/接口/ NIC 到clientPort的任何连接。</p>
</li>
<li><p><code>minSessionTimeout</code>：（非Java系统属性）3.3.0中的新增功能：服务器允许客户端进行协商的最小会话超时（以毫秒为单位）。默认为tickTime的 2倍。</p>
</li>
<li><p><code>maxSessionTimeout</code>：（非Java系统属性）3.3.0中的新增功能：服务器允许客户端进行协商的最大会话超时（以毫秒为单位）。默认为tickTime的 20倍。</p>
</li>
<li><p><code>fsync.warningthresholdms</code>：（Java系统属性：zookeeper.fsync.warningthresholdms）3.3.4中的新增功能：每当事务日志（WAL）中的fsync花费的时间超过此值时，就会向该日志输出警告消息。该值以毫秒为单位指定，默认为1000。只能将其设置为系统属性。</p>
</li>
<li><p><code>autopurge.snapRetainCount</code>：（非Java系统属性）<strong>3.4.0中的新增</strong>功能<strong>：</strong>启用后，ZooKeeper自动清除功能<strong>会将autopurge.snapRetainCount</strong>最新快照和相应的事务日志分别保留在<strong>dataDir</strong>和<strong>dataLogDir中，</strong>并删除其余部分。默认值为3。最小值为3。</p>
</li>
<li><p><code>autopurge.purgeInterval</code>：（非Java系统属性）<strong>3.4.0中的新增功能：</strong>必须触发清除任务的时间间隔（以小时为单位）。设置为正整数（1或更大）以启用自动清除。预设为0。也就是关闭了自动清除功能。</p>
</li>
<li><p><code>syncEnabled</code>：（Java系统属性：zookeeper.observer.syncEnabled）新的3.4.6，3.5.0：现在的观察员在默认情况下，如参与者登录交易和写入快照磁盘。这减少了重新启动时观察者的恢复时间。设置为“ false”以禁用此功能。默认值为“ true”</p>
</li>
<li><p><code>zookeeper.extendedTypesEnabled</code>：（仅Java系统属性：zookeeper.extendedTypesEnabled）3.5.4中的新增功能：定义为“ true”以启用扩展功能，例如创建TTL节点。默认情况下禁用它们。重要信息：由于内部限制，启用的服务器ID必须小于255。</p>
</li>
<li><p><code>zookeeper.emulate353TTLNodes</code>：（仅Java系统属性：zookeeper.emulate353TTLNodes）3.5.4中的新增功能：由于ZOOKEEPER-2901，在3.5.4 / 3.6.0中不支持在3.5.3版中创建的TTL节点。但是，可通过zookeeper.emulate353TTLNodes系统属性提供解决方法。如果您在ZooKeeper 3.5.3中使用了TTL节点，并且需要维护兼容性，那么除了zookeeper.extendedTypesEnabled之外，请将zookeeper.emulate353TTLNodes设置为“ true” 。注意：由于该错误，服务器ID必须小于或等于127。此外，最大支持TTL值是1099511627775，小于3.5.3中允许的值（1152921504606846975）</p>
</li>
<li><p><code>serverCnxnFactory</code>：（Java系统属性：zookeeper.serverCnxnFactory）指定ServerCnxnFactory的实现。应该将其设置为NettyServerCnxnFactory使用基于TLS的服务器通信。默认值为NIOServerCnxnFactory。</p>
</li>
<li><p><code>snapshot.trust.empty</code>：（仅Java系统属性：zookeeper.snapshot.trust.empty）3.5.6中的新增功能：此属性控制ZooKeeper是否应将丢失的快照文件视为无法恢复的致命状态。设置为true允许ZooKeeper服务器在没有快照文件的情况下恢复。仅应在从旧版本的ZooKeeper（3.4.x，3.5.3之前）进行升级的过程中进行设置，在该版本中，ZooKeeper可能仅具有事务日志文件，而没有快照文件。如果在升级期间设置了该值，我们建议在升级后将该值重新设置为false并重新启动ZooKeeper进程，以便ZooKeeper可以在恢复过程中继续进行正常的数据一致性检查。默认值为false。</p>
</li>
</ul>
<h4 id="集群选项配置"><a href="#集群选项配置" class="headerlink" title="集群选项配置"></a>集群选项配置</h4><ul>
<li><code>lectionAlg</code>：（非Java系统属性）要使用的选举实现。值“ 1”对应于快速领导者选举的未经身份验证的基于UDP的版本，“ 2”对应于快速领导者的基于身份验证的UDP的版本，而“ 3”对应于快速领导者的基于TCP的版本选举。当前，算法3是默认算法。</li>
</ul>
<blockquote>
<p>注意 现在<strong>不推荐使用</strong>领导人选举1和2的实现。下一个版本中删除它们，届时只有FastLeaderElection将可用。</p>
</blockquote>
<ul>
<li><p><code>initLimit</code> 超时设定，用于限制zk集群中zk server连接到leader的时间长度，乘以基本事件单位<code>tickTime</code>为具体的时间，比如<code>initLimit=5</code> 那么就是5乘以<code>tickTime=2000</code>等于10000毫秒，即10秒。如果ZooKeeper管理的数据量很大，请根据需要增加此值。</p>
</li>
<li><p><code>leaderServes</code>：leader负责接受客户端连接。默认值为“是”。领导机器协调更新。为了获得较高的更新吞吐量，而以读取吞吐量为代价，可以将领导者配置为不接受客户端并专注于协调。此选项的默认值为“是”，这意味着领导者将接受客户端连接。</p>
</li>
</ul>
<blockquote>
<p>注意 当您在一个集合中拥有三个以上的ZooKeeper服务器时，强烈建议打开领导者选择。</p>
</blockquote>
<ul>
<li><p><code>syncLimit</code> 限制多长和leader的数据同步时间，limits how far out of date a server can be from a leader，乘以基本事件单位<code>tickTime</code>为具体的时间；允许以关注者与ZooKeeper同步的时间，如果追随者远远落后于领导者，他们将被丢弃。</p>
</li>
<li><p><code>server.X=host:port1:port2</code> 指定zk集群中的节点数，一行代表一个节点，X为1-255的整数，用于表示节点id，不能重复，因为在zk集群中，每个节点的这个server list配置都是一样的，当zk启动的时候根据位于<code>dataDir</code>目录中的myid文件中的数（X）来指定当前节点需要监听的两个端口，以及和其他zk节点通信的端口。注意myid文件只能包含一个X字符，不能包含其他。port1：用于followers和leader建立连接所用；port2：用于zk间选举leader。</p>
</li>
</ul>
<ul>
<li><p><code>group.x = nnnnn [：nnnnn]</code>：（无Java系统属性）启用分层仲裁。“ x”是组标识符，“ =”后的数字对应于服务器标识符。分配的左侧是用冒号分隔的服务器标识符列表。请注意，组必须是不相交的，并且所有组的并集必须是ZooKeeper集合。您会<a href="https://zookeeper.apache.org/doc/r3.5.6/zookeeperHierarchicalQuorums.html" target="_blank" rel="noopener">在这里</a>找到一个例子</p>
<p>基本思想很简单。首先，我们将服务器分为几组，并为每个组添加一行，列出组成该组的服务器。接下来，我们必须为每个服务器分配权重。</p>
<p>以下示例显示如何配置一个由三组组成的系统，每组三台服务器，并且我们为每个服务器分配权重1：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">group.1=1:2:3</span><br><span class="line">group.2=4:5:6</span><br><span class="line">group.3=7:8:9</span><br><span class="line"></span><br><span class="line">weight.1=1</span><br><span class="line">weight.2=1</span><br><span class="line">weight.3=1</span><br><span class="line">weight.4=1</span><br><span class="line">weight.5=1</span><br><span class="line">weight.6=1</span><br><span class="line">weight.7=1</span><br><span class="line">weight.8=1</span><br><span class="line">weight.9=1</span><br></pre></td></tr></table></figure>

<p>在运行系统时，一旦我们获得了来自大多数非零权重组的多数票，便能够形成法定人数。权重为零的组将被丢弃，并且在形成仲裁时不会被考虑。看这个例子，一旦我们有来自两个不同组中每个组的至少两台服务器的投票，便能够形成法定人数。</p>
</li>
<li><p><code>weight.x = nnnnn</code>：（无Java系统属性）与“ group”一起使用，它在形成仲裁时为服务器分配权重。该值对应于投票时服务器的权重。ZooKeeper的某些部分需要投票，例如领导人选举和原子广播协议。默认情况下，服务器的权重为1。如果配置定义了组，但没有定义权重，则将为所有服务器分配值1。您会<a href="https://zookeeper.apache.org/doc/r3.5.6/zookeeperHierarchicalQuorums.html" target="_blank" rel="noopener">在这里</a>找到一个例子</p>
</li>
<li><p><code>cnxTimeout</code>：（Java系统属性：饲养员<strong>cnxTimeout</strong>）设置为打开领导人选举通知连接超时值。仅在您使用选举算法3时适用。</p>
<blockquote>
<p>默认值为5秒。</p>
</blockquote>
</li>
<li><p><code>standaloneEnabled</code>：（无Java系统属性）<strong>3.5.0中的新增功能：</strong>设置为false时，可以以复制模式启动单个服务器，可以由观察者运行单个参与者，并且群集可以重新配置为一个节点，然后从一个节点。为了向后兼容，默认值为true。可以使用QuorumPeerConfig的setStandaloneEnabled方法或通过将“ standaloneEnabled = false”或“ standaloneEnabled = true”添加到服务器的配置文件中来进行设置。</p>
</li>
<li><p><code>reconfigEnabled</code>：（无Java系统属性）<strong>3.5.3的新增功能：</strong>此功能控制启用或禁用<a href="https://zookeeper.apache.org/doc/r3.5.6/zookeeperReconfig.html" target="_blank" rel="noopener">动态重新配置</a>功能。启用此功能后，假设用户被授权执行此类操作，则用户可以通过ZooKeeper客户端API或通过ZooKeeper命令行工具执行重新配置操作。禁用此功能后，包括超级用户在内的任何用户都无法执行重新配置。任何重新配置的尝试都将返回错误。可以将<strong>“ reconfigEnabled”</strong>选项设置为<strong>“ reconfigEnabled = false”</strong>或<strong>“ reconfigEnabled = true”</strong>到服务器的配置文件，或使用QuorumPeerConfig的setReconfigEnabled方法。默认值为false。如果存在，则该值在整个集合中的每个服务器上都应保持一致。在某些服务器上将该值设置为true，而在其他服务器上将该值设置为false，则将导致不一致的行为，具体取决于哪个服务器被选为领导者。如果领导者的设置为<strong>“ reconfigEnabled = true”</strong>，则集成将启用重新配置功能。如果领导者的设置为<strong>“ reconfigEnabled = false”</strong>，则该集成将禁用重新配置功能。因此，建议在集成服务器中的<strong>“ reconfigEnabled”</strong>具有一致的值。</p>
</li>
<li><p><code>4lw.commands.whitelist</code>：（Java系统属性：<strong>zookeeper.4lw.commands.whitelist</strong>）<strong>3.5.3的新增功能：</strong>用户要使用的逗号分隔的<a href="https://zookeeper.apache.org/doc/r3.5.6/zookeeperAdmin.html#sc_4lw" target="_blank" rel="noopener">四个字母单词</a>命令列表。必须在此列表中输入有效的四个字母的命令，否则ZooKeeper服务器将不会启用该命令。默认情况下，白名单仅包含zkServer.sh使用的“ srvr”命令。默认情况下，其余四个字母单词命令是禁用的。这是启用stat，ruok，conf和isro命令同时禁用其余四个字母单词命令的配置示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">4lw.commands.whitelist=stat, ruok, conf, isro</span><br></pre></td></tr></table></figure>

<p>如果确实需要默认情况下启用所有四个字母词命令，则可以使用星号选项，这样您就不必在列表中一个接一个地添加每个命令。例如，这将启用所有四个字母词命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">4lw.commands.whitelist=*</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>tcpKeepAlive</code>：（Java系统属性：<strong>zookeeper.tcpKeepAlive</strong>）<strong>3.5.4的新增功能：</strong>将此属性设置为true可以在仲裁成员用于执行选举的套接字上设置TCP keepAlive标志。当存在可能破坏仲裁成员的网络基础结构时，这将允许仲裁成员之间的连接保持连接状态。对于长时间运行或空闲的连接，某些NAT和防火墙可能会终止或丢失状态。启用此选项取决于操作系统级别的设置才能正常工作，有关更多信息，请检查操作系统有关TCP keepalive的选项。默认为<strong>false</strong>。</p>
</li>
<li><p><code>lectionPortBindRetry</code>：（仅Java系统属性：<strong>zookeeper.electionPortBindRetry</strong>）该属性设置当Zookeeper服务器无法绑定领导者选举端口时的最大重试次数。此类错误可以是临时的且可恢复的（例如<a href="https://issues.apache.org/jira/projects/ZOOKEEPER/issues/ZOOKEEPER-3320" target="_blank" rel="noopener">ZOOKEEPER-3320中</a>描述的DNS问题），也可以是不可重试的（例如已在使用的端口）。<br>如果出现暂时性错误，此属性可以提高Zookeeper服务器的可用性并帮助其自我恢复。默认值3.在容器环境中，尤其是在Kubernetes中，应增加该值或将其设置为0（无限重试），以解决与DNS名称解析有关的问题。</p>
</li>
</ul>
<h4 id="加密，身份验证，授权选项"><a href="#加密，身份验证，授权选项" class="headerlink" title="加密，身份验证，授权选项"></a>加密，身份验证，授权选项</h4><p>本节中的选项允许控制服务执行的加密/认证/授权。</p>
<ul>
<li><em>DigestAuthenticationProvider.superDigest</em>：（Java系统属性：<strong>zookeeper.DigestAuthenticationProvider.superDigest</strong>）默认情况下，此功能<strong>处于**</strong>禁用状态** 。3.2中的<strong>新增</strong>功能<strong>：</strong>使ZooKeeper集成管理员以“超级”用户身份访问znode层次结构。特别是，对于通过身份验证为超级的用户，不会进行ACL检查。org.apache.zookeeper.server.auth.DigestAuthenticationProvider可用于生成superDigest，并使用一个参数“ super：提供生成的“超级”：作为启动集合的每个服务器时的系统属性值。（从ZooKeeper客户端对ZooKeeper服务器进行身份验证时，传递“ digest”方案和“ super：authdata”方案）“。请注意，摘要身份验证将纯文本身份验证数据传递给服务器，因此，最好仅在本地主机（而不是通过网络）或加密连接上使用此身份验证方法。</li>
<li><em>X509AuthenticationProvider.superUser</em>：（ Java系统属性：<strong>zookeeper.X509AuthenticationProvider.superUser</strong>）SSL支持的方法，使ZooKeeper集成管理员能够以“超级”用户身份访问znode层次结构。当此参数设置为X500主体名称时，只有具有该主体的经过身份验证的客户端才能够绕过ACL检查，并具有对所有znode的完全特权。</li>
<li><em>zookeeper.superUser</em>：（Java系统属性：<strong>zookeeper.superUser</strong>）类似于<strong>zookeeper.X509AuthenticationProvider.superUser，</strong>但对于基于SASL的登录名是通用的。它存储可以作为“超级”用户访问znode层次结构的用户名。</li>
<li><em>ssl.authProvider</em>：（Java系统属性：<strong>zookeeper.ssl.authProvider</strong>）指定用于安全客户端身份验证的<strong>org.apache.zookeeper.auth.X509AuthenticationProvider</strong>的子类。这在不使用JKS的证书密钥基础结构中很有用。可能需要扩展<strong>javax.net.ssl.X509KeyManager</strong>和<strong>javax.net.ssl.X509TrustManager</strong>才能从SSL堆栈中获得所需的行为。要将ZooKeeper服务器配置为使用自定义提供程序进行身份验证，请为自定义AuthenticationProvider选择方案名称，然后设置属性<strong>zookeeper.authProvider。[方案]</strong>自定义实现的全限定类名。这会将提供程序加载到ProviderRegistry中。然后设置此属性<strong>zookeeper.ssl.authProvider = [scheme]</strong>，该提供程序将用于安全身份验证。</li>
<li><em>sslQuorum</em>：（Java系统属性：<strong>zookeeper.sslQuorum</strong>）<strong>3.5.5的新增功能：</strong>启用加密的仲裁通信。默认值为<code>false</code>。</li>
<li><em>ssl.keyStore.location和ssl.keyStore.password</em>和<em>ssl.quorum.keyStore.location</em>和<em>ssl.quorum.keyStore.password</em>：（Java系统属性：<strong>zookeeper.ssl.keyStore.location</strong>和<strong>zookeeper.ssl.keyStore.password</strong>和<strong>zookeeper （.ssl.quorum.keyStore.location</strong>和<strong>zookeeper.ssl.quorum.keyStore.password</strong>）<strong>3.5.5中的新增功能：</strong>指定Java密钥库的文件路径，该路径包含用于客户端和仲裁TLS连接的本地凭据以及密码解锁文件。</li>
<li><em>ssl.keyStore.type</em>和<em>ssl.quorum.keyStore.type</em>：（Java系统属性：<strong>zookeeper.ssl.keyStore.type</strong>和<strong>zookeeper.ssl.quorum.keyStore.type</strong>）<strong>3.5.5中的新增功能：</strong>指定客户端和客户端的文件格式。法定密钥库。值：JKS，PEM，PKCS12或为空（按文件名检测）。<br>默认值：空</li>
<li><em>ssl.trustStore.location</em>和<em>ssl.trustStore.password</em>和<em>ssl.quorum.trustStore.location</em>和<em>ssl.quorum.trustStore.password</em>：（Java系统属性：<strong>zookeeper.ssl.trustStore.location</strong>和<strong>zookeeper.ssl.trustStore.password</strong>和<strong>zookeeper .ssl.quorum.trustStore.location</strong>和<strong>zookeeper.ssl.quorum.trustStore.password</strong>）<strong>3.5.5中的新增功能：</strong>指定Java信任库的文件路径，该路径包含用于客户端和仲裁TLS连接的远程凭据以及密码。解锁文件。</li>
<li><em>ssl.trustStore.type</em>和<em>ssl.quorum.trustStore.type</em>：（Java系统属性：<strong>zookeeper.ssl.trustStore.type</strong>和<strong>zookeeper.ssl.quorum.trustStore.type</strong>）<strong>3.5.5中的新增功能：</strong>指定客户端和客户端的文件格式。仲裁信任库。值：JKS，PEM，PKCS12或为空（按文件名检测）。<br>默认值：空</li>
<li><em>ssl.protocol</em>和<em>ssl.quorum.protocol</em>：（Java系统属性：<strong>zookeeper.ssl.protocol</strong>和<strong>zookeeper.ssl.quorum.protocol</strong>）<strong>3.5.5中的新增功能：</strong>指定要在客户端和仲裁TLS协商中使用的协议。默认值：TLSv1.2</li>
<li><em>ssl.enabledProtocols</em>和<em>ssl.quorum.enabledProtocols</em>：（Java系统属性：<strong>zookeeper.ssl.enabledProtocols</strong>和<strong>zookeeper.ssl.quorum.enabledProtocols</strong>）<strong>3.5.5中的新增</strong>功能<strong>：</strong>指定客户端和仲裁TLS协商中的已启用协议。默认值：<code>protocol</code>属性值</li>
<li><em>ssl.ciphersuites</em>和<em>ssl.quorum.ciphersuites</em>：（Java系统属性：<strong>zookeeper.ssl.ciphersuites</strong>和<strong>zookeeper.ssl.quorum.ciphersuites</strong>）<strong>3.5.5中的新增</strong>功能<strong>：</strong>指定在客户端和仲裁TLS协商中使用的已启用密码套件。默认值：启用的密码套件取决于所使用的Java运行时版本。</li>
<li><em>ssl.context.supplier.class</em>和<em>ssl.quorum.context.supplier.class</em>：（Java系统属性：<strong>zookeeper.ssl.context.supplier.class</strong>和<strong>zookeeper.ssl.quorum.context.supplier.class</strong>）<strong>3.5.5中的新功能：</strong>指定用于在客户端和仲裁SSL通信中创建SSL上下文的类。这使您可以使用自定义SSL上下文并实现以下方案：<ol>
<li>使用使用PKCS11或类似工具加载的硬件密钥库。</li>
<li>您无权访问软件密钥库，但可以从其容器中检索已经构造的SSLContext。默认值：空</li>
</ol>
</li>
<li><em>ssl.hostnameVerification</em>和<em>ssl.quorum.hostnameVerification</em>：（Java系统属性：<strong>zookeeper.ssl.hostnameVerification</strong>和<strong>zookeeper.ssl.quorum.hostnameVerification</strong>）<strong>3.5.5中的新增功能：</strong>指定是否在客户端和仲裁TLS协商过程中启用主机名验证。仅出于测试目的而建议禁用它。默认值：true</li>
<li><em>ssl.crl</em>和<em>ssl.quorum.crl</em>：（Java系统属性：<strong>zookeeper.ssl.crl</strong>和<strong>zookeeper.ssl.quorum.crl</strong>）<strong>3.5.5中的新增功能：</strong>指定是否在客户端和仲裁TLS协议中启用证书吊销列表。默认值：false</li>
<li><em>ssl.ocsp</em>和<em>ssl.quorum.ocsp</em>：（Java系统属性：<strong>zookeeper.ssl.ocsp</strong>和<strong>zookeeper.ssl.quorum.ocsp</strong>）<strong>3.5.5中的新增功能：</strong>指定是否在客户端和仲裁TLS协议中启用了“在线证书状态协议”。默认值：false</li>
<li><em>ssl.clientAuth</em>和<em>ssl.quorum.clientAuth</em>：（Java系统属性：<strong>zookeeper.ssl.clientAuth</strong>和<strong>zookeeper.ssl.quorum.clientAuth</strong>）<strong>3.5.5中的新增功能：</strong> TBD</li>
<li><em>ssl.handshakeDetectionTimeoutMillis</em>和<em>ssl.quorum.handshakeDetectionTimeoutMillis</em>：（Java系统属性：<strong>zookeeper.ssl.handshakeDetectionTimeoutMillis</strong>和<strong>zookeeper.ssl.quorum.handshakeDetectionTimeoutMillis</strong>）<strong>3.5.5中的新功能：</strong> TBD</li>
</ul>
<h4 id="性能调整选项"><a href="#性能调整选项" class="headerlink" title="性能调整选项"></a>性能调整选项</h4><p><strong>3.5.0中的新增功能：</strong>已对多个子系统进行了改进，以提高读取吞吐量。这包括NIO通信子系统和请求处理管道（Commit Processor）的多线程。NIO是默认的客户端/服务器通信子系统。它的线程模型包括1个接收器线程，1-N个选择器线程和0-M个套接字I / O工作线程。在请求处理管道中，系统可以配置为一次处理多个读取请求，同时保持相同的一致性保证（相同的会话写入后读取）。提交处理器线程模型包括1个主线程和0-N个工作线程。</p>
<p>默认值旨在最大化专用ZooKeeper机器上的读取吞吐量。这两个子系统都需要有足够数量的线程才能达到峰值读取吞吐量。</p>
<ul>
<li><em>zookeeper.nio.numSelectorThreads</em>：（仅Java系统属性：<strong>zookeeper.nio.numSelectorThreads</strong>）<strong>3.5.0中的新增功能：</strong> NIO选择器线程数。至少需要1个选择器线程。对于大量的客户端连接，建议使用多个选择器。默认值为sqrt（cpu核心数/ 2）。</li>
<li><em>zookeeper.nio.numWorkerThreads</em>：（仅Java系统属性：<strong>zookeeper.nio.numWorkerThreads</strong>）<strong>3.5.0中的新增功能：</strong> NIO工作线程数。如果配置了0个工作线程，则选择器线程直接执行套接字I / O。默认值为cpu核心数的2倍。</li>
<li><em>zookeeper.commitProcessor.numWorkerThreads</em>：（仅Java系统属性：<strong>zookeeper.commitProcessor.numWorkerThreads</strong>）<strong>3.5.0中的新增功能：</strong>提交处理器工作线程数。如果配置了0个工作线程，则主线程将直接处理请求。默认值为cpu核心数。</li>
<li><em>znode.container.checkIntervalMs</em>：（仅Java系统属性）<strong>3.5.1中的新增功能：</strong>每次检查候选容器和ttl节点的时间间隔（以毫秒为单位）。默认值为“ 60000”。</li>
<li><em>znode.container.maxPerMinute</em>：（仅Java系统属性）<strong>3.5.1中的新增功能：</strong>每分钟可以删除的容器和ttl节点的最大数量。这样可以防止在删除容器时放牧。默认值为“ 10000”。</li>
</ul>
<h4 id="AdminServer配置"><a href="#AdminServer配置" class="headerlink" title="AdminServer配置"></a>AdminServer配置</h4><p><strong>3.5.0中的新增功能：</strong>以下选项用于配置<a href="https://zookeeper.apache.org/doc/r3.5.6/zookeeperAdmin.html#sc_adminserver" target="_blank" rel="noopener">AdminServer</a>。</p>
<ul>
<li><em>admin.enableServer</em>：（Java系统属性：<strong>zookeeper.admin.enableServer</strong>）设置为“ false”以禁用AdminServer。默认情况下，AdminServer是启用的。</li>
<li><em>admin.serverAddress</em>：（Java系统属性：<strong>zookeeper.admin.serverAddress</strong>）嵌入式Jetty服务器侦听的地址。默认为0.0.0.0。</li>
<li><em>admin.serverPort</em>：（Java系统属性：<strong>zookeeper.admin.serverPort</strong>）嵌入式Jetty服务器侦听的端口。默认为8080</li>
<li><em>admin.idleTimeout</em>：（Java系统属性：<strong>zookeeper.admin.idleTimeout</strong>）设置连接在发送或接收数据之前可以等待的最大空闲时间（以毫秒为单位）。默认为30000毫秒</li>
<li><em>admin.commandURL</em>：（Java系统属性：<strong>zookeeper.admin.commandURL</strong>）相对于根URL列出和发布命令的URL。默认为“ /commands”。</li>
</ul>
<h3 id="Zookeeper-集群配置"><a href="#Zookeeper-集群配置" class="headerlink" title="Zookeeper 集群配置"></a>Zookeeper 集群配置</h3><p>了解的具体的配置的作用，下面来配置ZK集群。</p>
<h4 id="zk01-配置"><a href="#zk01-配置" class="headerlink" title="zk01 配置"></a>zk01 配置</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat zk01/conf/zoo.cfg</span><br><span class="line"><span class="comment"># The number of milliseconds of each tick 时间单位为2s</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="comment"># The number of ticks that the initial </span></span><br><span class="line"><span class="comment"># synchronization phase can take。此时是20s</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="comment"># The number of ticks that can pass between </span></span><br><span class="line"><span class="comment"># sending a request and getting an acknowledgement 此时是10s</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just </span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line">dataDir=/data/zkdata/zk01</span><br><span class="line"></span><br><span class="line">dataLogDir=/data/zklog/zk01</span><br><span class="line"><span class="comment"># the port at which the clients will connect 只监听的172.17.0.87这个地址上，端口是2181，多网卡很有用</span></span><br><span class="line">clientPort=2181</span><br><span class="line">clientPortAddress=172.17.0.87</span><br><span class="line"><span class="comment"># the maximum number of client connections.</span></span><br><span class="line"><span class="comment"># increase this if you need to handle more clients 客户端最大连接数</span></span><br><span class="line">maxClientCnxns=1000</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Be sure to read the maintenance section of the </span></span><br><span class="line"><span class="comment"># administrator guide before turning on autopurge.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The number of snapshots to retain in dataDir 保留3个snapshots</span></span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"><span class="comment"># Purge task interval in hours</span></span><br><span class="line"><span class="comment"># Set to "0" to disable auto purge feature 每隔3h执行一次</span></span><br><span class="line">autopurge.purgeInterval=3</span><br><span class="line"></span><br><span class="line"><span class="comment"># cluster 集群配置</span></span><br><span class="line">server.1=zk01:2888:3888</span><br><span class="line">server.2=zk02:2887:3887</span><br><span class="line">server.3=zk03:2886:3886</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4lw 可以通过telnet，nc，执行的4个字符的命令白名单，我这里全部允许。生产要注意</span></span><br><span class="line">4lw.commands.whitelist=*</span><br><span class="line"></span><br><span class="line"><span class="comment"># admin server 开启admin server，可以代替4lw，通过http://IP:clientPort/knner/xxxx 方式访问，返回json格式。</span></span><br><span class="line">admin.enableServer=<span class="literal">true</span></span><br><span class="line">admin.serverAddress=172.17.0.87</span><br><span class="line">admin.serverPort=8081</span><br><span class="line">admin.commandURL=/knner <span class="comment"># 默认是/commands</span></span><br></pre></td></tr></table></figure>

<h4 id="zk02-配置"><a href="#zk02-配置" class="headerlink" title="zk02 配置"></a>zk02 配置</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat zk02/conf/zoo.cfg</span><br><span class="line"><span class="comment"># The number of milliseconds of each tick</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="comment"># The number of ticks that the initial </span></span><br><span class="line"><span class="comment"># synchronization phase can take</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="comment"># The number of ticks that can pass between </span></span><br><span class="line"><span class="comment"># sending a request and getting an acknowledgement</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just </span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line">dataDir=/data/zkdata/zk02</span><br><span class="line"></span><br><span class="line">dataLogDir=/data/zklog/zk02</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line">clientPort=2182</span><br><span class="line">clientPortAddress=172.17.0.87</span><br><span class="line"><span class="comment"># the maximum number of client connections.</span></span><br><span class="line"><span class="comment"># increase this if you need to handle more clients</span></span><br><span class="line">maxClientCnxns=1000</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Be sure to read the maintenance section of the </span></span><br><span class="line"><span class="comment"># administrator guide before turning on autopurge.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The number of snapshots to retain in dataDir</span></span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"><span class="comment"># Purge task interval in hours</span></span><br><span class="line"><span class="comment"># Set to "0" to disable auto purge feature</span></span><br><span class="line">autopurge.purgeInterval=3</span><br><span class="line"></span><br><span class="line"><span class="comment"># cluster</span></span><br><span class="line">server.1=zk01:2888:3888</span><br><span class="line">server.2=zk02:2887:3887</span><br><span class="line">server.3=zk03:2886:3886</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4lw</span></span><br><span class="line">4lw.commands.whitelist=*</span><br><span class="line"></span><br><span class="line"><span class="comment"># admin server</span></span><br><span class="line">admin.enableServer=<span class="literal">true</span></span><br><span class="line">admin.serverAddress=172.17.0.87</span><br><span class="line">admin.serverPort=8082</span><br><span class="line">admin.commandURL=/knner</span><br></pre></td></tr></table></figure>



<h4 id="zk03-配置"><a href="#zk03-配置" class="headerlink" title="zk03 配置"></a>zk03 配置</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat zk03/conf/zoo.cfg</span><br><span class="line"><span class="comment"># The number of milliseconds of each tick</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="comment"># The number of ticks that the initial </span></span><br><span class="line"><span class="comment"># synchronization phase can take</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="comment"># The number of ticks that can pass between </span></span><br><span class="line"><span class="comment"># sending a request and getting an acknowledgement</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just </span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line">dataDir=/data/zkdata/zk03</span><br><span class="line"></span><br><span class="line">dataLogDir=/data/zklog/zk03</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line">clientPort=2183</span><br><span class="line">clientPortAddress=172.17.0.87</span><br><span class="line"><span class="comment"># the maximum number of client connections.</span></span><br><span class="line"><span class="comment"># increase this if you need to handle more clients</span></span><br><span class="line">maxClientCnxns=1000</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Be sure to read the maintenance section of the </span></span><br><span class="line"><span class="comment"># administrator guide before turning on autopurge.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The number of snapshots to retain in dataDir</span></span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"><span class="comment"># Purge task interval in hours</span></span><br><span class="line"><span class="comment"># Set to "0" to disable auto purge feature</span></span><br><span class="line">autopurge.purgeInterval=3</span><br><span class="line"></span><br><span class="line"><span class="comment"># cluster</span></span><br><span class="line">server.1=zk01:2888:3888</span><br><span class="line">server.2=zk02:2887:3887</span><br><span class="line">server.3=zk03:2886:3886</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4lw</span></span><br><span class="line">4lw.commands.whitelist=*</span><br><span class="line"></span><br><span class="line"><span class="comment"># admin server</span></span><br><span class="line">admin.enableServer=<span class="literal">true</span></span><br><span class="line">admin.serverAddress=172.17.0.87</span><br><span class="line">admin.serverPort=8083</span><br><span class="line">admin.commandURL=/knner</span><br></pre></td></tr></table></figure>



<p>创建所需的目录：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建dataDir:</span></span><br><span class="line">$ mkdir -p /data/zkdata/zk0&#123;1..3&#125; -pv</span><br><span class="line">mkdir: created directory ‘/data/zkdata’</span><br><span class="line">mkdir: created directory ‘/data/zkdata/zk01’</span><br><span class="line">mkdir: created directory ‘/data/zkdata/zk02’</span><br><span class="line">mkdir: created directory ‘/data/zkdata/zk03’</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建dataLogDir:</span></span><br><span class="line">$ mkdir -p /data/zklog/zk0&#123;1..3&#125; -pv</span><br><span class="line">mkdir: created directory ‘/data/zklog’</span><br><span class="line">mkdir: created directory ‘/data/zklog/zk01’</span><br><span class="line">mkdir: created directory ‘/data/zklog/zk02’</span><br><span class="line">mkdir: created directory ‘/data/zklog/zk03’</span><br></pre></td></tr></table></figure>

<p>创建<code>myid</code>文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> 1 &gt; /data/zkdata/zk01/myid</span><br><span class="line">$ <span class="built_in">echo</span> 2 &gt; /data/zkdata/zk02/myid</span><br><span class="line">$ <span class="built_in">echo</span> 3 &gt; /data/zkdata/zk03/myid</span><br></pre></td></tr></table></figure>



<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>配置完毕，我们准备启动ZK节点，首先查看启动命令的帮助：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看帮助</span></span><br><span class="line">$ ./zk01/bin/zkServer.sh </span><br><span class="line">/usr/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /data/wanghk/zk01/bin/../conf/zoo.cfg</span><br><span class="line">Usage: ./zk01/bin/zkServer.sh [--config &lt;conf-dir&gt;] &#123;start|start-foreground|stop|restart|status|<span class="built_in">print</span>-cmd&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们先用print-cmd来打印一下启动的命令</span></span><br><span class="line"><span class="comment"># 注意--config 时指定配置文件所在的文件夹，配置文件必须名为zoo.cfg</span></span><br><span class="line">$ ./zk01/bin/zkServer.sh --config zk01/conf <span class="built_in">print</span>-cmd</span><br><span class="line">/usr/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: zk01/conf/zoo.cfg</span><br><span class="line"><span class="string">"java"</span>  -Dzookeeper.log.dir=<span class="string">"/data/knner/zk01/bin/../logs"</span>     -Dzookeeper.log.file=<span class="string">"zookeeper-ec2-user-server-test01.dev.awsbj.cn.log"</span> -Dzookeeper.root.logger=<span class="string">"INFO,CONSOLE"</span>     -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=<span class="string">'kill -9 %p'</span>     -cp <span class="string">"/data/knner/zk01/bin/../zookeeper-server/target/classes:/data/knner/zk01/bin/../build/classes:/data/knner/zk01/bin/../zookeeper-server/target/lib/*.jar:/data/knner/zk01/bin/../build/lib/*.jar:/data/knner/zk01/bin/../lib/zookeeper-jute-3.5.6.jar:/data/knner/zk01/bin/../lib/zookeeper-3.5.6.jar:/data/knner/zk01/bin/../lib/slf4j-log4j12-1.7.25.jar:/data/knner/zk01/bin/../lib/slf4j-api-1.7.25.jar:/data/knner/zk01/bin/../lib/netty-transport-native-unix-common-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-transport-native-epoll-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-transport-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-resolver-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-handler-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-common-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-codec-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-buffer-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/log4j-1.2.17.jar:/data/knner/zk01/bin/../lib/json-simple-1.1.1.jar:/data/knner/zk01/bin/../lib/jline-2.11.jar:/data/knner/zk01/bin/../lib/jetty-util-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-servlet-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-server-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-security-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-io-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-http-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/javax.servlet-api-3.1.0.jar:/data/knner/zk01/bin/../lib/jackson-databind-2.9.10.jar:/data/knner/zk01/bin/../lib/jackson-core-2.9.10.jar:/data/knner/zk01/bin/../lib/jackson-annotations-2.9.10.jar:/data/knner/zk01/bin/../lib/commons-cli-1.2.jar:/data/knner/zk01/bin/../lib/audience-annotations-0.5.0.jar:/data/knner/zk01/bin/../zookeeper-*.jar:/data/knner/zk01/bin/../zookeeper-server/src/main/resources/lib/*.jar:zk01/conf:"</span> -Xmx1000m   -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=<span class="literal">false</span> org.apache.zookeeper.server.quorum.QuorumPeerMain <span class="string">"zk01/conf/zoo.cfg"</span> &gt; <span class="string">"/data/knner/zk01/bin/../logs/zookeeper-ec2-user-server-test01.dev.awsbj.cn.out"</span> 2&gt;&amp;1 &lt; /dev/null</span><br><span class="line"></span><br><span class="line"><span class="comment">#我们发现-Xmx1000m，默认是1G的大小，配置位置：</span></span><br><span class="line">$ vim bin/zkEnv.sh <span class="comment"># 文章最后</span></span><br><span class="line"><span class="comment"># default heap for zookeeper server</span></span><br><span class="line">ZK_SERVER_HEAP=<span class="string">"<span class="variable">$&#123;ZK_SERVER_HEAP:-1000&#125;</span>"</span></span><br><span class="line"><span class="built_in">export</span> SERVER_JVMFLAGS=<span class="string">"-Xmx<span class="variable">$&#123;ZK_SERVER_HEAP&#125;</span>m <span class="variable">$SERVER_JVMFLAGS</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># default heap for zookeeper client</span></span><br><span class="line">ZK_CLIENT_HEAP=<span class="string">"<span class="variable">$&#123;ZK_CLIENT_HEAP:-256&#125;</span>"</span></span><br><span class="line"><span class="built_in">export</span> CLIENT_JVMFLAGS=<span class="string">"-Xmx<span class="variable">$&#123;ZK_CLIENT_HEAP&#125;</span>m <span class="variable">$CLIENT_JVMFLAGS</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当然也可以使用环境变量，注意单位值m</span></span><br><span class="line">$ ZK_SERVER_HEAP=512 ./zk01/bin/zkServer.sh --config zk01/conf <span class="built_in">print</span>-cmd</span><br><span class="line">/usr/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: zk01/conf/zoo.cfg</span><br><span class="line">...... -Xmx512m</span><br></pre></td></tr></table></figure>

<p>正式启动：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./zk01/bin/zkServer.sh --config zk01/conf start</span><br><span class="line">/usr/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: zk01/conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure>

<p>查看进程以及端口监听：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ps -ef|grep zk01</span><br><span class="line">ec2-user 30504     1  1 13:08 pts/5    00:00:01 java -Dzookeeper.log.dir=/data/knner/zk01/bin/../logs -Dzookeeper.log.file=zookeeper-ec2-user-server-test01.dev.awsbj.cn.log -Dzookeeper.root.logger=INFO,CONSOLE -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=<span class="built_in">kill</span> -9 %p -cp /data/knner/zk01/bin/../zookeeper-server/target/classes:/data/knner/zk01/bin/../build/classes:/data/knner/zk01/bin/../zookeeper-server/target/lib/*.jar:/data/knner/zk01/bin/../build/lib/*.jar:/data/knner/zk01/bin/../lib/zookeeper-jute-3.5.6.jar:/data/knner/zk01/bin/../lib/zookeeper-3.5.6.jar:/data/knner/zk01/bin/../lib/slf4j-log4j12-1.7.25.jar:/data/knner/zk01/bin/../lib/slf4j-api-1.7.25.jar:/data/knner/zk01/bin/../lib/netty-transport-native-unix-common-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-transport-native-epoll-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-transport-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-resolver-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-handler-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-common-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-codec-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-buffer-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/log4j-1.2.17.jar:/data/knner/zk01/bin/../lib/json-simple-1.1.1.jar:/data/knner/zk01/bin/../lib/jline-2.11.jar:/data/knner/zk01/bin/../lib/jetty-util-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-servlet-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-server-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-security-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-io-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-http-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/javax.servlet-api-3.1.0.jar:/data/knner/zk01/bin/../lib/jackson-databind-2.9.10.jar:/data/knner/zk01/bin/../lib/jackson-core-2.9.10.jar:/data/knner/zk01/bin/../lib/jackson-annotations-2.9.10.jar:/data/knner/zk01/bin/../lib/commons-cli-1.2.jar:/data/knner/zk01/bin/../lib/audience-annotations-0.5.0.jar:/data/knner/zk01/bin/../zookeeper-*.jar:/data/knner/zk01/bin/../zookeeper-server/src/main/resources/lib/*.jar:zk01/conf: -Xmx1001m -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=<span class="literal">false</span> org.apache.zookeeper.server.quorum.QuorumPeerMain zk01/conf/zoo.cfg</span><br><span class="line"></span><br><span class="line"><span class="comment"># 监听端口</span></span><br><span class="line">$ netstat -lnutp|grep 30504</span><br><span class="line">(Not all processes could be identified, non-owned process info</span><br><span class="line"> will not be shown, you would have to be root to see it all.)</span><br><span class="line">tcp6       0      0 172.17.0.87:3888        :::*                    LISTEN      30504/java          </span><br><span class="line">tcp6       0      0 172.17.0.87:8081        :::*                    LISTEN      30504/java          </span><br><span class="line">tcp6       0      0 :::46075                :::*                    LISTEN      30504/java          </span><br><span class="line">tcp6       0      0 172.17.0.87:2181        :::*                    LISTEN      30504/java</span><br></pre></td></tr></table></figure>

<p>启动其余两台zk：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./zk02/bin/zkServer.sh --config zk02/conf start</span><br><span class="line">$ ./zk03/bin/zkServer.sh --config zk03/conf start</span><br></pre></td></tr></table></figure>

<p>检查zk集群状态：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># zk01 此时为follower</span></span><br><span class="line">$ ./zk01/bin/zkServer.sh status</span><br><span class="line">/usr/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /data/knner/zk01/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: 172.17.0.87.</span><br><span class="line">Mode: follower</span><br><span class="line"><span class="comment"># zk02 此时为leader</span></span><br><span class="line">$ ./zk02/bin/zkServer.sh status</span><br><span class="line">/usr/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /data/knner/zk02/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2182. Client address: 172.17.0.87.</span><br><span class="line">Mode: leader</span><br><span class="line"><span class="comment"># zk03 此时为follower</span></span><br><span class="line">$ ./zk03/bin/zkServer.sh status</span><br><span class="line">/usr/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /data/knner/zk03/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2183. Client address: 172.17.0.87.</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure>

<h4 id="集群可用性测试"><a href="#集群可用性测试" class="headerlink" title="集群可用性测试"></a>集群可用性测试</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 停止zk02节点</span></span><br><span class="line">$ ./zk02/bin/zkServer.sh stop</span><br><span class="line">/usr/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /data/knner/zk02/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确认进程已经退出</span></span><br><span class="line">$ ps -ef|grep zk02</span><br><span class="line">ec2-user  1364 13046  0 13:24 pts/5    00:00:00 grep --color=auto zk02</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看zk01 节点状态，follower</span></span><br><span class="line">$ ./zk01/bin/zkServer.sh status</span><br><span class="line">/usr/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /data/knner/zk01/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: 172.17.0.87.</span><br><span class="line">Mode: follower</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看zk03节点状态，此时它为leader</span></span><br><span class="line">$ ./zk03/bin/zkServer.sh status</span><br><span class="line">/usr/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /data/knner/zk03/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2183. Client address: 172.17.0.87.</span><br><span class="line">Mode: leader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过zkCli.sh 命令，连接测试：</span></span><br><span class="line">$ ./zk01/bin/zkCli.sh -server 172.17.0.87:2181</span><br><span class="line">[zk: 172.17.0.87:2181(CONNECTED) 0] <span class="built_in">help</span></span><br><span class="line">ZooKeeper -server host:port cmd args</span><br><span class="line">addauth scheme auth</span><br><span class="line">close </span><br><span class="line">config [-c] [-w] [-s]</span><br><span class="line">connect host:port</span><br><span class="line">create [-s] [-e] [-c] [-t ttl] path [data] [acl]</span><br><span class="line">delete [-v version] path</span><br><span class="line">deleteall path</span><br><span class="line">delquota [-n|-b] path</span><br><span class="line">get [-s] [-w] path</span><br><span class="line">getAcl [-s] path</span><br><span class="line"><span class="built_in">history</span> </span><br><span class="line">listquota path</span><br><span class="line">ls [-s] [-w] [-R] path</span><br><span class="line">ls2 path [watch]</span><br><span class="line">printwatches on|off</span><br><span class="line">quit </span><br><span class="line">reconfig [-s] [-v version] [[-file path] | [-members serverID=host:port1:port2;port3[,...]*]] | [-add serverId=host:port1:port2;port3[,...]]* [-remove serverId[,...]*]</span><br><span class="line">redo cmdno</span><br><span class="line">removewatches path [-c|-d|-a] [-l]</span><br><span class="line">rmr path</span><br><span class="line"><span class="built_in">set</span> [-s] [-v version] path data</span><br><span class="line">setAcl [-s] [-v version] [-R] path acl</span><br><span class="line">setquota -n|-b val path</span><br><span class="line"><span class="built_in">stat</span> [-w] path</span><br><span class="line">sync path</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此时再把zk03给停掉：</span></span><br><span class="line">$ ./zk03/bin/zkServer.sh stop</span><br><span class="line">ps -ef|grep zk03</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次查看zk01 状态，提示不再运行了</span></span><br><span class="line">$ ./zk01/bin/zkServer.sh status</span><br><span class="line">/usr/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /data/knner/zk01/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: 172.17.0.87.</span><br><span class="line">Error contacting service. It is probably not running.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们查看进程还是存在的，集群中已经挡掉了多数的zk节点，则无法提供服务了</span></span><br><span class="line">$ ps -ef|grep zk</span><br><span class="line">ec2-user 30504     1  0 13:08 pts/5    00:00:02 java -Dzookeeper.log.dir=/data/knner/zk01/bin/../logs -Dzookeeper.log.file=zookeeper-ec2-user-server-test01.dev.awsbj.cn.log -Dzookeeper.root.logger=INFO,CONSOLE -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=<span class="built_in">kill</span> -9 %p -cp /data/knner/zk01/bin/../............</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后再次启动zk02</span></span><br><span class="line">$ ./zk02/bin/zkServer.sh --config zk02/conf start</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次查看zk01，zk02状态，集群又恢复正常了。</span></span><br><span class="line">$ ./zk01/bin/zkServer.sh status</span><br><span class="line">/usr/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /data/knner/zk01/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: 172.17.0.87.</span><br><span class="line">Mode: leader</span><br><span class="line">[ec2-user@test01 knner]$ ./zk02/bin/zkServer.sh status</span><br><span class="line">/usr/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /data/knner/zk02/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2182. Client address: 172.17.0.87.</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure>

<p>经过测试，Zookeeper集群中多数zk节点存活，那么zk集群服务就是可用的。</p>
<h4 id="监管Zookeeper进程"><a href="#监管Zookeeper进程" class="headerlink" title="监管Zookeeper进程"></a>监管Zookeeper进程</h4><p>ZK服务被设计成”快速失败”，这意味着如果发生不可恢复的错误，它将关闭，退出进程。由于ZooKeeper服务群集高度可靠，这意味着尽管服务器可能宕机，但群集仍处于活动状态并正在处理请求。此外，由于群集是“自我修复”的，因此一旦失败的服务器重新启动，将自动重新加入该集合，而无需任何手动交互。</p>
<p>有一个监控程序，例如<a href="http://cr.yp.to/daemontools.html" target="_blank" rel="noopener">daemontools</a>或<a href="http://en.wikipedia.org/wiki/Service_Management_Facility" target="_blank" rel="noopener">SMF</a>或<a href="http://supervisord.org/" target="_blank" rel="noopener">Supervisor</a>，管理ZooKeeper服务器可确保该进程确实退出时将自动重启并迅速重新加入集群。</p>
<p>还建议将ZooKeeper服务器进程配置为在发生OutOfMemoryError *<em>时终止并转储其堆。这是通过分别在Linux和Windows上使用以下参数启动JVM来实现的。ZooKeeper *附带</em>的<em>zkServer.sh</em>和<em>zkServer.cmd</em>脚本设置了这些选项。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=&apos;kill -9 %p&apos;</span><br><span class="line"></span><br><span class="line">&quot;-XX:+HeapDumpOnOutOfMemoryError&quot; &quot;-XX:OnOutOfMemoryError=cmd /c taskkill /pid %%%%p /t /f&quot;</span><br></pre></td></tr></table></figure>



<h3 id="ZooKeeper命令"><a href="#ZooKeeper命令" class="headerlink" title="ZooKeeper命令"></a>ZooKeeper命令</h3><h4 id="四小写字母命令"><a href="#四小写字母命令" class="headerlink" title="四小写字母命令"></a>四小写字母命令</h4><p>ZooKeeper响应少量命令。每个命令由四个字母组成。您可以在客户端端口通过telnet或nc向ZooKeeper发出命令。</p>
<p>三个更有趣的命令：“ stat”提供有关服务器和连接的客户端的一些常规信息，而“ srvr”和“ cons”分别提供有关服务器和连接的扩展详细信息。</p>
<p><strong>3.5.3中的新增功能：</strong>四个字母词必须在使用前明确列出白色。有关详细信息，请参考<a href="https://zookeeper.apache.org/doc/r3.5.6/zookeeperAdmin.html#sc_clusterOptions" target="_blank" rel="noopener">群集配置部分中</a>描述的<strong>4lw.commands.whitelist</strong>。展望未来，不推荐使用四个字母词，请改用<a href="https://zookeeper.apache.org/doc/r3.5.6/zookeeperAdmin.html#sc_adminserver" target="_blank" rel="noopener">AdminServer</a>。</p>
<ul>
<li><p><em>conf</em>：<strong>3.3.0中的新增功能：</strong>打印有关服务配置的详细信息。</p>
</li>
<li><p><em>缺点</em>：<strong>3.3.0中的新增功能：</strong>列出了连接到该服务器的所有客户端的完整连接/会话详细信息。包括有关已接收/已发送的数据包数量，会话ID，操作等待时间，最后执行的操作等信息。</p>
</li>
<li><p><em>crst</em>：<strong>3.3.0中的新增功能：</strong>重置所有连接的连接/会话统计信息。</p>
</li>
<li><p><em>dump</em>：列出未完成的会话和临时节点。这仅适用于领导者。</p>
</li>
<li><p><em>envi</em>：打印有关服务环境的详细信息</p>
</li>
<li><p><em>ruok</em>：测试服务器是否以非错误状态运行。如果服务器正在运行，它将以imok响应。否则，它将完全不响应。响应“ imok”不一定表示服务器已加入仲裁，只是服务器进程处于活动状态并绑定到指定的客户端端口。使用“ stat”获取有关状态仲裁和客户端连接信息的详细信息。</p>
</li>
<li><p><em>srst</em>：重置服务器统计信息。</p>
</li>
<li><p><em>srvr</em>：<strong>3.3.0中的新功能：</strong>列出服务器的完整详细信息。</p>
</li>
<li><p><em>stat</em>：列出服务器和连接的客户端的简要详细信息。</p>
</li>
<li><p><em>wchs</em>：<strong>3.3.0中的新增功能：</strong>列出有关服务器<em>监视的</em>简要信息。</p>
</li>
<li><p><em>wchc</em>：<strong>3.3.0中的新增功能：</strong>按会话列出有关服务器<em>监视的</em>详细信息。这将输出具有相关监视（路径）的会话（连接）列表。请注意，根据手表的数量，此操作可能会很昂贵（即影响服务器性能），请小心使用。</p>
</li>
<li><p><em>dirs</em>：<strong>3.5.1中的新增功能：</strong>以字节为单位显示快照和日志文件的总大小</p>
</li>
<li><p><em>wchp</em>：<strong>3.3.0中的新增功能：</strong>按路径列出有关服务器<em>监视的</em>详细信息。这将输出具有关联会话的路径（znode）列表。请注意，根据手表的数量，此操作可能会很昂贵（即影响服务器性能），请小心使用。</p>
</li>
<li><p><em>mntr</em>：<strong>3.4.0中的新增功能：</strong>输出可用于监视集群运行状况的变量列表。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ echo mntr | nc localhost 2185</span><br><span class="line">zk_version 3.4.0 zk_avg_latency 0 zk_max_latency 0 zk_min_latency 0 zk_packets_received 70 zk_packets_sent 69 zk_num_alive_connections 1 zk_outstanding_requests 0 zk_server_state leader zk_znode_count 4 zk_watch_count 0 zk_ephemerals_count 0 zk_approximate_data_size 27 zk_followers 4 - only exposed by the Leader zk_synced_followers 4 - only exposed by the Leader zk_pending_syncs 0 - only exposed by the Leader zk_open_file_descriptor_count 23 - only available on Unix platforms zk_max_file_descriptor_count 1024 - only available on Unix platforms zk_last_proposal_size 23 zk_min_proposal_size 23 zk_max_proposal_size 64</span><br></pre></td></tr></table></figure>



</li>
</ul>
<p>输出与Java属性格式兼容，并且内容可能会随时间变化（添加了新键）。您的脚本应该期待更改。注意：一些密钥是特定于平台的，而某些密钥仅由Leader导出。输出包含具有以下格式的多行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">key \t value</span><br></pre></td></tr></table></figure>

<ul>
<li><p><em>isro</em>：<strong>3.4.0中的新增功能：</strong>测试服务器是否以只读模式运行。如果服务器处于只读模式，则服务器将以“ ro”响应，如果不是处于只读模式，则服务器将以“ rw”响应。</p>
</li>
<li><p><em>gtmk</em>：以十进制格式获取当前的跟踪掩码，作为64位带符号的long值。请参阅<code>stmk</code>以获取可能值的说明。</p>
</li>
<li><p><em>stmk</em>：设置当前的跟踪掩码。跟踪掩码是64位，其中每个位启用或禁用服务器上特定类别的跟踪日志记录。必须将Log4J配置为<code>TRACE</code>首先启用级别，才能查看跟踪日志记录消息。跟踪掩码的位对应于以下跟踪日志记录类别。</p>
<table>
<thead>
<tr>
<th>跟踪掩码位值</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>0b0000000000</td>
<td>未使用，保留以备将来使用。</td>
</tr>
<tr>
<td>0b0000000010</td>
<td>记录客户端请求，但不包括ping请求。</td>
</tr>
<tr>
<td>0b0000000100</td>
<td>未使用，保留以备将来使用。</td>
</tr>
<tr>
<td>0b0000001000</td>
<td>记录客户端ping请求。</td>
</tr>
<tr>
<td>0b0000010000</td>
<td>记录从作为当前领导者的仲裁对等方收到的数据包，但不包括ping请求。</td>
</tr>
<tr>
<td>0b0000100000</td>
<td>记录客户端会话的添加，删除和验证。</td>
</tr>
<tr>
<td>0b0001000000</td>
<td>记录监视事件到客户端会话的传递。</td>
</tr>
<tr>
<td>0b0010000000</td>
<td>记录从作为当前领导者的仲裁对等方收到的ping数据包。</td>
</tr>
<tr>
<td>0b0100000000</td>
<td>未使用，保留以备将来使用。</td>
</tr>
<tr>
<td>0b1000000000</td>
<td>未使用，保留以备将来使用。</td>
</tr>
</tbody></table>
<p>64位值中的所有其余位均未使用，并保留以供将来使用。通过计算记录值的按位或，可以指定多个跟踪日志记录类别。默认跟踪掩码为0b0100110010。因此，默认情况下，跟踪日志记录包括客户端请求，从领导者接收的数据包和会话。要设置其他跟踪掩码，请发送一个包含<code>stmk</code>四个字母的单词的请求，后跟一个跟踪掩码，表示为一个64位带符号的long值。本示例使用Perl <code>pack</code>函数构造一个跟踪掩码，该掩码启用上述所有跟踪日志记录类别，并将其转换为具有big-endian字节顺序的64位有符号long值。<code>stmk</code>使用netcat 将结果附加到服务器并发送到服务器。服务器以十进制格式响应新的跟踪掩码。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ perl -e <span class="string">"print 'stmk', pack('q&gt;', 0b0011111010)"</span> | nc localhost 2181 250</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>这是<strong>ruok</strong>命令的示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ echo ruok | nc 127.0.0.1 5111</span><br><span class="line">    imok</span><br></pre></td></tr></table></figure>

<h4 id="四小写字母命令测试"><a href="#四小写字母命令测试" class="headerlink" title="四小写字母命令测试"></a>四小写字母命令测试</h4><p>安装命令工具telnet，nc：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo yum install telnet nc -y</span><br><span class="line">Installed:</span><br><span class="line">nmap-ncat.x86_64 2:6.40-13.amzn2       telnet.x86_64 1:0.17-64.amzn2.0.1</span><br></pre></td></tr></table></figure>

<p>测试：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不知道telnet如何使用</span></span><br><span class="line">$ telnet 172.17.0.87 2181</span><br><span class="line">Trying 172.17.0.87...</span><br><span class="line">Connected to 172.17.0.87.</span><br><span class="line">Escape character is <span class="string">'^]'</span>.</span><br><span class="line"></span><br><span class="line">conf <span class="comment"># 输入的命令</span></span><br><span class="line">Connection closed by foreign host.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建议用nc命令，连接任何一个zk节点都可以的，端口用clientPort的端口</span></span><br><span class="line">$ <span class="built_in">echo</span> conf |nc 172.17.0.87 2181</span><br><span class="line">clientPort=2181</span><br><span class="line">secureClientPort=-1</span><br><span class="line">dataDir=/data/zkdata/zk01/version-2</span><br><span class="line">dataDirSize=67108880</span><br><span class="line">dataLogDir=/data/zklog/zk01/version-2</span><br><span class="line">dataLogSize=1625</span><br><span class="line">tickTime=2000</span><br><span class="line">maxClientCnxns=1000</span><br><span class="line">minSessionTimeout=4000</span><br><span class="line">maxSessionTimeout=40000</span><br><span class="line">serverId=1</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">electionAlg=3</span><br><span class="line">electionPort=3888</span><br><span class="line">quorumPort=2888</span><br><span class="line">peerType=0</span><br><span class="line">membership: </span><br><span class="line">server.1=zk01:2888:3888:participant</span><br><span class="line">server.2=zk02:2887:3887:participant</span><br><span class="line">server.3=zk03:2886:3886:participant</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> cons |nc 172.17.0.87 2181</span><br><span class="line"> /172.17.0.87:52088[0](queued=0,recved=1,sent=0)</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> crst |nc 172.17.0.87 2181</span><br><span class="line">Connection stats reset.</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> dump |nc 172.17.0.87 2181</span><br><span class="line">SessionTracker dump:</span><br><span class="line">Session Sets (0)/(0):</span><br><span class="line">ephemeral nodes dump:</span><br><span class="line">Sessions with Ephemerals (0):</span><br><span class="line">Connections dump:</span><br><span class="line">Connections Sets (1)/(1):</span><br><span class="line">1 expire at Fri Dec 06 13:50:21 CST 2019:</span><br><span class="line">4ip: /172.17.0.87:52154 sessionId: 0x0</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> envi |nc 172.17.0.87 2181</span><br><span class="line">Environment:</span><br><span class="line">zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT</span><br><span class="line">host.name=node-1</span><br><span class="line">java.version=1.8.0_201</span><br><span class="line">java.vendor=Oracle Corporation</span><br><span class="line">java.home=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.201.b09-0.amzn2.x86_64/jre</span><br><span class="line">java.class.path=/data/knner/zk01/bin/../zookeeper-server/target/classes:/data/knner/zk01/bin/../build/classes:/data/knner/zk01/bin/../zookeeper-server/target/lib/*.jar:/data/knner/zk01/bin/../build/lib/*.jar:/data/knner/zk01/bin/../lib/zookeeper-jute-3.5.6.jar:/data/knner/zk01/bin/../lib/zookeeper-3.5.6.jar:/data/knner/zk01/bin/../lib/slf4j-log4j12-1.7.25.jar:/data/knner/zk01/bin/../lib/slf4j-api-1.7.25.jar:/data/knner/zk01/bin/../lib/netty-transport-native-unix-common-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-transport-native-epoll-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-transport-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-resolver-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-handler-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-common-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-codec-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-buffer-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/log4j-1.2.17.jar:/data/knner/zk01/bin/../lib/json-simple-1.1.1.jar:/data/knner/zk01/bin/../lib/jline-2.11.jar:/data/knner/zk01/bin/../lib/jetty-util-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-servlet-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-server-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-security-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-io-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-http-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/javax.servlet-api-3.1.0.jar:/data/knner/zk01/bin/../lib/jackson-databind-2.9.10.jar:/data/knner/zk01/bin/../lib/jackson-core-2.9.10.jar:/data/knner/zk01/bin/../lib/jackson-annotations-2.9.10.jar:/data/knner/zk01/bin/../lib/commons-cli-1.2.jar:/data/knner/zk01/bin/../lib/audience-annotations-0.5.0.jar:/data/knner/zk01/bin/../zookeeper-*.jar:/data/knner/zk01/bin/../zookeeper-server/src/main/resources/lib/*.jar:zk01/conf:</span><br><span class="line">java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</span><br><span class="line">java.io.tmpdir=/tmp</span><br><span class="line">java.compiler=&lt;NA&gt;</span><br><span class="line">os.name=Linux</span><br><span class="line">os.arch=amd64</span><br><span class="line">os.version=4.14.77-81.59.amzn2.x86_64</span><br><span class="line">user.name=ec2-user</span><br><span class="line">user.home=/home/ec2-user</span><br><span class="line">user.dir=/data/knner</span><br><span class="line">os.memory.free=211MB</span><br><span class="line">os.memory.max=891MB</span><br><span class="line">os.memory.total=240MB</span><br><span class="line"></span><br><span class="line"><span class="comment"># are you ok? I'm ok.</span></span><br><span class="line">$ <span class="built_in">echo</span> ruok |nc 172.17.0.87 2181</span><br><span class="line">imok</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> srst |nc 172.17.0.87 2181</span><br><span class="line">Server stats reset.</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> srvr |nc 172.17.0.87 2181</span><br><span class="line">Zookeeper version: 3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT</span><br><span class="line">Latency min/avg/max: 0/0/0</span><br><span class="line">Received: 1</span><br><span class="line">Sent: 1</span><br><span class="line">Connections: 1</span><br><span class="line">Outstanding: 0</span><br><span class="line">Zxid: 0x300000000</span><br><span class="line">Mode: leader</span><br><span class="line">Node count: 5</span><br><span class="line">Proposal sizes last/min/max: -1/-1/-1</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> <span class="built_in">stat</span> |nc 172.17.0.87 2181</span><br><span class="line">Zookeeper version: 3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT</span><br><span class="line">Clients:</span><br><span class="line"> /172.17.0.87:52312[0](queued=0,recved=1,sent=0)</span><br><span class="line"></span><br><span class="line">Latency min/avg/max: 0/0/0</span><br><span class="line">Received: 2</span><br><span class="line">Sent: 2</span><br><span class="line">Connections: 1</span><br><span class="line">Outstanding: 0</span><br><span class="line">Zxid: 0x300000000</span><br><span class="line">Mode: leader</span><br><span class="line">Node count: 5</span><br><span class="line">Proposal sizes last/min/max: -1/-1/-1</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> wchs |nc 172.17.0.87 2181</span><br><span class="line">0 connections watching 0 paths</span><br><span class="line">Total watches:0</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> <span class="built_in">dirs</span> |nc 172.17.0.87 2181</span><br><span class="line">datadir_size: 67108880</span><br><span class="line">logdir_size: 1625</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> wchc |nc 172.17.0.87 2181</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> wchp |nc 172.17.0.87 2181</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> mntr |nc 172.17.0.87 2181</span><br><span class="line">zk_version	3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT</span><br><span class="line">zk_avg_latency	0</span><br><span class="line">zk_max_latency	0</span><br><span class="line">zk_min_latency	0</span><br><span class="line">zk_packets_received	7</span><br><span class="line">zk_packets_sent	7</span><br><span class="line">zk_num_alive_connections	1</span><br><span class="line">zk_outstanding_requests	0</span><br><span class="line">zk_server_state	leader</span><br><span class="line">zk_znode_count	5</span><br><span class="line">zk_watch_count	0</span><br><span class="line">zk_ephemerals_count	0</span><br><span class="line">zk_approximate_data_size	161</span><br><span class="line">zk_open_file_descriptor_count	63</span><br><span class="line">zk_max_file_descriptor_count	65536</span><br><span class="line">zk_followers	2</span><br><span class="line">zk_synced_followers	2</span><br><span class="line">zk_pending_syncs	0</span><br><span class="line">zk_last_proposal_size	-1</span><br><span class="line">zk_max_proposal_size	-1</span><br><span class="line">zk_min_proposal_size	-1</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> isro |nc 172.17.0.87 2181</span><br><span class="line">rw</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> gtmk |nc 172.17.0.87 2181</span><br><span class="line">306</span><br></pre></td></tr></table></figure>



<h4 id="AdminServer"><a href="#AdminServer" class="headerlink" title="AdminServer"></a>AdminServer</h4><p><strong>3.5.0中的新增功能：</strong> AdminServer是嵌入式Jetty服务器，为四个字母单词命令提供HTTP接口。默认情况下，服务器在端口8080上启动，并且通过访问URL<code>/commands/[command name]</code>发出命令，例如<code>http://localhost:8080/commands/stat</code>。命令响应作为JSON返回。与原始协议不同，命令不限于四个字母的名称，命令可以具有多个名称。例如，<code>stmk</code>也可以称为<code>set_trace_mask</code>。要查看所有可用命令的列表，请将浏览器指向<code>URL/commands</code>（例如，<code>http://localhost:8080/commands</code>）。请参阅 <a href="https://zookeeper.apache.org/doc/r3.5.6/zookeeperAdmin.html#sc_adminserver_config" target="_blank" rel="noopener">AdminServer配置选项，</a>以了解如何更改端口和URL。</p>
<p>AdminServer默认情况下处于启用状态，但可以通过以下任一方式禁用：</p>
<ul>
<li>将zookeeper.admin.enableServer系统属性设置为false。</li>
<li>从类路径中删除Jetty。（如果您想覆盖ZooKeeper的码头依赖，则此选项很有用。）</li>
</ul>
<p>请注意，如果AdminServer被禁用，则TCP四字母词接口仍然可用。</p>
<h4 id="AdminServer-测试"><a href="#AdminServer-测试" class="headerlink" title="AdminServer 测试"></a>AdminServer 测试</h4><p>注意：默认的path路径是/commands，而我这里在配置文件中将其改为/knner了，所以：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"># 格式：172.17.0.87:8081/knner/xxxx</span><br><span class="line">$ curl -XGET 172.17.0.87:8081/knner/stat</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"version"</span> : <span class="string">"3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT"</span>,</span><br><span class="line">  <span class="attr">"read_only"</span> : <span class="literal">false</span>,</span><br><span class="line">  <span class="attr">"server_stats"</span> : &#123;</span><br><span class="line">    <span class="attr">"packets_sent"</span> : <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"packets_received"</span> : <span class="number">9</span>,</span><br><span class="line">    <span class="attr">"max_latency"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="attr">"min_latency"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="attr">"fsync_threshold_exceed_count"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="attr">"client_response_stats"</span> : &#123;</span><br><span class="line">      <span class="attr">"last_buffer_size"</span> : <span class="number">-1</span>,</span><br><span class="line">      <span class="attr">"min_buffer_size"</span> : <span class="number">-1</span>,</span><br><span class="line">      <span class="attr">"max_buffer_size"</span> : <span class="number">-1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"provider_null"</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"num_alive_client_connections"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="attr">"server_state"</span> : <span class="string">"leader"</span>,</span><br><span class="line">    <span class="attr">"outstanding_requests"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="attr">"avg_latency"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="attr">"data_dir_size"</span> : <span class="number">67108880</span>,</span><br><span class="line">    <span class="attr">"log_dir_size"</span> : <span class="number">1625</span>,</span><br><span class="line">    <span class="attr">"last_processed_zxid"</span> : <span class="number">12884901888</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"client_response"</span> : &#123;</span><br><span class="line">    <span class="attr">"last_buffer_size"</span> : <span class="number">-1</span>,</span><br><span class="line">    <span class="attr">"min_buffer_size"</span> : <span class="number">-1</span>,</span><br><span class="line">    <span class="attr">"max_buffer_size"</span> : <span class="number">-1</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"proposal_stats"</span> : &#123;</span><br><span class="line">    <span class="attr">"last_buffer_size"</span> : <span class="number">-1</span>,</span><br><span class="line">    <span class="attr">"min_buffer_size"</span> : <span class="number">-1</span>,</span><br><span class="line">    <span class="attr">"max_buffer_size"</span> : <span class="number">-1</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"node_count"</span> : <span class="number">5</span>,</span><br><span class="line">  <span class="attr">"connections"</span> : [ ],</span><br><span class="line">  <span class="attr">"command"</span> : <span class="string">"stats"</span>,</span><br><span class="line">  <span class="attr">"error"</span> : <span class="literal">null</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 查看命令：</span><br><span class="line">$ curl -XGET 172.17.0.87:8081/knner</span><br><span class="line">&lt;a href="/knner/configuration"&gt;configuration&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/connection_stat_reset"&gt;connection_stat_reset&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/connections"&gt;connections&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/dirs"&gt;dirs&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/dump"&gt;dump&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/environment"&gt;environment&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/get_trace_mask"&gt;get_trace_mask&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/is_read_only"&gt;is_read_only&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/monitor"&gt;monitor&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/ruok"&gt;ruok&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/server_stats"&gt;server_stats&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/set_trace_mask"&gt;set_trace_mask&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/stat_reset"&gt;stat_reset&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/stats"&gt;stats&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/watch_summary"&gt;watch_summary&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/watches"&gt;watches&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;a href="/knner/watches_by_path"&gt;watches_by_path&lt;/a&gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line"></span><br><span class="line"># 可以写成4字母的，或者全称</span><br><span class="line">$ curl -XGET 172.17.0.87:8081/knner/environment</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"zookeeper.version"</span> : <span class="string">"3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT"</span>,</span><br><span class="line">  <span class="attr">"host.name"</span> : <span class="string">"node-1"</span>,</span><br><span class="line">.....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Troubleshooting-故障排查"><a href="#Troubleshooting-故障排查" class="headerlink" title="Troubleshooting 故障排查"></a>Troubleshooting 故障排查</h3><ul>
<li><p><em>服务器由于文件损坏</em> 而无法启动：由于ZooKeeper服务器的事务日志中的某些文件损坏，服务器可能无法读取其数据库并无法启动。在加载ZooKeeper数据库时，您将看到一些IOException。在这种情况下，请确保您集合中的所有其他服务器都已启动并正常工作。在命令端口上使用<code>stat</code>命令查看它们是否状况良好。确认集成中的所有其他服务器都已启动后，可以继续清理损坏的服务器的数据库。删除<code>datadir/version-2</code>和<code>datalogdir/version-2/</code>中的所有文件。重新启动服务器。</p>
</li>
<li><h4 id="恢复-TxnLogToolkit-恢复带有损坏CRC的事务日志条目"><a href="#恢复-TxnLogToolkit-恢复带有损坏CRC的事务日志条目" class="headerlink" title="恢复-TxnLogToolkit 恢复带有损坏CRC的事务日志条目"></a>恢复-TxnLogToolkit 恢复带有损坏CRC的事务日志条目</h4><p>TxnLogToolkit是ZooKeeper附带的命令行工具，能够恢复带有损坏CRC的事务日志条目。</p>
<p>在不使用任何命令行参数或不使用参数的情况下运行它<code>-h,--help</code>，它将输出以下帮助页面：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ bin/zkTxnLogToolkit.sh</span><br><span class="line">usage: TxnLogToolkit [-dhrv] txn_log_file_name</span><br><span class="line">-d,--dump      Dump mode. Dump all entries of the <span class="built_in">log</span> file. (this is the default)</span><br><span class="line">-h,--<span class="built_in">help</span>      Print <span class="built_in">help</span> message</span><br><span class="line">-r,--recover   Recovery mode. Re-calculate CRC <span class="keyword">for</span> broken entries.</span><br><span class="line">-v,--verbose   Be verbose <span class="keyword">in</span> recovery mode: <span class="built_in">print</span> all entries, not just fixed ones.</span><br><span class="line">-y,--yes       Non-interactive mode: repair all CRC errors without asking</span><br></pre></td></tr></table></figure>

<p>默认行为是安全的：将给定事务日志文件的条目转储到屏幕上：（与using <code>-d,--dump</code>参数相同）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ bin/zkTxnLogToolkit.sh log.100000001</span><br><span class="line">ZooKeeper Transactional Log File with dbid 0 txnlog format version 2</span><br><span class="line">4/5/18 2:15:58 PM CEST session 0x16295bafcc40000 cxid 0x0 zxid 0x100000001 createSession 30000</span><br><span class="line">CRC ERROR - 4/5/18 2:16:05 PM CEST session 0x16295bafcc40000 cxid 0x1 zxid 0x100000002 closeSession null <span class="comment"># 这里有错误</span></span><br><span class="line">4/5/18 2:16:05 PM CEST session 0x16295bafcc40000 cxid 0x1 zxid 0x100000002 closeSession null</span><br><span class="line">4/5/18 2:16:12 PM CEST session 0x26295bafcc90000 cxid 0x0 zxid 0x100000003 createSession 30000</span><br><span class="line">4/5/18 2:17:34 PM CEST session 0x26295bafcc90000 cxid 0x0 zxid 0x200000001 closeSession null</span><br><span class="line">4/5/18 2:17:34 PM CEST session 0x16295bd23720000 cxid 0x0 zxid 0x200000002 createSession 30000</span><br><span class="line">4/5/18 2:18:02 PM CEST session 0x16295bd23720000 cxid 0x2 zxid 0x200000003 create <span class="string">'/andor,#626262,v&#123;s&#123;31,s&#123;'</span>world,<span class="string">'anyone&#125;&#125;&#125;,F,1</span></span><br><span class="line"><span class="string">EOF reached after 6 txns.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 没有错误的</span></span><br><span class="line"><span class="string">$ ./zk01/bin/zkTxnLogToolkit.sh /data/zklog/zk01/version-2/log.200000001 </span></span><br><span class="line"><span class="string">/usr/bin/java</span></span><br><span class="line"><span class="string">ZooKeeper Transactional Log File with dbid 0 txnlog format version 2</span></span><br><span class="line"><span class="string">12/6/19 1:27:35 PM CST session 0x1028877ea6a0000 cxid 0x0 zxid 0x200000001 createSession 30000</span></span><br><span class="line"><span class="string">12/6/19 1:27:42 PM CST session 0x1028877ea6a0000 cxid 0x1 zxid 0x200000002 closeSession </span></span><br><span class="line"><span class="string">EOF reached after 2 txns.</span></span><br></pre></td></tr></table></figure>

<p>上面的事务日志文件的第二个条目中有一个CRC错误。在<strong>转储</strong>模式下，该工具包仅将此信息打印到屏幕上，而无需触摸原始文件。在<strong>恢复</strong>模式（<code>-r,--recover</code>标志）下，原始文件仍然保持不变，所有事务将被复制到后缀为“ .fixed”的新txn日志文件中。如果它与原始txn条目不匹配，它将重新计算CRC值并复制计算出的值。默认情况下，该工具以交互方式工作：遇到CRC错误时，它会要求进行确认。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ bin/zkTxnLogToolkit.sh -r log.100000001</span><br><span class="line">ZooKeeper Transactional Log File with dbid 0 txnlog format version 2</span><br><span class="line">CRC ERROR - 4/5/18 2:16:05 PM CEST session 0x16295bafcc40000 cxid 0x1 zxid 0x100000002 closeSession null</span><br><span class="line">Would you like to fix it (Yes/No/Abort) ?</span><br></pre></td></tr></table></figure>

<p>回答“ <strong>是”</strong>意味着新计算的CRC值将输出到新文件。<strong>No</strong>表示将复制原始CRC值。<strong>中止</strong>将中止整个操作并退出。（在这种情况下，“。fixed”将不会被删除，而是处于半完成状态：仅包含已经处理过的条目，或者如果操作在第一个条目中止，则仅包含标头。）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ bin/zkTxnLogToolkit.sh -r log.100000001</span><br><span class="line">ZooKeeper Transactional Log File with dbid 0 txnlog format version 2</span><br><span class="line">CRC ERROR - 4/5/18 2:16:05 PM CEST session 0x16295bafcc40000 cxid 0x1 zxid 0x100000002 closeSession null</span><br><span class="line">Would you like to fix it (Yes/No/Abort) ? y</span><br><span class="line">EOF reached after 6 txns.</span><br><span class="line">Recovery file log.100000001.fixed has been written with 1 fixed CRC error(s)</span><br></pre></td></tr></table></figure>

<p>恢复的默认行为是保持沉默：仅将具有CRC错误的条目打印到屏幕上。可以使用<code>-v,--verbose</code>参数打开详细模式以查看所有记录。可以使用<code>-y,--yes</code>参数关闭交互模式。在这种情况下，所有CRC错误都将在新的事务文件中修复。</p>
</li>
</ul>
<h3 id="ZK-datadir-datalogdir-目录结构"><a href="#ZK-datadir-datalogdir-目录结构" class="headerlink" title="ZK datadir datalogdir 目录结构"></a>ZK datadir datalogdir 目录结构</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ tree /data/zkdata/zk01/</span><br><span class="line">/data/zkdata/zk01/</span><br><span class="line">├── myid <span class="comment"># 人类可读的ASCII文本中包含一个表示服务器ID的整数</span></span><br><span class="line">├── version-2 <span class="comment"># 保存数据树的模糊快照</span></span><br><span class="line">│   ├── acceptedEpoch</span><br><span class="line">│   ├── currentEpoch</span><br><span class="line">│   ├── snapshot.0</span><br><span class="line">│   ├── snapshot.100000000</span><br><span class="line">│   └── snapshot.200000002</span><br><span class="line">└── zookeeper_server.pid</span><br><span class="line"></span><br><span class="line">1 directory, 7 files</span><br><span class="line"><span class="comment"># 每个ZooKeeper服务器都有一个唯一的ID。此id在两个地方使用：myid文件和配置文件。该身份识别码文件标识服务器对应于给定的数据目录。配置文件列出了由服务器ID标识的每个服务器的联系信息。当ZooKeeper服务器实例启动时，它会从myid文件中读取其ID ，然后使用该ID从配置文件中读取，并查找其应侦听的端口。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从某种意义上说，存储在数据目录中的快照文件是模糊快照，即在ZooKeeper服务器获取快照的过程中，正在对数据树进行更新。快照文件名的后缀是zxid快照开始时最后提交的事务的ZooKeeper事务ID。因此，快照包括在快照进行过程中发生的数据树更新的子集。因此，快照可能不对应于实际存在的任何数据树，因此，我们将其称为模糊快照。尽管如此，ZooKeeper仍可以使用此快照进行恢复，因为它利用了其更新的幂等性质。通过针对模糊快照重播事务日志，ZooKeeper可以在日志末尾获取系统状态。</span></span><br><span class="line"></span><br><span class="line">$ tree /data/zklog/zk01/</span><br><span class="line">/data/zklog/zk01/</span><br><span class="line">└── version-2</span><br><span class="line">    └── log.200000001</span><br><span class="line"></span><br><span class="line">1 directory, 1 file</span><br><span class="line"></span><br><span class="line"><span class="comment"># 日志目录包含ZooKeeper事务日志。在进行任何更新之前，ZooKeeper确保将代表更新的事务写入非易失性存储。当写入当前日志文件的事务数达到（可变）阈值时，将启动一个新的日志文件。使用影响快照频率的相同参数计算阈值（请参见上面的snapCount）。日志文件的后缀是写入该日志的第一个zxid。</span></span><br><span class="line"><span class="comment"># 在独立的ZooKeeper服务器和复制的ZooKeeper服务器的不同配置之间，快照和日志文件的格式不会更改。因此，您可以将这些文件从运行中的复制的ZooKeeper服务器拉到带有独立ZooKeeper服务器的开发计算机上，以进行故障排除。</span></span><br></pre></td></tr></table></figure>



<h3 id="zkCli-使用测试"><a href="#zkCli-使用测试" class="headerlink" title="zkCli 使用测试"></a>zkCli 使用测试</h3><p>客户端命名行：<code>bin/zkCli.sh</code></p>
<p>查看命令帮助：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./zk01/bin/zkCli.sh <span class="built_in">help</span></span><br><span class="line">/usr/bin/java</span><br><span class="line">Connecting to localhost:2181 <span class="comment"># 默认连接的地址</span></span><br><span class="line">2019-12-06 14:42:10,584 [myid:] - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT</span><br><span class="line">2019-12-06 14:42:10,588 [myid:] - INFO  [main:Environment@109] - Client environment:host.name=node-1</span><br><span class="line">2019-12-06 14:42:10,589 [myid:] - INFO  [main:Environment@109] - Client environment:java.version=1.8.0_201</span><br><span class="line">2019-12-06 14:42:10,591 [myid:] - INFO  [main:Environment@109] - Client environment:java.vendor=Oracle Corporation</span><br><span class="line">2019-12-06 14:42:10,591 [myid:] - INFO  [main:Environment@109] - Client environment:java.home=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.201.b09-0.amzn2.x86_64/jre</span><br><span class="line">2019-12-06 14:42:10,591 [myid:] - INFO  [main:Environment@109] - Client environment:java.class.path=/data/knner/zk01/bin/../zookeeper-server/target/classes:/data/knner/zk01/bin/../build/classes:/data/knner/zk01/bin/../zookeeper-server/target/lib/*.jar:/data/knner/zk01/bin/../build/lib/*.jar:/data/knner/zk01/bin/../lib/zookeeper-jute-3.5.6.jar:/data/knner/zk01/bin/../lib/zookeeper-3.5.6.jar:/data/knner/zk01/bin/../lib/slf4j-log4j12-1.7.25.jar:/data/knner/zk01/bin/../lib/slf4j-api-1.7.25.jar:/data/knner/zk01/bin/../lib/netty-transport-native-unix-common-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-transport-native-epoll-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-transport-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-resolver-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-handler-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-common-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-codec-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/netty-buffer-4.1.42.Final.jar:/data/knner/zk01/bin/../lib/log4j-1.2.17.jar:/data/knner/zk01/bin/../lib/json-simple-1.1.1.jar:/data/knner/zk01/bin/../lib/jline-2.11.jar:/data/knner/zk01/bin/../lib/jetty-util-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-servlet-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-server-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-security-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-io-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/jetty-http-9.4.17.v20190418.jar:/data/knner/zk01/bin/../lib/javax.servlet-api-3.1.0.jar:/data/knner/zk01/bin/../lib/jackson-databind-2.9.10.jar:/data/knner/zk01/bin/../lib/jackson-core-2.9.10.jar:/data/knner/zk01/bin/../lib/jackson-annotations-2.9.10.jar:/data/knner/zk01/bin/../lib/commons-cli-1.2.jar:/data/knner/zk01/bin/../lib/audience-annotations-0.5.0.jar:/data/knner/zk01/bin/../zookeeper-*.jar:/data/knner/zk01/bin/../zookeeper-server/src/main/resources/lib/*.jar:/data/knner/zk01/bin/../conf:</span><br><span class="line">2019-12-06 14:42:10,592 [myid:] - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</span><br><span class="line">2019-12-06 14:42:10,592 [myid:] - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp</span><br><span class="line">2019-12-06 14:42:10,592 [myid:] - INFO  [main:Environment@109] - Client environment:java.compiler=&lt;NA&gt;</span><br><span class="line">2019-12-06 14:42:10,592 [myid:] - INFO  [main:Environment@109] - Client environment:os.name=Linux</span><br><span class="line">2019-12-06 14:42:10,592 [myid:] - INFO  [main:Environment@109] - Client environment:os.arch=amd64</span><br><span class="line">2019-12-06 14:42:10,593 [myid:] - INFO  [main:Environment@109] - Client environment:os.version=4.14.77-81.59.amzn2.x86_64</span><br><span class="line">2019-12-06 14:42:10,593 [myid:] - INFO  [main:Environment@109] - Client environment:user.name=ec2-user</span><br><span class="line">2019-12-06 14:42:10,593 [myid:] - INFO  [main:Environment@109] - Client environment:user.home=/home/ec2-user</span><br><span class="line">2019-12-06 14:42:10,593 [myid:] - INFO  [main:Environment@109] - Client environment:user.dir=/data/knner</span><br><span class="line">2019-12-06 14:42:10,593 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.free=233MB</span><br><span class="line">2019-12-06 14:42:10,595 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.max=240MB</span><br><span class="line">2019-12-06 14:42:10,596 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.total=240MB</span><br><span class="line">2019-12-06 14:42:10,599 [myid:] - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain<span class="variable">$MyWatcher</span>@3b764bce</span><br><span class="line">2019-12-06 14:42:10,606 [myid:] - INFO  [main:X509Util@79] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=<span class="literal">true</span> to <span class="built_in">disable</span> client-initiated TLS renegotiation</span><br><span class="line">2019-12-06 14:42:10,613 [myid:] - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes</span><br><span class="line">2019-12-06 14:42:10,623 [myid:] - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=</span><br><span class="line"></span><br><span class="line"><span class="comment"># 帮助：</span></span><br><span class="line"><span class="comment"># 格式：zkCli.sh -server host:port cmd args</span></span><br><span class="line">$ ./zk01/bin/zkCli.sh -server 172.17.0.87:2181</span><br><span class="line">[zk: 172.17.0.87:2181(CONNECTED) 0] <span class="built_in">help</span></span><br><span class="line">ZooKeeper -server host:port cmd args</span><br><span class="line">addauth scheme auth</span><br><span class="line">close </span><br><span class="line">config [-c] [-w] [-s]</span><br><span class="line">connect host:port</span><br><span class="line">create [-s] [-e] [-c] [-t ttl] path [data] [acl]</span><br><span class="line">delete [-v version] path</span><br><span class="line">deleteall path</span><br><span class="line">delquota [-n|-b] path</span><br><span class="line">get [-s] [-w] path</span><br><span class="line">getAcl [-s] path</span><br><span class="line"><span class="built_in">history</span> </span><br><span class="line">listquota path</span><br><span class="line">ls [-s] [-w] [-R] path</span><br><span class="line">ls2 path [watch]</span><br><span class="line">printwatches on|off</span><br><span class="line">quit </span><br><span class="line">reconfig [-s] [-v version] [[-file path] | [-members serverID=host:port1:port2;port3[,...]*]] | [-add serverId=host:port1:port2;port3[,...]]* [-remove serverId[,...]*]</span><br><span class="line">redo cmdno</span><br><span class="line">removewatches path [-c|-d|-a] [-l]</span><br><span class="line">rmr path</span><br><span class="line"><span class="built_in">set</span> [-s] [-v version] path data</span><br><span class="line">setAcl [-s] [-v version] [-R] path acl</span><br><span class="line">setquota -n|-b val path</span><br><span class="line"><span class="built_in">stat</span> [-w] path</span><br><span class="line">sync path</span><br><span class="line"></span><br><span class="line">例如：</span><br><span class="line">$ ./zk01/bin/zkCli.sh -server 172.17.0.87:2181 ls /</span><br><span class="line">/usr/bin/java</span><br><span class="line">Connecting to 172.17.0.87:2181</span><br><span class="line">......省略</span><br><span class="line">[zookeeper]</span><br></pre></td></tr></table></figure>



<h2 id="Kafka："><a href="#Kafka：" class="headerlink" title="Kafka："></a>Kafka：</h2><p>官网：</p>
<p><a href="https://kafka.apache.org/" target="_blank" rel="noopener">https://kafka.apache.org/</a></p>
<p>文档：</p>
<p><a href="https://kafka.apache.org/documentation" target="_blank" rel="noopener">https://kafka.apache.org/documentation</a></p>
<p>下载：</p>
<p><a href="https://kafka.apache.org/downloads" target="_blank" rel="noopener">https://kafka.apache.org/downloads</a></p>
<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><blockquote>
<p>参考：<a href="https://kafka.apache.org/intro" target="_blank" rel="noopener">https://kafka.apache.org/intro</a></p>
</blockquote>
<p>Kafka是一个分布式的流处理平台（distributed streaming platform）；</p>
<p>具有三个关键功能：</p>
<ul>
<li>发布和订阅记录流，类似与消息队列</li>
<li>以容错的持久方式存储记录流</li>
<li>处理记录流</li>
</ul>
<p>几个概念：</p>
<ul>
<li>Kafka可以在一个或者多个可以跨多个数据中心的服务器上作为集群运行</li>
<li>Kafka集群将记录流存储在成为Topic的类别中</li>
<li>每个记录由一个键，一个值和一个时间戳组成</li>
</ul>
<p>四个核心API：</p>
<ul>
<li><a href="https://kafka.apache.org/documentation.html#producerapi" target="_blank" rel="noopener">Producer API</a> 允许应用程序发布记录流到一个或者多个topic主题中。</li>
<li><a href="https://kafka.apache.org/documentation.html#consumerapi" target="_blank" rel="noopener">Consumer API</a> 运行应用程序订阅一个或者多个Topic主题，并处理所产生的对他们记录的数据流。</li>
<li><a href="https://kafka.apache.org/documentation/streams" target="_blank" rel="noopener">Streams API</a> 允许应用程序充当流处理器，从一个或多个主题消耗的输入流，并产生一个输出流至一个或多个输出的Topic。</li>
<li><a href="https://kafka.apache.org/documentation.html#connect" target="_blank" rel="noopener">Connector API</a> 允许构建和运行可重复使用的生产者或消费者连接Kafka 主题，以现有的应用程序或数据系统。例如，关系数据库的连接器可能会捕获对表的所有更改。</li>
</ul>
<p><img src="https://imgs.knner.wang/images/install-and-config-zookeeper-and-kafka-cluster/kafka-apis.png" alt></p>
<p>在Kafka中，客户端和服务器之间的通信是通过简单，高性能，与语言无关的<a href="https://kafka.apache.org/protocol.html" target="_blank" rel="noopener">TCP协议完成的</a>。</p>
<h4 id="主题和日志-Topics-and-Logs"><a href="#主题和日志-Topics-and-Logs" class="headerlink" title="主题和日志 Topics and Logs"></a>主题和日志 Topics and Logs</h4><p>Kafka提供的记录主题的核心抽象。</p>
<p>主题是将记录发布到的类别或订阅源名称。Kafka中的主题始终是多用户的；也就是说，一个主题可以有零个，一个或多个消费者来订阅写入该主题的数据。</p>
<p>对于每个主题，Kafka集群都会维护一个分区日志，如下所示：</p>
<p><img src="https://imgs.knner.wang/images/install-and-config-zookeeper-and-kafka-cluster/log_anatomy.png" alt></p>
<p>每个分区都是有序的，不变的记录序列，这些记录连续地附加到结构化的提交日志中。每个分区中的记录都分配有一个称为<em>偏移</em>的顺序ID号，该ID 唯一地标识分区中的每个记录。</p>
<p>Kafka集群使用可配置的保留期限持久地保留所有已发布的记录（无论是否已使用它们）。例如，如果将保留策略设置为两天，则在发布记录后的两天内，该记录可供使用，之后将被丢弃以释放空间。Kafka的性能相对于数据大小实际上是恒定的，因此长时间存储数据不是问题。</p>
<p><img src="https://imgs.knner.wang/images/install-and-config-zookeeper-and-kafka-cluster/log_consumer.png" alt></p>
<p>实际上，基于每个消费者保留的唯一元数据是该消费者在日志中的偏移量或位置。此偏移量由使用者控制：通常，使用者在读取记录时会线性地推进其偏移量，但是实际上，由于位置是由使用者控制的，因此它可以按喜欢的任何顺序使用记录。例如，使用者可以重置到较旧的偏移量以重新处理过去的数据，或者跳到最近的记录并从“现在”开始使用。</p>
<p>这些功能的组合意味着Kafka的消费者非常便宜-他们来来去去对集群或其他消费者没有太大影响。例如，您可以使用我们的命令行工具来“尾部”任何主题的内容，而无需更改任何现有使用者所消耗的内容。</p>
<p>日志中的分区有多种用途。首先，它们允许日志扩展到超出单个服务器所能容纳的大小。每个单独的分区都必须适合托管它的服务器，但是一个主题可能有很多分区，因此它可以处理任意数量的数据。其次，它们充当并行性的单元–稍有更多。</p>
<h4 id="分布"><a href="#分布" class="headerlink" title="分布"></a>分布</h4><p>日志的分区分布在Kafka群集中的服务器上，每个服务器处理数据并要求共享分区。每个分区都在可配置数量的服务器之间复制，以实现容错功能。</p>
<p>每个分区都有一个充当“领导者”的服务器和零个或多个充当“跟随者”的服务器。领导者处理对分区的所有读写请求，而跟随者则被动地复制领导者。如果领导者失败，则跟随者之一将自动成为新领导者。每个服务器充当其某些分区的领导者，而充当其他分区的跟随者，因此群集中的负载得到了很好的平衡。</p>
<h4 id="地址复制"><a href="#地址复制" class="headerlink" title="地址复制"></a>地址复制</h4><p>Kafka MirrorMaker为您的集群提供地理复制支持。使用MirrorMaker，可以在多个数据中心或云区域之间复制消息。您可以在主动/被动方案中使用它进行备份和恢复。或在主动/主动方案中将数据放置在离您的用户更近的位置，或支持数据位置要求。</p>
<h4 id="生产者-Producers"><a href="#生产者-Producers" class="headerlink" title="生产者 Producers"></a>生产者 Producers</h4><p>生产者将数据发布到他们选择的主题。生产者负责选择将哪个记录分配给主题中的哪个分区。可以以循环方式完成此操作，仅是为了平衡负载，也可以根据某些语义分区功能（例如基于记录中的某些键）进行此操作。</p>
<h4 id="消费者-Consumers"><a href="#消费者-Consumers" class="headerlink" title="消费者 Consumers"></a>消费者 Consumers</h4><p>消费者使用<strong>消费者组</strong>名称标记自己，并且发布到主题的每条记录都会传递到<strong>每个订阅消费者组中的<em>一个</em>消费者实例</strong>。使用者实例可以在单独的进程中或在单独的机器上。</p>
<p>如果所有使用者实例都具有相同的使用者组，那么将在这些使用者实例上有效地平衡记录。</p>
<p>如果所有使用者实例具有不同的使用者组，则每条记录将广播到所有使用者进程</p>
<p><img src="https://imgs.knner.wang/images/install-and-config-zookeeper-and-kafka-cluster/consumer-groups.png" alt></p>
<p>一个由两台服务器组成的Kafka群集，其中包含四个带有两个使用者组的分区（P0-P3）。使用者组A有两个使用者实例，组B有四个。</p>
<p>但是，更常见的是，我们发现主题具有少量的消费者组，每个“逻辑订户”一个。每个组均由许多使用者实例组成，以实现可伸缩性和容错能力。这无非就是发布-订阅语义，其中订阅者是消费者的集群而不是单个进程。</p>
<p>在Kafka中实现消耗的方式是通过在消费者实例上划分日志中的分区，以便每个实例在任何时间点都是分区“公平份额”的排他消费者。Kafka协议动态处理了维护组成员身份的过程。如果新实例加入该组，它们将接管该组其他成员的某些分区；如果实例死亡，则其分区将分配给其余实例。</p>
<p>卡夫卡只提供了记录的总订单<em>中的</em>一个分区，而不是一个主题的不同分区之间。对于大多数应用程序，按分区排序以及按键对数据进行分区的能力就足够了。但是，如果您需要记录的总订单量，则可以通过只有一个分区的主题来实现，尽管这将意味着每个使用者组只有一个使用者进程。</p>
<p>例如：</p>
<p>一个Topic中，分区为4，</p>
<ul>
<li><p>那么你启动一个消费者组中有4个消费者实例，那么每个消费者实例都会分配一个分区，各自的消费者实例只会消费分配给自己的分区，当一个消费者实例宕机了，那么分配给该实例的分区会被分配给剩余的一个消费者实例，此时会变成一个消费者实例消费两个分区，另外两个消费者各自消费一个分区。</p>
</li>
<li><p>当你启动一个消费者组中包含2个消费者实例，那么每个消费者实例会分配到两个分区。</p>
<ul>
<li>当有一个实例宕机了，那么此时只有一个消费者实例消费全部的4个分区</li>
<li>当有一个新的实例加入到这个消费者组中，此时会将之前两个消费者实例分配的两个分区中拿一个给新的消费者实例。此时一个消费者实例消费两个分区，另外两个消费者实例各自消费一个分区。</li>
</ul>
</li>
<li><p>当你启动一个消费者组中有5个消费者实例，那么四个消费者实例各自分配一个分区进行消费，另一个消费者实例处于闲置状态，只有当其他消费者实例宕机了，他才会消费这个宕机实例所分配的分区，起到备份的作用。</p>
</li>
</ul>
<h4 id="消息复制与分区"><a href="#消息复制与分区" class="headerlink" title="消息复制与分区"></a>消息复制与分区</h4><ul>
<li>一个主题可以包含多个分区。kafka无法在整个主体范围内保证消息的顺序，但是可以保证消息在单个分区内的顺序</li>
<li>Kafka在物理上把Topic分成一个或多个Partition，每个Partiton在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。每个分片分区日志都放在Kafka日志目录下的自己的文件夹中。这类文件夹的名称由主题名称(由破折号(-)和分区id组成)组成。由于典型的文件夹名称长度不能超过255个字符，因此对主题名称的长度有限制。我们假设分区的数量永远不会超过100,000个。因此，主题名不能超过249个字符。这在文件夹名中只留下了足够的空间来放置一个dash和一个可能有5位数字的长分区id。</li>
<li>Kafka通过分区设计可以实现数据冗余和伸缩，分区可以分布在不同的服务器上，以此为高并发提供可能。</li>
<li>消息复制指的是每一个分区都可能会有一个或者多个副本，其中有一个副本会被推选为领袖节点，其余的落选的为从节点。其中领袖节点将会跟踪与其保持同步的副本列表，该列表称为<code>ISR（In-Sync Replica）</code>。</li>
<li>分区的数量有几个影响：<ul>
<li>每个分区必须被一个服务器处理，不可能一个分区同时被服务器两个处理。例如有20个分区，那么整个集群集将由不超过20个服务器处理。</li>
<li>分区的个数影响使用者的最大并发数。</li>
</ul>
</li>
</ul>
<h4 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h4><ul>
<li>Broker 一个Kafka实例</li>
<li>集群 多个Broker组成的一个Kafka集群</li>
<li>Topic 主题，一个逻辑的分组</li>
<li>Partition 分区，一个Topic中有多个分区，每个分区可以多个副本数（Replica），实现容错功能，每个分区都是有序的，不变的记录序列。每个分区都有一个充当领导者的服务器和多个充当跟随者的服务器。领导者处理分区的所有读写，跟随者只是被动的复制领导者，当领导者宕机了，其中一个跟随者会成为新的领导者。</li>
<li>消息 Kafka中的数据单元，通过topic–&gt;Partition–&gt;offset可以找到在Kafka中唯一对应的一条消息。</li>
<li>生产者 往Kafka中写入数据</li>
<li>消费者 从Kafka里读取数据进行处理</li>
</ul>
<h4 id="保证"><a href="#保证" class="headerlink" title="保证"></a>保证</h4><p>在较高级别上，Kafka提供以下保证：</p>
<ul>
<li>生产者发送到特定主题分区的消息将按其发送顺序附加。也就是说，如果记录M1是由与记录M2相同的生产者发送的，并且首先发送M1，则M1的偏移量将小于M2，并在日志中更早地出现。</li>
<li>消费者实例按记录在日志中的存储顺序查看记录。</li>
<li>对于复制因子为N的主题，我们最多可以容忍N-1个服务器故障，而不会丢失提交给日志的任何记录。</li>
</ul>
<h4 id="Kafka作为消息传递系统"><a href="#Kafka作为消息传递系统" class="headerlink" title="Kafka作为消息传递系统"></a>Kafka作为消息传递系统</h4><p>传统上，消息传递具有两种模型：<a href="http://en.wikipedia.org/wiki/Message_queue" target="_blank" rel="noopener">队列</a>和<a href="http://en.wikipedia.org/wiki/Publish–subscribe_pattern" target="_blank" rel="noopener">发布-订阅</a>。在队列中，一组使用者可以从服务器读取数据，并且每条记录都将转到其中一个。在发布-订阅记录中广播给所有消费者。这两个模型中的每一个都有优点和缺点。排队的优势在于，它允许您将数据处理划分到多个使用者实例上，从而扩展处理量。不幸的是，队列不是多用户的—一次进程读取了丢失的数据。发布－订阅允许您将数据广播到多个进程，但是由于每条消息都传递给每个订阅者，因此无法扩展处理。</p>
<p>卡夫卡的消费群体概念概括了这两个概念。与队列一样，使用者组允许您将处理划分为一组进程（使用者组的成员）。与发布订阅一样，Kafka允许您将消息广播到多个消费者组。</p>
<p>Kafka模型的优势在于，每个主题都具有这些属性-可以扩展处理范围，并且是多订阅者-无需选择其中一个。</p>
<p>与传统的消息传递系统相比，Kafka还具有更强的订购保证。</p>
<p>传统队列将记录按顺序保留在服务器上，如果多个使用者从队列中消费，则服务器将按记录的存储顺序分发记录。但是，尽管服务器按顺序分发记录，但是这些记录是异步传递给使用者的，因此它们可能会在不同的使用者上无序到达。这实际上意味着在并行使用的情况下会丢失记录的顺序。消息传递系统通常通过具有“专用使用者”的概念来解决此问题，该概念仅允许一个进程从队列中使用，但是，这当然意味着在处理中没有并行性。</p>
<p>卡夫卡做得更好。通过在主题内具有并行性（即分区）的概念，Kafka能够在用户进程池中提供排序保证和负载均衡。这是通过将主题中的分区分配给消费者组中的消费者来实现的，以便每个分区都由组中的一个消费者完全消费。通过这样做，我们确保使用者是该分区的唯一读取器，并按顺序使用数据。由于存在许多分区，因此仍然可以平衡许多使用者实例上的负载。但是请注意，使用者组中的使用者实例不能超过分区。</p>
<h4 id="Kafka用于流处理"><a href="#Kafka用于流处理" class="headerlink" title="Kafka用于流处理"></a>Kafka用于流处理</h4><p>仅读取，写入和存储数据流是不够的，目的是实现对流的实时处理。</p>
<p>在Kafka中，流处理器是指从输入主题中获取连续数据流，对该输入进行一些处理并生成连续数据流以输出主题的任何东西。</p>
<p>例如，零售应用程序可以接收销售和装运的输入流，并输出根据此数据计算出的重新订购和价格调整流。</p>
<p>可以直接使用生产者和消费者API进行简单处理。但是，对于更复杂的转换，Kafka提供了完全集成的<a href="https://kafka.apache.org/documentation/streams" target="_blank" rel="noopener">Streams API</a>。这允许构建执行非平凡处理的应用程序，这些应用程序计算流的聚合或将流连接在一起。</p>
<p>该功能有助于解决此类应用程序所面临的难题：处理无序数据，在代码更改时重新处理输入，执行状态计算等。</p>
<p>流API建立在Kafka提供的核心原语之上：它使用生产者和使用者API作为输入，使用Kafka进行状态存储，并使用相同的组机制来实现流处理器实例之间的容错。</p>
<h4 id="拼凑在一起"><a href="#拼凑在一起" class="headerlink" title="拼凑在一起"></a>拼凑在一起</h4><p>消息传递，存储和流处理的这种组合看似不寻常，但这对于Kafka作为流平台的角色而言至关重要。</p>
<p>像HDFS这样的分布式文件系统允许存储静态文件以进行批处理。实际上，像这样的系统可以存储和处理过去的<em>历史</em>数据。</p>
<p>传统的企业消息传递系统允许处理将来的消息，这些消息将在您订阅后到达。以这种方式构建的应用程序会在将来的数据到达时对其进行处理。</p>
<p>Kafka结合了这两种功能，对于使用Kafka作为流应用程序平台和流数据管道平台来说，这种结合至关重要。</p>
<p>通过结合存储和低延迟订阅，流应用程序可以以相同的方式处理过去和将来的数据。那是一个单一的应用程序可以处理历史记录中存储的数据，而不是在到达最后一条记录时结束，而是可以在将来的数据到达时继续进行处理。这是流处理的通用概念，它包含批处理以及消息驱动的应用程序。</p>
<p>同样，对于流数据管道，对实时事件的订阅组合使得可以将Kafka用于非常低延迟的管道。但是可靠地存储数据的能力使其可以用于必须保证数据传输的关键数据，或与仅定期加载数据或可能停机很长时间进行维护的脱机系统集成。流处理设备使数据到达时可以进行转换。</p>
<p>有关Kafka提供的担保，API和功能的更多信息，请参阅本<a href="https://kafka.apache.org/documentation.html" target="_blank" rel="noopener">文档</a>的其余部分。</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>环境说明：</p>
<p>我这里还是采用一台机器，使用不同端口的方式来安装Kafka集群：</p>
<table>
<thead>
<tr>
<th>IP</th>
<th>Port</th>
<th>log.dir</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>172.17.0.87</td>
<td>9092</td>
<td>/data/kafka-logs/kafka01</td>
<td>kafka01</td>
</tr>
<tr>
<td>172.17.0.87</td>
<td>9293</td>
<td>/data/kafka-logs/kafka02</td>
<td>kafka02</td>
</tr>
<tr>
<td>172.17.0.87</td>
<td>9094</td>
<td>/data/kafka-logs/kafka03</td>
<td>kafka03</td>
</tr>
</tbody></table>
<p>下载：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ wget -c <span class="string">"https://archive.apache.org/dist/kafka/2.0.0/kafka_2.12-2.0.0.tgz"</span></span><br><span class="line">$ tar xf /opt/softs/kafka_2.12-2.0.0.tgz /data/knner</span><br><span class="line">$ <span class="built_in">cd</span> /data/knner</span><br><span class="line">$ mv kafka_2.12-2.0.0 kafka_2.12-2.0.0_01</span><br><span class="line">$ cp -a kafka_2.12-2.0.0_01 kafka_2.12-2.0.0_02</span><br><span class="line">$ cp -a kafka_2.12-2.0.0_01 kafka_2.12-2.0.0_03</span><br><span class="line">$ ln -s kafka_2.12-2.0.0_01 kafka01</span><br><span class="line">$ ln -s kafka_2.12-2.0.0_02 kafka02</span><br><span class="line">$ ln -s kafka_2.12-2.0.0_03 kafka03</span><br></pre></td></tr></table></figure>

<p>查看一下目录结构：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ll kafka01/</span><br><span class="line">total 52</span><br><span class="line">drwxr-xr-x 3 ec2-user ec2-user  4096 Jul 24  2018 bin <span class="comment"># 脚本文件目录</span></span><br><span class="line">drwxr-xr-x 2 ec2-user ec2-user  4096 Jul 24  2018 config <span class="comment"># 配置文件目录</span></span><br><span class="line">drwxr-xr-x 2 ec2-user ec2-user  4096 Dec  5 17:40 libs <span class="comment"># jar包目录</span></span><br><span class="line">-rw-r--r-- 1 ec2-user ec2-user 28824 Jul 24  2018 LICENSE</span><br><span class="line">-rw-r--r-- 1 ec2-user ec2-user   336 Jul 24  2018 NOTICE</span><br><span class="line">drwxr-xr-x 2 ec2-user ec2-user    44 Jul 24  2018 site-docs <span class="comment"># doc目录</span></span><br><span class="line"></span><br><span class="line">$ tree kafka01</span><br><span class="line">kafka01</span><br><span class="line">├── bin</span><br><span class="line">│   ├── connect-distributed.sh</span><br><span class="line">│   ├── connect-standalone.sh</span><br><span class="line">│   ├── kafka-acls.sh</span><br><span class="line">│   ├── kafka-broker-api-versions.sh</span><br><span class="line">│   ├── kafka-configs.sh</span><br><span class="line">│   ├── kafka-console-consumer.sh</span><br><span class="line">│   ├── kafka-console-producer.sh</span><br><span class="line">│   ├── kafka-consumer-groups.sh</span><br><span class="line">│   ├── kafka-consumer-perf-test.sh</span><br><span class="line">│   ├── kafka-delegation-tokens.sh</span><br><span class="line">│   ├── kafka-delete-records.sh</span><br><span class="line">│   ├── kafka-dump-log.sh</span><br><span class="line">│   ├── kafka-log-dirs.sh</span><br><span class="line">│   ├── kafka-mirror-maker.sh</span><br><span class="line">│   ├── kafka-preferred-replica-election.sh</span><br><span class="line">│   ├── kafka-producer-perf-test.sh</span><br><span class="line">│   ├── kafka-reassign-partitions.sh</span><br><span class="line">│   ├── kafka-replica-verification.sh</span><br><span class="line">│   ├── kafka-run-class.sh</span><br><span class="line">│   ├── kafka-server-start.sh</span><br><span class="line">│   ├── kafka-server-stop.sh</span><br><span class="line">│   ├── kafka-streams-application-reset.sh</span><br><span class="line">│   ├── kafka-topics.sh</span><br><span class="line">│   ├── kafka-verifiable-consumer.sh</span><br><span class="line">│   ├── kafka-verifiable-producer.sh</span><br><span class="line">│   ├── trogdor.sh</span><br><span class="line">│   ├── windows</span><br><span class="line">│   │   ├── connect-distributed.bat</span><br><span class="line">│   │   ├── connect-standalone.bat</span><br><span class="line">│   │   ├── kafka-acls.bat</span><br><span class="line">│   │   ├── kafka-broker-api-versions.bat</span><br><span class="line">│   │   ├── kafka-configs.bat</span><br><span class="line">│   │   ├── kafka-console-consumer.bat</span><br><span class="line">│   │   ├── kafka-console-producer.bat</span><br><span class="line">│   │   ├── kafka-consumer-groups.bat</span><br><span class="line">│   │   ├── kafka-consumer-perf-test.bat</span><br><span class="line">│   │   ├── kafka-delegation-tokens.bat</span><br><span class="line">│   │   ├── kafka-dump-log.bat</span><br><span class="line">│   │   ├── kafka-mirror-maker.bat</span><br><span class="line">│   │   ├── kafka-preferred-replica-election.bat</span><br><span class="line">│   │   ├── kafka-producer-perf-test.bat</span><br><span class="line">│   │   ├── kafka-reassign-partitions.bat</span><br><span class="line">│   │   ├── kafka-replica-verification.bat</span><br><span class="line">│   │   ├── kafka-run-class.bat</span><br><span class="line">│   │   ├── kafka-server-start.bat</span><br><span class="line">│   │   ├── kafka-server-stop.bat</span><br><span class="line">│   │   ├── kafka-topics.bat</span><br><span class="line">│   │   ├── zookeeper-server-start.bat</span><br><span class="line">│   │   ├── zookeeper-server-stop.bat</span><br><span class="line">│   │   └── zookeeper-shell.bat</span><br><span class="line">│   ├── zookeeper-security-migration.sh</span><br><span class="line">│   ├── zookeeper-server-start.sh</span><br><span class="line">│   ├── zookeeper-server-stop.sh</span><br><span class="line">│   └── zookeeper-shell.sh</span><br><span class="line">├── config</span><br><span class="line">│   ├── connect-console-sink.properties</span><br><span class="line">│   ├── connect-console-source.properties</span><br><span class="line">│   ├── connect-distributed.properties</span><br><span class="line">│   ├── connect-file-sink.properties</span><br><span class="line">│   ├── connect-file-source.properties</span><br><span class="line">│   ├── connect-log4j.properties</span><br><span class="line">│   ├── connect-standalone.properties</span><br><span class="line">│   ├── consumer.properties</span><br><span class="line">│   ├── log4j.properties</span><br><span class="line">│   ├── producer.properties</span><br><span class="line">│   ├── server.properties</span><br><span class="line">│   ├── tools-log4j.properties</span><br><span class="line">│   ├── trogdor.conf</span><br><span class="line">│   └── zookeeper.properties</span><br><span class="line">├── libs</span><br><span class="line">│   ├── activation-1.1.1.jar</span><br><span class="line">│   ├── aopalliance-repackaged-2.5.0-b42.jar</span><br><span class="line">│   ├── argparse4j-0.7.0.jar</span><br><span class="line">│   ├── audience-annotations-0.5.0.jar</span><br><span class="line">│   ├── commons-lang3-3.5.jar</span><br><span class="line">│   ├── connect-api-2.0.0.jar</span><br><span class="line">│   ├── connect-basic-auth-extension-2.0.0.jar</span><br><span class="line">│   ├── connect-file-2.0.0.jar</span><br><span class="line">│   ├── connect-json-2.0.0.jar</span><br><span class="line">│   ├── connect-runtime-2.0.0.jar</span><br><span class="line">│   ├── connect-transforms-2.0.0.jar</span><br><span class="line">│   ├── guava-20.0.jar</span><br><span class="line">│   ├── hk2-api-2.5.0-b42.jar</span><br><span class="line">│   ├── hk2-locator-2.5.0-b42.jar</span><br><span class="line">│   ├── hk2-utils-2.5.0-b42.jar</span><br><span class="line">│   ├── jackson-annotations-2.9.6.jar</span><br><span class="line">│   ├── jackson-core-2.9.6.jar</span><br><span class="line">│   ├── jackson-databind-2.9.6.jar</span><br><span class="line">│   ├── jackson-jaxrs-base-2.9.6.jar</span><br><span class="line">│   ├── jackson-jaxrs-json-provider-2.9.6.jar</span><br><span class="line">│   ├── jackson-module-jaxb-annotations-2.9.6.jar</span><br><span class="line">│   ├── javassist-3.22.0-CR2.jar</span><br><span class="line">│   ├── javax.annotation-api-1.2.jar</span><br><span class="line">│   ├── javax.inject-1.jar</span><br><span class="line">│   ├── javax.inject-2.5.0-b42.jar</span><br><span class="line">│   ├── javax.servlet-api-3.1.0.jar</span><br><span class="line">│   ├── javax.ws.rs-api-2.1.jar</span><br><span class="line">│   ├── jaxb-api-2.3.0.jar</span><br><span class="line">│   ├── jersey-client-2.27.jar</span><br><span class="line">│   ├── jersey-common-2.27.jar</span><br><span class="line">│   ├── jersey-container-servlet-2.27.jar</span><br><span class="line">│   ├── jersey-container-servlet-core-2.27.jar</span><br><span class="line">│   ├── jersey-hk2-2.27.jar</span><br><span class="line">│   ├── jersey-media-jaxb-2.27.jar</span><br><span class="line">│   ├── jersey-server-2.27.jar</span><br><span class="line">│   ├── jetty-client-9.4.11.v20180605.jar</span><br><span class="line">│   ├── jetty-continuation-9.4.11.v20180605.jar</span><br><span class="line">│   ├── jetty-http-9.4.11.v20180605.jar</span><br><span class="line">│   ├── jetty-io-9.4.11.v20180605.jar</span><br><span class="line">│   ├── jetty-security-9.4.11.v20180605.jar</span><br><span class="line">│   ├── jetty-server-9.4.11.v20180605.jar</span><br><span class="line">│   ├── jetty-servlet-9.4.11.v20180605.jar</span><br><span class="line">│   ├── jetty-servlets-9.4.11.v20180605.jar</span><br><span class="line">│   ├── jetty-util-9.4.11.v20180605.jar</span><br><span class="line">│   ├── jopt-simple-5.0.4.jar</span><br><span class="line">│   ├── kafka_2.12-2.0.0.jar</span><br><span class="line">│   ├── kafka_2.12-2.0.0.jar.asc</span><br><span class="line">│   ├── kafka_2.12-2.0.0-javadoc.jar</span><br><span class="line">│   ├── kafka_2.12-2.0.0-javadoc.jar.asc</span><br><span class="line">│   ├── kafka_2.12-2.0.0-scaladoc.jar</span><br><span class="line">│   ├── kafka_2.12-2.0.0-scaladoc.jar.asc</span><br><span class="line">│   ├── kafka_2.12-2.0.0-sources.jar</span><br><span class="line">│   ├── kafka_2.12-2.0.0-sources.jar.asc</span><br><span class="line">│   ├── kafka_2.12-2.0.0-test.jar</span><br><span class="line">│   ├── kafka_2.12-2.0.0-test.jar.asc</span><br><span class="line">│   ├── kafka_2.12-2.0.0-test-sources.jar</span><br><span class="line">│   ├── kafka_2.12-2.0.0-test-sources.jar.asc</span><br><span class="line">│   ├── kafka-clients-2.0.0.jar</span><br><span class="line">│   ├── kafka-log4j-appender-2.0.0.jar</span><br><span class="line">│   ├── kafka-streams-2.0.0.jar</span><br><span class="line">│   ├── kafka-streams-examples-2.0.0.jar</span><br><span class="line">│   ├── kafka-streams-scala_2.12-2.0.0.jar</span><br><span class="line">│   ├── kafka-streams-test-utils-2.0.0.jar</span><br><span class="line">│   ├── kafka-tools-2.0.0.jar</span><br><span class="line">│   ├── log4j-1.2.17.jar</span><br><span class="line">│   ├── lz4-java-1.4.1.jar</span><br><span class="line">│   ├── maven-artifact-3.5.3.jar</span><br><span class="line">│   ├── metrics-core-2.2.0.jar</span><br><span class="line">│   ├── osgi-resource-locator-1.0.1.jar</span><br><span class="line">│   ├── plexus-utils-3.1.0.jar</span><br><span class="line">│   ├── reflections-0.9.11.jar</span><br><span class="line">│   ├── rocksdbjni-5.7.3.jar</span><br><span class="line">│   ├── scala-library-2.12.6.jar</span><br><span class="line">│   ├── scala-logging_2.12-3.9.0.jar</span><br><span class="line">│   ├── scala-reflect-2.12.6.jar</span><br><span class="line">│   ├── slf4j-api-1.7.25.jar</span><br><span class="line">│   ├── slf4j-log4j12-1.7.25.jar</span><br><span class="line">│   ├── snappy-java-1.1.7.1.jar</span><br><span class="line">│   ├── validation-api-1.1.0.Final.jar</span><br><span class="line">│   ├── zkclient-0.10.jar</span><br><span class="line">│   └── zookeeper-3.4.13.jar</span><br><span class="line">├── LICENSE</span><br><span class="line">├── NOTICE</span><br><span class="line">└── site-docs</span><br><span class="line">    └── kafka_2.12-2.0.0-site-docs.tgz</span><br><span class="line"></span><br><span class="line">5 directories, 151 files</span><br></pre></td></tr></table></figure>



<h3 id="Kafka-配置详解"><a href="#Kafka-配置详解" class="headerlink" title="Kafka 配置详解"></a>Kafka 配置详解</h3><blockquote>
<p>参考：</p>
<p><a href="https://kafka.apache.org/documentation/#brokerconfigs" target="_blank" rel="noopener">https://kafka.apache.org/documentation/#brokerconfigs</a>     Broker配置</p>
<p><a href="https://kafka.apache.org/documentation/#topicconfigs" target="_blank" rel="noopener">https://kafka.apache.org/documentation/#topicconfigs</a>    topic配置</p>
<p><a href="https://kafka.apache.org/documentation/#producerconfigs" target="_blank" rel="noopener">https://kafka.apache.org/documentation/#producerconfigs</a>    生产者配置</p>
<p><a href="https://kafka.apache.org/documentation/#consumerconfigs" target="_blank" rel="noopener">https://kafka.apache.org/documentation/#consumerconfigs</a>    消费者配置</p>
<p><a href="https://kafka.apache.org/documentation/#connectconfigs" target="_blank" rel="noopener">https://kafka.apache.org/documentation/#connectconfigs</a>    kafka connect配置</p>
<p><a href="https://kafka.apache.org/documentation/#sourceconnectconfigs" target="_blank" rel="noopener">https://kafka.apache.org/documentation/#sourceconnectconfigs</a>    source connector配置</p>
<p><a href="https://kafka.apache.org/documentation/#sinkconnectconfigs" target="_blank" rel="noopener">https://kafka.apache.org/documentation/#sinkconnectconfigs</a>     sink connector配置</p>
<p><a href="https://kafka.apache.org/documentation/#streamsconfigs" target="_blank" rel="noopener">https://kafka.apache.org/documentation/#streamsconfigs</a>    streams 配置</p>
<p><a href="https://kafka.apache.org/documentation/#adminclientconfigs" target="_blank" rel="noopener">https://kafka.apache.org/documentation/#adminclientconfigs</a>    adminClient配置</p>
</blockquote>
<p>下面列出常用的配置以及解释：</p>
<ul>
<li><p><code>broker.id</code> Broker ID，Kafka集群中必须唯一。</p>
</li>
<li><p><code>listeners</code> Broker 监听的配置：非加密的用：PLAINTEXT://IP:PORT，加密的用SSL://IP:PORT</p>
</li>
<li><p><code>advertised.listeners</code> 宣告的地址，也是向zk注册的地址，此地址用于生产者和消费者，格式同<code>listeners</code></p>
</li>
<li><p><code>num.network.threads</code> 用于从网络上接收发送的线程数</p>
</li>
<li><p><code>num.io.threads</code> 处理请求，包括磁盘I/O的线程数</p>
</li>
<li><p><code>socket.send.buffer.bytes</code> 发送的buffer，如果值为-1，使用系统默认值</p>
</li>
<li><p><code>socket.receive.buffer.bytes</code> 接收的buffer，如果值为-1，使用系统默认值</p>
</li>
<li><p><code>socket.request.max.bytes</code> 接收的最大字节数</p>
</li>
<li><p><code>log.dirs</code> log存放的位置</p>
</li>
<li><p><code>num.partitions</code> 默认的每个topic中partitions数量，默认是1</p>
</li>
<li><p><code>default.replication.factor</code> 默认topic中的副本数，默认是1，这个更改成2。</p>
</li>
<li><p><code>log.retention.hours</code> 日志最大保存的小时，过期的将被删除，这里设置成48h</p>
</li>
<li><p><code>zookeeper.connect</code> zookeeper的地址，IP:PORT，多个zk用逗号隔开</p>
</li>
<li><p><code>message.max.bytes</code> 默认消息的大小，增大到5M</p>
</li>
<li><p><code>replica.fetch.max.bytes</code> 取消息的最大大小，默认是1M，增加到5M</p>
</li>
<li><p><code>auto.create.topics.enable</code> 是否可以自动创建topoc</p>
</li>
<li><p><code>controlled.shutdown.enable</code> 可以用于优雅停服，注意<code>default.replication.factor</code>的值需要大于1.</p>
</li>
<li><p><code>broker.rack</code> 配置Broker机架感知，可将同一分区的副本分布在不同机架上，扩展了Kafka为代理故障提供的保证，以涵盖机架故障，从而限制了机架上所有代理立即发生故障时数据丢失的风险。</p>
</li>
<li><p><code>auto.leader.rebalance.enable</code> 自动分区领导者平衡，每当代理停止或崩溃时，该代理的分区就会转移到其他副本。这意味着默认情况下，重新启动代理时，它将仅是其所有分区的关注者，这意味着它将不用于客户端读取和写入。</p>
<p>为了避免这种不平衡，Kafka提出了首选副本的概念。如果分区的副本列表为1,5,9，则首选节点1作为节点5或9的引导者，因为它在副本列表中较早。您可以让Kafka集群尝试通过运行以下命令来恢复对已还原副本的领导权：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-preferred-replica-election.sh --zookeeper zk_host:port</span><br></pre></td></tr></table></figure>



</li>
</ul>
<p>下面是Kafka集群三个节点的配置：因为我这里都是在一台机器上，所以其中的端口和log路径不能相同，如果你是在不同机器上的节点，配置文件尽可能的保持一样。</p>
<h4 id="Kafka01-配置："><a href="#Kafka01-配置：" class="headerlink" title="Kafka01 配置："></a>Kafka01 配置：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cat kafka01/config/server.properties |grep -vE &quot;^#|^$&quot;</span><br><span class="line">broker.id=0</span><br><span class="line">listeners=PLAINTEXT://172.17.0.87:9092</span><br><span class="line">advertised.listeners=PLAINTEXT://172.17.0.87:9092</span><br><span class="line">num.network.threads=3</span><br><span class="line">num.io.threads=8</span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line">log.dirs=/data/kafka-logs/kafka01</span><br><span class="line">num.partitions=1</span><br><span class="line">default.replication.factor=2</span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line">offsets.topic.replication.factor=1</span><br><span class="line">transaction.state.log.replication.factor=1</span><br><span class="line">transaction.state.log.min.isr=1</span><br><span class="line">log.retention.hours=48</span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line">zookeeper.connect=zk01:2181,zk02:2182,zk03:2183</span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line">group.initial.rebalance.delay.ms=0</span><br><span class="line">message.max.bytes=5242880</span><br><span class="line">replica.fetch.max.bytes=5242880</span><br><span class="line">auto.create.topics.enable=false</span><br><span class="line">controlled.shutdown.enable=true</span><br><span class="line">broker.rack=cn-north-1a</span><br><span class="line">auto.leader.rebalance.enable=true</span><br></pre></td></tr></table></figure>

<h4 id="Kafka02-配置："><a href="#Kafka02-配置：" class="headerlink" title="Kafka02 配置："></a>Kafka02 配置：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cat kafka02/config/server.properties |grep -vE &quot;^#|^$&quot;</span><br><span class="line">broker.id=1</span><br><span class="line">listeners=PLAINTEXT://172.17.0.87:9093</span><br><span class="line">advertised.listeners=PLAINTEXT://172.17.0.87:9093</span><br><span class="line">num.network.threads=3</span><br><span class="line">num.io.threads=8</span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line">log.dirs=/data/kafka-logs/kafka02</span><br><span class="line">num.partitions=1</span><br><span class="line">default.replication.factor=2</span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line">offsets.topic.replication.factor=1</span><br><span class="line">transaction.state.log.replication.factor=1</span><br><span class="line">transaction.state.log.min.isr=1</span><br><span class="line">log.retention.hours=48</span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line">zookeeper.connect=zk01:2181,zk02:2182,zk03:2183</span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line">group.initial.rebalance.delay.ms=0</span><br><span class="line">message.max.bytes=5242880</span><br><span class="line">replica.fetch.max.bytes=5242880</span><br><span class="line">auto.create.topics.enable=false</span><br><span class="line">controlled.shutdown.enable=true</span><br><span class="line">broker.rack=cn-north-1a</span><br><span class="line">auto.leader.rebalance.enable=true</span><br></pre></td></tr></table></figure>

<h4 id="Kafka03-配置："><a href="#Kafka03-配置：" class="headerlink" title="Kafka03 配置："></a>Kafka03 配置：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cat kafka03/config/server.properties |grep -vE &quot;^#|^$&quot;</span><br><span class="line">broker.id=2</span><br><span class="line">listeners=PLAINTEXT://172.17.0.87:9094</span><br><span class="line">advertised.listeners=PLAINTEXT://172.17.0.87:9094</span><br><span class="line">num.network.threads=3</span><br><span class="line">num.io.threads=8</span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line">log.dirs=/data/kafka-logs/kafka03</span><br><span class="line">num.partitions=1</span><br><span class="line">default.replication.factor=2</span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line">offsets.topic.replication.factor=1</span><br><span class="line">transaction.state.log.replication.factor=1</span><br><span class="line">transaction.state.log.min.isr=1</span><br><span class="line">log.retention.hours=48</span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line">zookeeper.connect=zk01:2181,zk02:2182,zk03:2183</span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line">group.initial.rebalance.delay.ms=0</span><br><span class="line">message.max.bytes=5242880</span><br><span class="line">replica.fetch.max.bytes=5242880</span><br><span class="line">auto.create.topics.enable=false</span><br><span class="line">controlled.shutdown.enable=true</span><br><span class="line">broker.rack=cn-north-1a</span><br><span class="line">auto.leader.rebalance.enable=true</span><br></pre></td></tr></table></figure>

<p>创建log存储目录：</p>
<p>Kafka 在启动的时候也会自动创建。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mkdir /data/kafka-logs/kafka0&#123;1..3&#125; -pv</span><br><span class="line">mkdir: created directory ‘/data/kafka-logs/kafka01’</span><br><span class="line">mkdir: created directory ‘/data/kafka-logs/kafka02’</span><br><span class="line">mkdir: created directory ‘/data/kafka-logs/kafka03’</span><br></pre></td></tr></table></figure>



<h3 id="启动-1"><a href="#启动-1" class="headerlink" title="启动"></a>启动</h3><p>查看启动脚本帮助：<code>kafka-server-start.sh</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-server-start.sh</span><br><span class="line">USAGE: ./kafka01/bin/kafka-server-start.sh [-daemon] server.properties [--override property=value]*</span><br></pre></td></tr></table></figure>

<p>启动三台Kafka：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-server-start.sh -daemon kafka01/config/server.properties</span><br><span class="line">$ ./kafka02/bin/kafka-server-start.sh -daemon kafka02/config/server.properties</span><br><span class="line">$ ./kafka03/bin/kafka-server-start.sh -daemon kafka03/config/server.properties</span><br></pre></td></tr></table></figure>

<p>检查进行，以及监听端口：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ps -ef|grep kafka01</span><br><span class="line">ec2-user 19933     1  3 13:21 pts/8    00:00:04 java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=<span class="literal">true</span> -Xloggc:/data/knner/kafka01/bin/../logs/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=<span class="literal">false</span> -Dcom.sun.management.jmxremote.ssl=<span class="literal">false</span> -Dkafka.logs.dir=/data/knner/kafka01/bin/../logs -Dlog4j.configuration=file:./kafka01/bin/../config/log4j.properties -cp /data/knner/kafka01/bin/../libs/activation-1.1.1.jar:/data/knner/kafka01/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/data/knner/kafka01/bin/../libs/argparse4j-0.7.0.jar:/data/knner/kafka01/bin/../libs/audience-annotations-0.5.0.jar:/data/knner/kafka01/bin/../libs/commons-lang3-3.5.jar:/data/knner/kafka01/bin/../libs/connect-api-2.0.0.jar:/data/knner/kafka01/bin/../libs/connect-basic-auth-extension-2.0.0.jar:/data/knner/kafka01/bin/../libs/connect-file-2.0.0.jar:/data/knner/kafka01/bin/../libs/connect-json-2.0.0.jar:/data/knner/kafka01/bin/../libs/connect-runtime-2.0.0.jar:/data/knner/kafka01/bin/../libs/connect-transforms-2.0.0.jar:/data/knner/kafka01/bin/../libs/guava-20.0.jar:/data/knner/kafka01/bin/../libs/hk2-api-2.5.0-b42.jar:/data/knner/kafka01/bin/../libs/hk2-locator-2.5.0-b42.jar:/data/knner/kafka01/bin/../libs/hk2-utils-2.5.0-b42.jar:/data/knner/kafka01/bin/../libs/jackson-annotations-2.9.6.jar:/data/knner/kafka01/bin/../libs/jackson-core-2.9.6.jar:/data/knner/kafka01/bin/../libs/jackson-databind-2.9.6.jar:/data/knner/kafka01/bin/../libs/jackson-jaxrs-base-2.9.6.jar:/data/knner/kafka01/bin/../libs/jackson-jaxrs-json-provider-2.9.6.jar:/data/knner/kafka01/bin/../libs/jackson-module-jaxb-annotations-2.9.6.jar:/data/knner/kafka01/bin/../libs/javassist-3.22.0-CR2.jar:/data/knner/kafka01/bin/../libs/javax.annotation-api-1.2.jar:/data/knner/kafka01/bin/../libs/javax.inject-1.jar:/data/knner/kafka01/bin/../libs/javax.inject-2.5.0-b42.jar:/data/knner/kafka01/bin/../libs/javax.servlet-api-3.1.0.jar:/data/knner/kafka01/bin/../libs/javax.ws.rs-api-2.1.jar:/data/knner/kafka01/bin/../libs/jaxb-api-2.3.0.jar:/data/knner/kafka01/bin/../libs/jersey-client-2.27.jar:/data/knner/kafka01/bin/../libs/jersey-common-2.27.jar:/data/knner/kafka01/bin/../libs/jersey-container-servlet-2.27.jar:/data/knner/kafka01/bin/../libs/jersey-container-servlet-core-2.27.jar:/data/knner/kafka01/bin/../libs/jersey-hk2-2.27.jar:/data/knner/kafka0/bin/../libs/jersey-media-jaxb-2.27.jar:/data/knner/kafka01/bin/../libs/jersey-server-2.27.jar:/data/knner/kafka01/bin/../libs/jetty-client-9.4.11.v20180605.jar:/data/knner/kafka01/bin/../libs/jetty-continuation-9.4.11.v20180605.jar:/data/knner/kafka01/bin/../libs/jetty-http-9.4.11.v20180605.jar:/data/knner/kafka01/bin/../libs/jetty-io-9.4.11.v20180605.jar:/data/knner/kafka01/bin/../libs/jetty-security-9.4.11.v20180605.jar:/data/knner/kafka01/bin/../libs/jetty-server-9.4.11.v20180605.jar:/data/knner/kafka01/bin/../libs/jetty-servlet-9.4.11.v20180605.jar:/data/knner/kafka01/bin/../libs/jetty-servlets-9.4.11.v20180605.jar:/data/knner/kafka01/bin/../libs/jetty-util-9.4.11.v20180605.jar:/data/knner/kafka01/bin/../libs/jopt-simple-5.0.4.jar:/data/knner/kafka01/bin/../libs/kafka_2.12-2.0.0.jar:/data/knner/kafka01/bin/../libs/kafka_2.12-2.0.0-sources.jar:/data/knner/kafka01/bin/../libs/kafka-clients-2.0.0.jar:/data/knner/kafka01/bin/../libs/kafka-log4j-appender-2.0.0.jar:/data/knner/kafka01/bin/../libs/kafka-streams-2.0.0.jar:/data/knner/kafka01/bin/../libs/kafka-streams-examples-2.0.0.jar:/data/knner/kafka01/bin/../libs/kafka-streams-scala_2.12-2.0.0.jar:/data/knner/kafka01/bin/../libs/kafka-streams-test-utils-2.0.0.jar:/data/knner/kafka01/bin/../libs/kafka-tools-2.0.0.jar:/data/knner/kafka01/bin/../libs/log4j-1.2.17.jar:/data/knner/kafka01/bin/../libs/lz4-java-1.4.1.jar:/data/knner/kafka01/bin/../libs/maven-artifact-3.5.3.jar:/data/knner/kafka01/bin/../libs/metrics-core-2.2.0.jar:/data/knner/kafka01/bin/../libs/osgi-resource-locator-1.0.1.jar:/data/knner/kafka01/bin/../libs/plexus-utils-3.1.0.jar:/data/knner/kafka01/bin/../libs/reflections-0.9.11.jar:/data/knner/kafka01/bin/../libs/rocksdbjni-5.7.3.jar:/data/knner/kafka01/bin/../libs/scala-library-2.12.6.jar:/data/knner/kafka01/bin/../libs/scala-logging_2.12-3.9.0.jar:/data/knner/kafka01/bin/../libs/scala-reflect-2.12.6.jar:/data/knner/kafka01/bin/../libs/slf4j-api-1.7.25.jar:/data/knner/kafka01/bin/../libs/slf4j-log4j12-1.7.25.jar:/data/knner/kafka01/bin/../libs/snappy-java-1.1.7.1.jar:/data/knner/kafka01/bin/../libs/validation-api-1.1.0.Final.jar:/data/knner/kafka01/bin/../libs/zkclient-0.10.jar:/data/knner/kafka01/bin/../libs/zookeeper-3.4.13.jar kafka.Kafka kafka01/config/server.properties</span><br><span class="line"></span><br><span class="line">$ netstat -lnutp|grep 19933</span><br><span class="line">tcp6       0      0 :::35773                :::*                    LISTEN      19933/java          </span><br><span class="line">tcp6       0      0 172.17.0.87:9092        :::*                    LISTEN      19933/java</span><br></pre></td></tr></table></figure>

<p>我们看到默认的Kafka JVM设置是1G堆内存：</p>
<h4 id="修改堆内存"><a href="#修改堆内存" class="headerlink" title="修改堆内存"></a>修改堆内存</h4><p>修改启动脚本：<code>bin/kafka-server-start.sh</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$KAFKA_HEAP_OPTS</span>"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-Xmx1G -Xms1G"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<p>可以通过环境变量的方式：KAFKA_HEAP_OPTS，也可以直接修改它的默认值。</p>
<p>检查Zookeeper中Kafka的注册情况：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./zk01/bin/zkCli.sh -server 172.17.0.87:2181</span><br><span class="line">[zk: 172.17.0.87:2181(CONNECTED) 0] ls /</span><br><span class="line">[admin, brokers, cluster, config, consumers, controller, controller_epoch, isr_change_notification, latest_producer_id_block, log_dir_event_notification, zookeeper]</span><br><span class="line">[zk: 172.17.0.87:2181(CONNECTED) 1] ls /brokers</span><br><span class="line">[ids, seqid, topics]</span><br><span class="line">[zk: 172.17.0.87:2181(CONNECTED) 2] ls /brokers/ids</span><br><span class="line">[0, 1, 2]</span><br><span class="line">[zk: 172.17.0.87:2181(CONNECTED) 3] ls /brokers/ids/0</span><br><span class="line">[]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看到有三台Broker Kafka节点已经注册了</span></span><br><span class="line">[zk: 172.17.0.87:2181(CONNECTED) 4] get /brokers/ids/0</span><br><span class="line">&#123;<span class="string">"listener_security_protocol_map"</span>:&#123;<span class="string">"PLAINTEXT"</span>:<span class="string">"PLAINTEXT"</span>&#125;,<span class="string">"endpoints"</span>:[<span class="string">"PLAINTEXT://172.17.0.87:9092"</span>],<span class="string">"jmx_port"</span>:-1,<span class="string">"host"</span>:<span class="string">"172.17.0.87"</span>,<span class="string">"timestamp"</span>:<span class="string">"1575868871067"</span>,<span class="string">"port"</span>:9092,<span class="string">"version"</span>:4&#125;</span><br><span class="line">[zk: 172.17.0.87:2181(CONNECTED) 5] get /brokers/ids/1</span><br><span class="line">&#123;<span class="string">"listener_security_protocol_map"</span>:&#123;<span class="string">"PLAINTEXT"</span>:<span class="string">"PLAINTEXT"</span>&#125;,<span class="string">"endpoints"</span>:[<span class="string">"PLAINTEXT://172.17.0.87:9093"</span>],<span class="string">"jmx_port"</span>:-1,<span class="string">"host"</span>:<span class="string">"172.17.0.87"</span>,<span class="string">"timestamp"</span>:<span class="string">"1575868952567"</span>,<span class="string">"port"</span>:9093,<span class="string">"version"</span>:4&#125;</span><br><span class="line">[zk: 172.17.0.87:2181(CONNECTED) 7] get /brokers/ids/2</span><br><span class="line">&#123;<span class="string">"listener_security_protocol_map"</span>:&#123;<span class="string">"PLAINTEXT"</span>:<span class="string">"PLAINTEXT"</span>&#125;,<span class="string">"endpoints"</span>:[<span class="string">"PLAINTEXT://172.17.0.87:9094"</span>],<span class="string">"jmx_port"</span>:-1,<span class="string">"host"</span>:<span class="string">"172.17.0.87"</span>,<span class="string">"timestamp"</span>:<span class="string">"1575868974088"</span>,<span class="string">"port"</span>:9094,<span class="string">"version"</span>:4&#125;</span><br><span class="line"></span><br><span class="line">[zk: 172.17.0.87:2181(CONNECTED) 17] get /cluster/id</span><br><span class="line">&#123;<span class="string">"version"</span>:<span class="string">"1"</span>,<span class="string">"id"</span>:<span class="string">"gduCYgbZSGG8eRfNF4hEMw"</span>&#125;</span><br><span class="line"></span><br><span class="line">[zk: 172.17.0.87:2181(CONNECTED) 25] get /controller</span><br><span class="line">&#123;<span class="string">"version"</span>:1,<span class="string">"brokerid"</span>:0,<span class="string">"timestamp"</span>:<span class="string">"1575868871176"</span>&#125;</span><br></pre></td></tr></table></figure>



<h3 id="优雅关机"><a href="#优雅关机" class="headerlink" title="优雅关机"></a>优雅关机</h3><p>Kafka群集将自动检测任何代理关闭或故障，并为该计算机上的分区选择新的领导者。无论服务器发生故障还是为了维护或配置更改而有意将其关闭，都会发生这种情况。对于后一种情况，Kafka支持一种更优雅的机制来停止服务器，而不仅仅是杀死服务器。当服务器正常停止时，它会进行两项优化：</p>
<ol>
<li>它将所有日志同步到磁盘上，以避免在重新启动时进行任何日志恢复（即，验证日志尾部所有消息的校验和）。日志恢复需要时间，因此可以加快有意重启的速度。</li>
<li>它将在关闭服务器之前将服务器领导的所有分区迁移到其他副本。这将使领导层转移更快，并将每个分区不可用的时间减少到几毫秒。</li>
</ol>
<p>只要服务器停止运行（不是通过强行终止），就会自动进行日志同步，但是受控的领导者迁移需要使用特殊设置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">controlled.shutdown.enable=true</span><br></pre></td></tr></table></figure>

<p>请注意，只有在代理上托管的<em>所有</em>分区都具有副本时（即复制因子大于1 <em>并且</em>这些副本中至少有一个处于活动状态），受控关闭才会成功。这通常是您想要的，因为关闭最后一个副本会使该主题分区不可用。</p>
<h3 id="Topic的增删改查"><a href="#Topic的增删改查" class="headerlink" title="Topic的增删改查"></a>Topic的增删改查</h3><blockquote>
<p>参考：<a href="https://cwiki.apache.org/confluence/display/KAFKA/Replication+tools" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/KAFKA/Replication+tools</a>    replication工具使用</p>
<p><a href="https://kafka.apache.org/documentation/#topicconfigs" target="_blank" rel="noopener">https://kafka.apache.org/documentation/#topicconfigs</a>    topic配置</p>
</blockquote>
<p>命令帮助：<code>kafka-topics.sh</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-topics.sh </span><br><span class="line">Create, delete, describe, or change a topic.</span><br><span class="line">Option                                   Description                            </span><br><span class="line">------                                   -----------                            </span><br><span class="line">--alter                                  Alter the number of partitions,        <span class="comment"># 修改</span></span><br><span class="line">                                           replica assignment, and/or           </span><br><span class="line">                                           configuration <span class="keyword">for</span> the topic.         </span><br><span class="line">--config &lt;String: name=value&gt;            A topic configuration override <span class="keyword">for</span> the </span><br><span class="line">                                           topic being created or altered.The   </span><br><span class="line">                                           following is a list of valid         </span><br><span class="line">                                           configurations:   <span class="comment"># 添加或修改topic的可用配置               </span></span><br><span class="line">                                         	cleanup.policy                        </span><br><span class="line">                                         	compression.type                      </span><br><span class="line">                                         	delete.retention.ms                   </span><br><span class="line">                                         	file.delete.delay.ms                  </span><br><span class="line">                                         	flush.messages                        </span><br><span class="line">                                         	flush.ms                              </span><br><span class="line">                                         	follower.replication.throttled.       </span><br><span class="line">                                           replicas                             </span><br><span class="line">                                         	index.interval.bytes                  </span><br><span class="line">                                         	leader.replication.throttled.replicas </span><br><span class="line">                                         	max.message.bytes                     </span><br><span class="line">                                         	message.downconversion.enable         </span><br><span class="line">                                         	message.format.version                </span><br><span class="line">                                         	message.timestamp.difference.max.ms   </span><br><span class="line">                                         	message.timestamp.type                </span><br><span class="line">                                         	min.cleanable.dirty.ratio             </span><br><span class="line">                                         	min.compaction.lag.ms                 </span><br><span class="line">                                         	min.insync.replicas                   </span><br><span class="line">                                         	preallocate                           </span><br><span class="line">                                         	retention.bytes                       </span><br><span class="line">                                         	retention.ms                          </span><br><span class="line">                                         	segment.bytes                         </span><br><span class="line">                                         	segment.index.bytes                   </span><br><span class="line">                                         	segment.jitter.ms                     </span><br><span class="line">                                         	segment.ms                            </span><br><span class="line">                                         	unclean.leader.election.enable        </span><br><span class="line">                                         See the Kafka documentation <span class="keyword">for</span> full   </span><br><span class="line">                                           details on the topic configs.        </span><br><span class="line">--create                                 Create a new topic.    <span class="comment"># 创建topic                </span></span><br><span class="line">--delete                                 Delete a topic         <span class="comment"># 删除topic               </span></span><br><span class="line">--delete-config &lt;String: name&gt;           A topic configuration override to be   </span><br><span class="line">                                           removed <span class="keyword">for</span> an existing topic (see   </span><br><span class="line">                                           the list of configurations under the </span><br><span class="line">                                           --config option).  <span class="comment"># 清除已有topic的配置                  </span></span><br><span class="line">--describe                               List details <span class="keyword">for</span> the given topics.     <span class="comment"># 查看topic 信息</span></span><br><span class="line">--<span class="built_in">disable</span>-rack-aware                     Disable rack aware replica assignment  </span><br><span class="line">--force                                  Suppress console prompts               </span><br><span class="line">--<span class="built_in">help</span>                                   Print usage information.               </span><br><span class="line">--<span class="keyword">if</span>-exists                              <span class="keyword">if</span> <span class="built_in">set</span> when altering or deleting       </span><br><span class="line">                                           topics, the action will only execute </span><br><span class="line">                                           <span class="keyword">if</span> the topic exists                  </span><br><span class="line">--<span class="keyword">if</span>-not-exists                          <span class="keyword">if</span> <span class="built_in">set</span> when creating topics, the       </span><br><span class="line">                                           action will only execute <span class="keyword">if</span> the      </span><br><span class="line">                                           topic does not already exist         </span><br><span class="line">--list                                   List all available topics.       <span class="comment"># 查看topic列表      </span></span><br><span class="line">--partitions &lt;Integer: <span class="comment"># of partitions&gt;  The number of partitions for the topic  # 指定分区数量，在创建或者修改的时候</span></span><br><span class="line">                                           being created or altered (WARNING:   </span><br><span class="line">                                           If partitions are increased <span class="keyword">for</span> a    </span><br><span class="line">                                           topic that has a key, the partition  </span><br><span class="line">                                           logic or ordering of the messages    </span><br><span class="line">                                           will be affected                     </span><br><span class="line">--replica-assignment &lt;String:            A list of manual partition-to-broker  <span class="comment"># replica手动分配replica和Broker之间的映射关系，在创建修改的时候</span></span><br><span class="line">  broker_id_for_part1_replica1 :           assignments <span class="keyword">for</span> the topic being      </span><br><span class="line">  broker_id_for_part1_replica2 ,           created or altered.                  </span><br><span class="line">  broker_id_for_part2_replica1 :                                                </span><br><span class="line">  broker_id_for_part2_replica2 , ...&gt;                                           </span><br><span class="line">--replication-factor &lt;Integer:           The replication factor <span class="keyword">for</span> each        <span class="comment"># 指定副本数，创建时</span></span><br><span class="line">  replication factor&gt;                      partition <span class="keyword">in</span> the topic being created.</span><br><span class="line">--topic &lt;String: topic&gt;                  The topic to be create, alter or       <span class="comment"># topic 名称</span></span><br><span class="line">                                           describe. Can also accept a regular  </span><br><span class="line">                                           expression except <span class="keyword">for</span> --create option</span><br><span class="line">--topics-with-overrides                  <span class="keyword">if</span> <span class="built_in">set</span> when describing topics, only    </span><br><span class="line">                                           show topics that have overridden     </span><br><span class="line">                                           configs                              </span><br><span class="line">--unavailable-partitions                 <span class="keyword">if</span> <span class="built_in">set</span> when describing topics, only    </span><br><span class="line">                                           show partitions whose leader is not  </span><br><span class="line">                                           available                            </span><br><span class="line">--under-replicated-partitions            <span class="keyword">if</span> <span class="built_in">set</span> when describing topics, only    </span><br><span class="line">                                           show under replicated partitions     </span><br><span class="line">--zookeeper &lt;String: hosts&gt;              REQUIRED: The connection string <span class="keyword">for</span>    <span class="comment"># 指定zookeeper地址</span></span><br><span class="line">                                           the zookeeper connection <span class="keyword">in</span> the form </span><br><span class="line">                                           host:port. Multiple hosts can be     </span><br><span class="line">                                           given to allow fail-over.</span><br></pre></td></tr></table></figure>

<div class="note danger">
            <p>在新版本的Kafka中，不需要指定–zookeeper地址了，需要指定kafka的地址即可：–bootstrap-server localhost:9092</p>
          </div>

<h4 id="Topic-的创建"><a href="#Topic-的创建" class="headerlink" title="Topic 的创建"></a>Topic 的创建</h4><p>指定–replication-factor 复制因子，不指定的话，使用配置文件中指定的默认值（default.replication.factor=1）</p>
<p>指定–partitions，指定这个topic 有多少个分区，不指定的话，使用配置文件中指定的默认值（num.partitions=1）</p>
<p>这里的Partitions和Broker的对应关系是Kafka自动分配的：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-topics.sh --create --zookeeper 172.17.0.87:2181 --replication-factor 1 --partitions 1 --topic <span class="built_in">test</span></span><br><span class="line">Created topic <span class="string">"test"</span>.</span><br></pre></td></tr></table></figure>

<h4 id="Topic-的查看"><a href="#Topic-的查看" class="headerlink" title="Topic 的查看"></a>Topic 的查看</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-topics.sh --zookeeper 172.17.0.87:2181 --list </span><br><span class="line"><span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">$ ./kafka01/bin/kafka-topics.sh --zookeeper 172.17.0.87:2181 --describe --topic <span class="built_in">test</span></span><br><span class="line">Topic:<span class="built_in">test</span>	PartitionCount:1	ReplicationFactor:1	Configs:</span><br><span class="line">4Topic: <span class="built_in">test</span>	Partition: 0	Leader: 2	Replicas: 2	Isr: 2</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<p>第一行是汇总信息，后面的每一行代表一个Partition的信息，多个partition会有多行：</p>
<ul>
<li><code>Topic:test</code> 指明Topic的名称为test</li>
<li><code>PartitionCount:1</code> 指明这个topic的分区数为1</li>
<li><code>ReplicationFactor:1</code> 指明这个topic的复制因子为1，就是复制数为1</li>
<li><code>Configs:</code> 下面是configs，每一行代表一个Partition，我这里的partition是1，所以只有一行<ul>
<li><code>Partition: 0</code> 编号0的partition信息：<ul>
<li><code>Leader:2</code> 该Partition的Leader在Broker.id为2的节点上，往后对于该partition都由该节点负责。</li>
<li><code>Replicas:2</code> 代表该partition所在node的node id列表，我这里只有一个副本，在Broker.id为2的节点上。</li>
<li><code>Isr: 2</code> 同步的副本集合。他是Replicas 列表的子集，它当前是活动的，并被提交给领导者。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>创建3个副本，4个partition的topic，然后再次查看一下该topic的详细信息：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-topics.sh --create --zookeeper 172.17.0.87:2181 --replication-factor 3 --partitions 4 --topic my-replicated-topic</span><br><span class="line">Created topic <span class="string">"my-replicated-topic"</span>.</span><br><span class="line"></span><br><span class="line">$ ./kafka01/bin/kafka-topics.sh --zookeeper 172.17.0.87:2181 --describe --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic	PartitionCount:4	ReplicationFactor:3	Configs:</span><br><span class="line">4Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1</span><br><span class="line">4Topic: my-replicated-topic	Partition: 1	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2</span><br><span class="line">4Topic: my-replicated-topic	Partition: 2	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0</span><br><span class="line">4Topic: my-replicated-topic	Partition: 3	Leader: 2	Replicas: 2,1,0	Isr: 2,1,0</span><br></pre></td></tr></table></figure>

<p>查看一下kafka的log目录：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">pwd</span></span><br><span class="line">/data/kafka-logs/kafka01</span><br><span class="line">$ ll</span><br><span class="line">total 20</span><br><span class="line">-rw-rw-r-- 1 ec2-user ec2-user   4 Dec  9 16:18 cleaner-offset-checkpoint</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-0</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-12</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-15</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-18</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-21</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-24</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-27</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-3</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-30</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-33</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-36</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-39</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-42</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-45</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-48</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:59 __consumer_offsets-6</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 16:51 __consumer_offsets-9</span><br><span class="line">-rw-rw-r-- 1 ec2-user ec2-user   4 Dec 10 10:10 <span class="built_in">log</span>-start-offset-checkpoint</span><br><span class="line">-rw-rw-r-- 1 ec2-user ec2-user  54 Dec  9 13:21 meta.properties</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 15:41 my-replicated-topic-0 格式：TopicName-PartitionID</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 15:41 my-replicated-topic-1</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 15:41 my-replicated-topic-2</span><br><span class="line">drwxrwxr-x 2 ec2-user ec2-user 141 Dec  9 15:41 my-replicated-topic-3</span><br><span class="line">-rw-rw-r-- 1 ec2-user ec2-user 505 Dec 10 10:10 recovery-point-offset-checkpoint</span><br><span class="line">-rw-rw-r-- 1 ec2-user ec2-user 508 Dec 10 10:11 replication-offset-checkpoint</span><br><span class="line"></span><br><span class="line">$ ll my-replicated-topic-0/</span><br><span class="line">total 20480</span><br><span class="line">-rw-rw-r-- 1 ec2-user ec2-user 10485760 Dec  9 15:41 00000000000000000000.index</span><br><span class="line">-rw-rw-r-- 1 ec2-user ec2-user        0 Dec  9 15:41 00000000000000000000.log</span><br><span class="line">-rw-rw-r-- 1 ec2-user ec2-user 10485756 Dec  9 15:41 00000000000000000000.timeindex</span><br><span class="line">-rw-rw-r-- 1 ec2-user ec2-user        0 Dec  9 15:41 leader-epoch-checkpoint</span><br></pre></td></tr></table></figure>



<h4 id="Topic-的创建，手动指定Partitions和Broker的对应关系："><a href="#Topic-的创建，手动指定Partitions和Broker的对应关系：" class="headerlink" title="Topic 的创建，手动指定Partitions和Broker的对应关系："></a>Topic 的创建，手动指定Partitions和Broker的对应关系：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-topics.sh --zookeeper 172.17.0.87:2181 --create --topic my-replicated-topic-manual --replica-assignment 0:1:2,1:2:0,2:0:1</span><br><span class="line">Created topic <span class="string">"my-replicated-topic-manual"</span>.</span><br><span class="line"></span><br><span class="line">$ ./kafka01/bin/kafka-topics.sh --zookeeper 172.17.0.87:2181 --describe --topic my-replicated-topic-manual </span><br><span class="line">Topic:my-replicated-topic-manual	PartitionCount:3	ReplicationFactor:3	Configs:</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 0	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 1	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 2	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1</span><br></pre></td></tr></table></figure>

<div class="note sucess">
            <p>手动replica分配，也就是replica和broker的对应关系</p><p>0:1:2,1:2:0,2:0:1</p><ul><li>所有的数字都代表broker的ID</li><li>表示三个partition，用逗号分割的</li><li>每个partition有三个副本replica，用冒号分割的</li><li>所以partition和broker的对应关系如下：<ul><li>partition 0：id0,id1,id2</li><li>partition 1：id1,id2,id0</li><li>partition 2：id2,id0,id1</li></ul></li></ul>
          </div>

<h4 id="Topic-的修改"><a href="#Topic-的修改" class="headerlink" title="Topic 的修改"></a>Topic 的修改</h4><p>注意：这里的修改只能修改Topic的partition数量（只能增加，不能减少）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$  ./kafka01/bin/kafka-topics.sh --zookeeper 172.17.0.87:2181 --describe --topic my-replicated-topic-manual </span><br><span class="line">Topic:my-replicated-topic-manual	PartitionCount:3	ReplicationFactor:3	Configs:</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 0	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 1	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 2	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1</span><br></pre></td></tr></table></figure>

<p>方式1：只需要指定<code>--partitions</code> 的数量即可：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-topics.sh --zookeeper 172.17.0.87:2181 --alter --topic my-replicated-topic-manual --partitions 4</span><br><span class="line">WARNING: If partitions are increased <span class="keyword">for</span> a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Adding partitions succeeded!</span><br><span class="line"></span><br><span class="line">$ ./kafka01/bin/kafka-topics.sh --zookeeper 172.17.0.87:2181 --describe --topic my-replicated-topic-manual </span><br><span class="line">Topic:my-replicated-topic-manual	PartitionCount:4	ReplicationFactor:3	Configs:</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 0	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 1	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 2	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 3	Leader: 0	Replicas: 0,2,1	Isr: 0,2,1</span><br></pre></td></tr></table></figure>

<p>当减少partitions数量时会报错：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-topics.sh --zookeeper zk01:2181 --describe --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic	PartitionCount:4	ReplicationFactor:3	Configs:</span><br><span class="line">4Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1</span><br><span class="line">4Topic: my-replicated-topic	Partition: 1	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2</span><br><span class="line">4Topic: my-replicated-topic	Partition: 2	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0</span><br><span class="line">4Topic: my-replicated-topic	Partition: 3	Leader: 2	Replicas: 2,1,0	Isr: 2,1,0</span><br><span class="line"></span><br><span class="line">$ ./bin/kafka-topics.sh --zookeeper zk01:2181 --alter --topic my-replicated-topic --partitions 2</span><br><span class="line">WARNING: If partitions are increased <span class="keyword">for</span> a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Error <span class="keyword">while</span> executing topic <span class="built_in">command</span> : The number of partitions <span class="keyword">for</span> a topic can only be increased. Topic my-replicated-topic currently has 4 partitions, 2 would not be an increase.</span><br><span class="line">[2019-12-10 10:22:31,772] ERROR org.apache.kafka.common.errors.InvalidPartitionsException: The number of partitions <span class="keyword">for</span> a topic can only be increased. Topic my-replicated-topic currently has 4 partitions, 2 would not be an increase.</span><br><span class="line"> (kafka.admin.TopicCommand$)</span><br></pre></td></tr></table></figure>



<p>方式2：手工指定replica的分配：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-topics.sh --zookeeper 172.17.0.87:2181 --describe --topic my-replicated-topic-manual </span><br><span class="line">Topic:my-replicated-topic-manual	PartitionCount:4	ReplicationFactor:3	Configs:</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 0	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 1	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 2	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 3	Leader: 0	Replicas: 0,2,1	Isr: 0,2,1</span><br><span class="line"></span><br><span class="line">$ ./kafka01/bin/kafka-topics.sh --zookeeper 172.17.0.87:2181 --alter --topic my-replicated-topic-manual --partitions 5 --replica-assignment 0:1:2,1:2:0,2:0:1,2:1:0,0:2:1</span><br><span class="line">WARNING: If partitions are increased <span class="keyword">for</span> a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Adding partitions succeeded!</span><br><span class="line"></span><br><span class="line">$ ./kafka01/bin/kafka-topics.sh --zookeeper 172.17.0.87:2181 --describe --topic my-replicated-topic-manual </span><br><span class="line">Topic:my-replicated-topic-manual	PartitionCount:5	ReplicationFactor:3	Configs:</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 0	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 1	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 2	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 3	Leader: 0	Replicas: 0,2,1	Isr: 0,2,1</span><br><span class="line">4Topic: my-replicated-topic-manual	Partition: 4	Leader: 0	Replicas: 0,2,1	Isr: 0,2,1</span><br></pre></td></tr></table></figure>

<div class="note danger">
            <p>对已已经分配的partition，你又手工分配的话，是不生效的：</p><p>如上面，我通过alter命令更改了<code>--partitions 4</code>,Partition: 3 已经分配到了0:2:1上了</p><p>我又手工指定replica 的分配<code>--partitions 5 --replica-assignment 0:1:2,1:2:0,2:0:1,2:1:0,0:2:1</code> 这里的Partition: 3应该是我手动指定的：2:1:0，但是实际还是之前分配好的：0:2:1</p>
          </div>

<h4 id="Topic-的删除"><a href="#Topic-的删除" class="headerlink" title="Topic 的删除"></a>Topic 的删除</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-topics.sh --zookeeper 172.17.0.87:2181 --delete --topic my-replicated-topic-manual</span><br><span class="line">Topic my-replicated-topic-manual is marked <span class="keyword">for</span> deletion.</span><br><span class="line">Note: This will have no impact <span class="keyword">if</span> delete.topic.enable is not <span class="built_in">set</span> to <span class="literal">true</span>.</span><br><span class="line"></span><br><span class="line">$ ./kafka01/bin/kafka-topics.sh --zookeeper 172.17.0.87:2181 --list</span><br><span class="line">my-replicated-topic</span><br><span class="line"><span class="built_in">test</span></span><br></pre></td></tr></table></figure>



<h3 id="Topic-的配置的增删改查"><a href="#Topic-的配置的增删改查" class="headerlink" title="Topic 的配置的增删改查"></a>Topic 的配置的增删改查</h3><blockquote>
<p>所有的Topic配置请参考：</p>
<p><a href="https://kafka.apache.org/documentation/#topicconfigs" target="_blank" rel="noopener">https://kafka.apache.org/documentation/#topicconfigs</a></p>
</blockquote>
<p>当然在创建Topic的时候可以通过参数：<code>--config</code> 来增加Topic的配置。</p>
<p>对于Kafka 实体配置（Topic，Client，User，Broker）的配置，同一使用命令行工具<code>kafka-configs.sh</code> 帮助如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-configs.sh </span><br><span class="line">Add/Remove entity config <span class="keyword">for</span> a topic, client, user or broker</span><br><span class="line">Option                                 Description                            </span><br><span class="line">------                                 -----------                            </span><br><span class="line">--add-config &lt;String&gt;                  Key Value pairs of configs to add.     <span class="comment"># 增加配置</span></span><br><span class="line">                                         Square brackets can be used to group </span><br><span class="line">                                         values <span class="built_in">which</span> contain commas: <span class="string">'k1=v1, </span></span><br><span class="line"><span class="string">                                         k2=[v1,v2,v2],k3=v3'</span>. The following  </span><br><span class="line">                                         is a list of valid configurations:   </span><br><span class="line">                                         For entity-type <span class="string">'topics'</span>:    <span class="comment"># topic配置        </span></span><br><span class="line">                                       	cleanup.policy                        </span><br><span class="line">                                       	compression.type                      </span><br><span class="line">                                       	delete.retention.ms                   </span><br><span class="line">                                       	file.delete.delay.ms                  </span><br><span class="line">                                       	flush.messages                        </span><br><span class="line">                                       	flush.ms                              </span><br><span class="line">                                       	follower.replication.throttled.       </span><br><span class="line">                                         replicas                             </span><br><span class="line">                                       	index.interval.bytes                  </span><br><span class="line">                                       	leader.replication.throttled.replicas </span><br><span class="line">                                       	max.message.bytes                     </span><br><span class="line">                                       	message.downconversion.enable         </span><br><span class="line">                                       	message.format.version                </span><br><span class="line">                                       	message.timestamp.difference.max.ms   </span><br><span class="line">                                       	message.timestamp.type                </span><br><span class="line">                                       	min.cleanable.dirty.ratio             </span><br><span class="line">                                       	min.compaction.lag.ms                 </span><br><span class="line">                                       	min.insync.replicas                   </span><br><span class="line">                                       	preallocate                           </span><br><span class="line">                                       	retention.bytes                       </span><br><span class="line">                                       	retention.ms                          </span><br><span class="line">                                       	segment.bytes                         </span><br><span class="line">                                       	segment.index.bytes                   </span><br><span class="line">                                       	segment.jitter.ms                     </span><br><span class="line">                                       	segment.ms                            </span><br><span class="line">                                       	unclean.leader.election.enable        </span><br><span class="line">                                       For entity-type <span class="string">'brokers'</span>:    <span class="comment"># brokers 配置         </span></span><br><span class="line">                                       	log.message.timestamp.type            </span><br><span class="line">                                       	ssl.client.auth                       </span><br><span class="line">                                       	log.retention.ms                      </span><br><span class="line">                                       	sasl.login.refresh.window.jitter      </span><br><span class="line">                                       	sasl.kerberos.ticket.renew.window.    </span><br><span class="line">                                         factor                               </span><br><span class="line">                                       	log.preallocate                       </span><br><span class="line">                                       	log.index.size.max.bytes              </span><br><span class="line">                                       	sasl.login.refresh.window.factor      </span><br><span class="line">                                       	ssl.truststore.type                   </span><br><span class="line">                                       	ssl.keymanager.algorithm              </span><br><span class="line">                                       	log.cleaner.io.buffer.load.factor     </span><br><span class="line">                                       	sasl.login.refresh.min.period.seconds </span><br><span class="line">                                       	ssl.key.password                      </span><br><span class="line">                                       	background.threads                    </span><br><span class="line">                                       	log.retention.bytes                   </span><br><span class="line">                                       	ssl.trustmanager.algorithm            </span><br><span class="line">                                       	log.segment.bytes                     </span><br><span class="line">                                       	log.cleaner.delete.retention.ms       </span><br><span class="line">                                       	log.segment.delete.delay.ms           </span><br><span class="line">                                       	min.insync.replicas                   </span><br><span class="line">                                       	ssl.keystore.location                 </span><br><span class="line">                                       	ssl.cipher.suites                     </span><br><span class="line">                                       	log.roll.jitter.ms                    </span><br><span class="line">                                       	log.cleaner.backoff.ms                </span><br><span class="line">                                       	sasl.jaas.config                      </span><br><span class="line">                                       	principal.builder.class               </span><br><span class="line">                                       	log.flush.interval.ms                 </span><br><span class="line">                                       	log.cleaner.dedupe.buffer.size        </span><br><span class="line">                                       	log.flush.interval.messages           </span><br><span class="line">                                       	advertised.listeners                  </span><br><span class="line">                                       	num.io.threads                        </span><br><span class="line">                                       	listener.security.protocol.map        </span><br><span class="line">                                       	log.message.downconversion.enable     </span><br><span class="line">                                       	sasl.enabled.mechanisms               </span><br><span class="line">                                       	sasl.login.refresh.buffer.seconds     </span><br><span class="line">                                       	ssl.truststore.password               </span><br><span class="line">                                       	listeners                             </span><br><span class="line">                                       	metric.reporters                      </span><br><span class="line">                                       	ssl.protocol                          </span><br><span class="line">                                       	sasl.kerberos.ticket.renew.jitter     </span><br><span class="line">                                       	ssl.keystore.password                 </span><br><span class="line">                                       	sasl.mechanism.inter.broker.protocol  </span><br><span class="line">                                       	log.cleanup.policy                    </span><br><span class="line">                                       	sasl.kerberos.principal.to.local.rules</span><br><span class="line">                                       	sasl.kerberos.min.time.before.relogin </span><br><span class="line">                                       	num.recovery.threads.per.data.dir     </span><br><span class="line">                                       	log.cleaner.io.max.bytes.per.second   </span><br><span class="line">                                       	log.roll.ms                           </span><br><span class="line">                                       	ssl.endpoint.identification.algorithm </span><br><span class="line">                                       	unclean.leader.election.enable        </span><br><span class="line">                                       	message.max.bytes                     </span><br><span class="line">                                       	log.cleaner.threads                   </span><br><span class="line">                                       	log.cleaner.io.buffer.size            </span><br><span class="line">                                       	sasl.kerberos.service.name            </span><br><span class="line">                                       	ssl.provider                          </span><br><span class="line">                                       	follower.replication.throttled.rate   </span><br><span class="line">                                       	log.index.interval.bytes              </span><br><span class="line">                                       	log.cleaner.min.compaction.lag.ms     </span><br><span class="line">                                       	log.message.timestamp.difference.max. </span><br><span class="line">                                         ms                                   </span><br><span class="line">                                       	ssl.enabled.protocols                 </span><br><span class="line">                                       	log.cleaner.min.cleanable.ratio       </span><br><span class="line">                                       	replica.alter.log.dirs.io.max.bytes.  </span><br><span class="line">                                         per.second                           </span><br><span class="line">                                       	ssl.keystore.type                     </span><br><span class="line">                                       	ssl.secure.random.implementation      </span><br><span class="line">                                       	ssl.truststore.location               </span><br><span class="line">                                       	sasl.kerberos.kinit.cmd               </span><br><span class="line">                                       	leader.replication.throttled.rate     </span><br><span class="line">                                       	num.network.threads                   </span><br><span class="line">                                       	compression.type                      </span><br><span class="line">                                       	num.replica.fetchers                  </span><br><span class="line">                                       For entity-type <span class="string">'users'</span>:    <span class="comment"># users配置           </span></span><br><span class="line">                                       	request_percentage                    </span><br><span class="line">                                       	producer_byte_rate                    </span><br><span class="line">                                       	SCRAM-SHA-256                         </span><br><span class="line">                                       	SCRAM-SHA-512                         </span><br><span class="line">                                       	consumer_byte_rate                    </span><br><span class="line">                                       For entity-type <span class="string">'clients'</span>:   <span class="comment"># clients配置          </span></span><br><span class="line">                                       	request_percentage                    </span><br><span class="line">                                       	producer_byte_rate                    </span><br><span class="line">                                       	consumer_byte_rate                    </span><br><span class="line">                                       Entity types <span class="string">'users'</span> and <span class="string">'clients'</span> may </span><br><span class="line">                                         be specified together to update      </span><br><span class="line">                                         config <span class="keyword">for</span> clients of a specific     </span><br><span class="line">                                         user.                                </span><br><span class="line">--alter                                Alter the configuration <span class="keyword">for</span> the entity. <span class="comment"># 修改</span></span><br><span class="line">--bootstrap-server &lt;String: server to  The Kafka server to connect to. This   </span><br><span class="line">  connect to&gt;                            is required <span class="keyword">for</span> describing and       </span><br><span class="line">                                         altering broker configs.             </span><br><span class="line">--<span class="built_in">command</span>-config &lt;String: <span class="built_in">command</span>      Property file containing configs to be </span><br><span class="line">  config property file&gt;                  passed to Admin Client. This is used </span><br><span class="line">                                         only with --bootstrap-server option  </span><br><span class="line">                                         <span class="keyword">for</span> describing and altering broker   </span><br><span class="line">                                         configs.                             </span><br><span class="line">--delete-config &lt;String&gt;               config keys to remove <span class="string">'k1,k2'</span>    <span class="comment"># 删除配置      </span></span><br><span class="line">--describe                             List configs <span class="keyword">for</span> the given entity.    <span class="comment"># 查看配置 </span></span><br><span class="line">--entity-default                       Default entity name <span class="keyword">for</span>                </span><br><span class="line">                                         clients/users/brokers (applies to    </span><br><span class="line">                                         corresponding entity <span class="built_in">type</span> <span class="keyword">in</span> <span class="built_in">command</span> </span><br><span class="line">                                         line)                                </span><br><span class="line">--entity-name &lt;String&gt;                 Name of entity (topic name/client     <span class="comment"># 实体名称 </span></span><br><span class="line">                                         id/user principal name/broker id)    </span><br><span class="line">--entity-type &lt;String&gt;                 Type of entity                        <span class="comment"># 实体类型 </span></span><br><span class="line">                                         (topics/clients/users/brokers)       </span><br><span class="line">--force                                Suppress console prompts               </span><br><span class="line">--<span class="built_in">help</span>                                 Print usage information.               </span><br><span class="line">--zookeeper &lt;String: urls&gt;             REQUIRED: The connection string <span class="keyword">for</span>    <span class="comment"># zk地址</span></span><br><span class="line">                                         the zookeeper connection <span class="keyword">in</span> the form </span><br><span class="line">                                         host:port. Multiple URLS can be      </span><br><span class="line">                                         given to allow fail-over.</span><br></pre></td></tr></table></figure>

<h4 id="列出Topic的配置："><a href="#列出Topic的配置：" class="headerlink" title="列出Topic的配置："></a>列出Topic的配置：</h4><p>以上面创建的Topic：<code>my-replicated-topic</code> 为例：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-configs.sh --zookeeper zk01:2181 --entity-type topics --entity-name my-replicated-topic --describe</span><br><span class="line">Configs <span class="keyword">for</span> topic <span class="string">'my-replicated-topic'</span> are </span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认没有配置</span></span><br></pre></td></tr></table></figure>

<h4 id="增加Topic的配置："><a href="#增加Topic的配置：" class="headerlink" title="增加Topic的配置："></a>增加Topic的配置：</h4><blockquote>
<p>参考：<a href="https://kafka.apache.org/documentation/#topicconfigs" target="_blank" rel="noopener">https://kafka.apache.org/documentation/#topicconfigs</a></p>
</blockquote>
<table>
<thead>
<tr>
<th align="left">NAME</th>
<th align="left">DESCRIPTION</th>
<th align="left">TYPE</th>
<th align="left">DEFAULT</th>
<th align="left">VALID VALUES</th>
<th align="left">SERVER DEFAULT PROPERTY</th>
<th align="left">IMPORTANCE</th>
</tr>
</thead>
<tbody><tr>
<td align="left">cleanup.policy</td>
<td align="left">A string that is either “delete” or “compact” or both. This string designates the retention policy to use on old log segments. The default policy (“delete”) will discard old segments when their retention time or size limit has been reached. The “compact” setting will enable <a href="https://kafka.apache.org/documentation/#compaction" target="_blank" rel="noopener">log compaction</a> on the topic.</td>
<td align="left">list</td>
<td align="left">delete</td>
<td align="left">[compact, delete]</td>
<td align="left">log.cleanup.policy</td>
<td align="left">medium</td>
</tr>
<tr>
<td align="left">compression.type</td>
<td align="left">Specify the final compression type for a given topic. This configuration accepts the standard compression codecs (‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’). It additionally accepts ‘uncompressed’ which is equivalent to no compression; and ‘producer’ which means retain the original compression codec set by the producer.</td>
<td align="left">string</td>
<td align="left">producer</td>
<td align="left">[uncompressed, zstd, lz4, snappy, gzip, producer]</td>
<td align="left">compression.type</td>
<td align="left">medium</td>
</tr>
</tbody></table>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> <span class="comment"># 增加配置：cleanup.policy=compact</span></span><br><span class="line"> $ ./bin/kafka-configs.sh --zookeeper zk01:2181 --entity-type topics --entity-name my-replicated-topic --alter --add-config cleanup.policy=compact</span><br><span class="line">Completed Updating config <span class="keyword">for</span> entity: topic <span class="string">'my-replicated-topic'</span>.</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">$ ./bin/kafka-configs.sh --zookeeper zk01:2181 --entity-type topics --entity-name my-replicated-topic --describe</span><br><span class="line">Configs <span class="keyword">for</span> topic <span class="string">'my-replicated-topic'</span> are cleanup.policy=compact</span><br><span class="line"><span class="comment"># 增加配置：compression.type=gzip</span></span><br><span class="line">$ ./bin/kafka-configs.sh --zookeeper zk01:2181 --entity-type topics --entity-name my-replicated-topic --alter --add-config compression.type=gzip</span><br><span class="line">Completed Updating config <span class="keyword">for</span> entity: topic <span class="string">'my-replicated-topic'</span>.</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">$ ./bin/kafka-configs.sh --zookeeper zk01:2181 --entity-type topics --entity-name my-replicated-topic --describe</span><br><span class="line">Configs <span class="keyword">for</span> topic <span class="string">'my-replicated-topic'</span> are cleanup.policy=compact,compression.type=gzip</span><br></pre></td></tr></table></figure>

<h4 id="修改Topic的现有配置"><a href="#修改Topic的现有配置" class="headerlink" title="修改Topic的现有配置"></a>修改Topic的现有配置</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改：cleanup.policy=delete</span></span><br><span class="line">$ ./bin/kafka-configs.sh --zookeeper zk01:2181 --entity-type topics --entity-name my-replicated-topic --alter --add-config cleanup.policy=delete</span><br><span class="line">Completed Updating config <span class="keyword">for</span> entity: topic <span class="string">'my-replicated-topic'</span>.</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">$ ./bin/kafka-configs.sh --zookeeper zk01:2181 --entity-type topics --entity-name my-replicated-topic --describe</span><br><span class="line">Configs <span class="keyword">for</span> topic <span class="string">'my-replicated-topic'</span> are compression.type=gzip,cleanup.policy=delete</span><br></pre></td></tr></table></figure>

<h4 id="删除Topic配置"><a href="#删除Topic配置" class="headerlink" title="删除Topic配置"></a>删除Topic配置</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注意，删除配置时，只需要指定配置的KEY即可，不需要也不能指定KEY=VALUE</span></span><br><span class="line">$ ./bin/kafka-configs.sh --zookeeper zk01:2181 --entity-type topics --entity-name my-replicated-topic --alter --delete-config compression.type=gzip</span><br><span class="line">Invalid config(s): compression.type=gzip <span class="comment"># 报错了</span></span><br><span class="line"></span><br><span class="line">$ ./bin/kafka-configs.sh --zookeeper zk01:2181 --entity-type topics --entity-name my-replicated-topic --alter --delete-config compression.type</span><br><span class="line">Completed Updating config <span class="keyword">for</span> entity: topic <span class="string">'my-replicated-topic'</span>.</span><br><span class="line"></span><br><span class="line">$ ./bin/kafka-configs.sh --zookeeper zk01:2181 --entity-type topics --entity-name my-replicated-topic --describe</span><br><span class="line">Configs <span class="keyword">for</span> topic <span class="string">'my-replicated-topic'</span> are cleanup.policy=delete</span><br></pre></td></tr></table></figure>





<h3 id="Kafka-的生产者和消费者测试"><a href="#Kafka-的生产者和消费者测试" class="headerlink" title="Kafka 的生产者和消费者测试"></a>Kafka 的生产者和消费者测试</h3><p>我们使用上面已经创建好的Topic：test</p>
<p>首先查看<code>kafka-console-consumer.sh</code>脚本的帮助：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-console-consumer.sh </span><br><span class="line">The console consumer is a tool that reads data from Kafka and outputs it to standard output.</span><br><span class="line">Option                                   Description                            </span><br><span class="line">------                                   -----------                            </span><br><span class="line">--bootstrap-server &lt;String: server to    REQUIRED: The server(s) to connect to. </span><br><span class="line">  connect to&gt;      <span class="comment"># 指定kafka的地址                                                             </span></span><br><span class="line">--consumer-property &lt;String:             A mechanism to pass user-defined       </span><br><span class="line">  consumer_prop&gt;                           properties <span class="keyword">in</span> the form key=value to  </span><br><span class="line">                                           the consumer.                        </span><br><span class="line">--consumer.config &lt;String: config file&gt;  Consumer config properties file. Note  </span><br><span class="line">                                           that [consumer-property] takes       </span><br><span class="line">                                           precedence over this config.         </span><br><span class="line">--<span class="built_in">enable</span>-systest-events                  Log lifecycle events of the consumer   </span><br><span class="line">                                           <span class="keyword">in</span> addition to logging consumed      </span><br><span class="line">                                           messages. (This is specific <span class="keyword">for</span>      </span><br><span class="line">                                           system tests.)                       </span><br><span class="line">--formatter &lt;String: class&gt;              The name of a class to use <span class="keyword">for</span>         </span><br><span class="line">                                           formatting kafka messages <span class="keyword">for</span>        </span><br><span class="line">                                           display. (default: kafka.tools.      </span><br><span class="line">                                           DefaultMessageFormatter)             </span><br><span class="line">--from-beginning                         If the consumer does not already have  <span class="comment"># 从开头开始消费</span></span><br><span class="line">                                           an established offset to consume     </span><br><span class="line">                                           from, start with the earliest        </span><br><span class="line">                                           message present <span class="keyword">in</span> the <span class="built_in">log</span> rather    </span><br><span class="line">                                           than the latest message.             </span><br><span class="line">--group &lt;String: consumer group id&gt;      The consumer group id of the consumer. <span class="comment"># 指定消费者组</span></span><br><span class="line">--isolation-level &lt;String&gt;               Set to read_committed <span class="keyword">in</span> order to      </span><br><span class="line">                                           filter out transactional messages    </span><br><span class="line">                                           <span class="built_in">which</span> are not committed. Set to      </span><br><span class="line">                                           read_uncommittedto <span class="built_in">read</span> all          </span><br><span class="line">                                           messages. (default: read_uncommitted)</span><br><span class="line">--key-deserializer &lt;String:                                                     </span><br><span class="line">  deserializer <span class="keyword">for</span> key&gt;                                                         </span><br><span class="line">--max-messages &lt;Integer: num_messages&gt;   The maximum number of messages to      </span><br><span class="line">                                           consume before exiting. If not <span class="built_in">set</span>,  </span><br><span class="line">                                           consumption is continual.            </span><br><span class="line">--offset &lt;String: consume offset&gt;        The offset id to consume from (a non-  </span><br><span class="line">                                           negative number), or <span class="string">'earliest'</span>      </span><br><span class="line">                                           <span class="built_in">which</span> means from beginning, or       </span><br><span class="line">                                           <span class="string">'latest'</span> <span class="built_in">which</span> means from end        </span><br><span class="line">                                           (default: latest)                    </span><br><span class="line">--partition &lt;Integer: partition&gt;         The partition to consume from.         </span><br><span class="line">                                           Consumption starts from the end of   </span><br><span class="line">                                           the partition unless <span class="string">'--offset'</span> is   </span><br><span class="line">                                           specified.                           </span><br><span class="line">--property &lt;String: prop&gt;                The properties to initialize the       </span><br><span class="line">                                           message formatter. Default           </span><br><span class="line">                                           properties include:                  </span><br><span class="line">                                         	print.timestamp=<span class="literal">true</span>|<span class="literal">false</span>            </span><br><span class="line">                                         	print.key=<span class="literal">true</span>|<span class="literal">false</span>                  </span><br><span class="line">                                         	print.value=<span class="literal">true</span>|<span class="literal">false</span>                </span><br><span class="line">                                         	key.separator=&lt;key.separator&gt;         </span><br><span class="line">                                         	line.separator=&lt;line.separator&gt;       </span><br><span class="line">                                         	key.deserializer=&lt;key.deserializer&gt;   </span><br><span class="line">                                         	value.deserializer=&lt;value.            </span><br><span class="line">                                           deserializer&gt;                        </span><br><span class="line">                                         Users can also pass <span class="keyword">in</span> customized      </span><br><span class="line">                                           properties <span class="keyword">for</span> their formatter; more </span><br><span class="line">                                           specifically, users can pass <span class="keyword">in</span>      </span><br><span class="line">                                           properties keyed with <span class="string">'key.          </span></span><br><span class="line"><span class="string">                                           deserializer.'</span> and <span class="string">'value.           </span></span><br><span class="line"><span class="string">                                           deserializer.'</span> prefixes to configure </span><br><span class="line">                                           their deserializers.                 </span><br><span class="line">--skip-message-on-error                  If there is an error when processing a </span><br><span class="line">                                           message, skip it instead of halt.    </span><br><span class="line">--timeout-ms &lt;Integer: timeout_ms&gt;       If specified, <span class="built_in">exit</span> <span class="keyword">if</span> no message is    </span><br><span class="line">                                           available <span class="keyword">for</span> consumption <span class="keyword">for</span> the    </span><br><span class="line">                                           specified interval.                  </span><br><span class="line">--topic &lt;String: topic&gt;                  The topic id to consume on.            </span><br><span class="line">--value-deserializer &lt;String:                                                   </span><br><span class="line">  deserializer <span class="keyword">for</span> values&gt;                                                      </span><br><span class="line">--whitelist &lt;String: whitelist&gt;          Whitelist of topics to include <span class="keyword">for</span>     </span><br><span class="line">                                           consumption.</span><br></pre></td></tr></table></figure>

<p><code>kafka-console-producer.sh</code> 脚本帮助：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-console-producer.sh </span><br><span class="line">Read data from standard input and publish it to Kafka.</span><br><span class="line">Option                                   Description                            </span><br><span class="line">------                                   -----------                            </span><br><span class="line">--batch-size &lt;Integer: size&gt;             Number of messages to send <span class="keyword">in</span> a single </span><br><span class="line">                                           batch <span class="keyword">if</span> they are not being sent     </span><br><span class="line">                                           synchronously. (default: 200)        </span><br><span class="line">--broker-list &lt;String: broker-list&gt;      REQUIRED: The broker list string <span class="keyword">in</span>    <span class="comment"># 指定kafka broker地址</span></span><br><span class="line">                                           the form HOST1:PORT1,HOST2:PORT2.    </span><br><span class="line">--compression-codec [String:             The compression codec: either <span class="string">'none'</span>,  </span><br><span class="line">  compression-codec]                       <span class="string">'gzip'</span>, <span class="string">'snappy'</span>, or <span class="string">'lz4'</span>.If        </span><br><span class="line">                                           specified without value, <span class="keyword">then</span> it     </span><br><span class="line">                                           defaults to <span class="string">'gzip'</span>                   </span><br><span class="line">--line-reader &lt;String: reader_class&gt;     The class name of the class to use <span class="keyword">for</span> </span><br><span class="line">                                           reading lines from standard <span class="keyword">in</span>. By   </span><br><span class="line">                                           default each line is <span class="built_in">read</span> as a       </span><br><span class="line">                                           separate message. (default: kafka.   </span><br><span class="line">                                           tools.                               </span><br><span class="line">                                           ConsoleProducer<span class="variable">$LineMessageReader</span>)   </span><br><span class="line">--max-block-ms &lt;Long: max block on       The max time that the producer will    </span><br><span class="line">  send&gt;                                    block <span class="keyword">for</span> during a send request      </span><br><span class="line">                                           (default: 60000)                     </span><br><span class="line">--max-memory-bytes &lt;Long: total memory   The total memory used by the producer  </span><br><span class="line">  <span class="keyword">in</span> bytes&gt;                                to buffer records waiting to be sent </span><br><span class="line">                                           to the server. (default: 33554432)   </span><br><span class="line">--max-partition-memory-bytes &lt;Long:      The buffer size allocated <span class="keyword">for</span> a        </span><br><span class="line">  memory <span class="keyword">in</span> bytes per partition&gt;           partition. When records are received </span><br><span class="line">                                           <span class="built_in">which</span> are smaller than this size the </span><br><span class="line">                                           producer will attempt to             </span><br><span class="line">                                           optimistically group them together   </span><br><span class="line">                                           until this size is reached.          </span><br><span class="line">                                           (default: 16384)                     </span><br><span class="line">--message-send-max-retries &lt;Integer&gt;     Brokers can fail receiving the message </span><br><span class="line">                                           <span class="keyword">for</span> multiple reasons, and being      </span><br><span class="line">                                           unavailable transiently is just one  </span><br><span class="line">                                           of them. This property specifies the </span><br><span class="line">                                           number of retires before the         </span><br><span class="line">                                           producer give up and drop this       </span><br><span class="line">                                           message. (default: 3)                </span><br><span class="line">--metadata-expiry-ms &lt;Long: metadata     The period of time <span class="keyword">in</span> milliseconds     </span><br><span class="line">  expiration interval&gt;                     after <span class="built_in">which</span> we force a refresh of    </span><br><span class="line">                                           metadata even <span class="keyword">if</span> we haven<span class="string">'t seen any </span></span><br><span class="line"><span class="string">                                           leadership changes. (default: 300000)</span></span><br><span class="line"><span class="string">--producer-property &lt;String:             A mechanism to pass user-defined       </span></span><br><span class="line"><span class="string">  producer_prop&gt;                           properties in the form key=value to  </span></span><br><span class="line"><span class="string">                                           the producer.                        </span></span><br><span class="line"><span class="string">--producer.config &lt;String: config file&gt;  Producer config properties file. Note  </span></span><br><span class="line"><span class="string">                                           that [producer-property] takes       </span></span><br><span class="line"><span class="string">                                           precedence over this config.         </span></span><br><span class="line"><span class="string">--property &lt;String: prop&gt;                A mechanism to pass user-defined       </span></span><br><span class="line"><span class="string">                                           properties in the form key=value to  </span></span><br><span class="line"><span class="string">                                           the message reader. This allows      </span></span><br><span class="line"><span class="string">                                           custom configuration for a user-     </span></span><br><span class="line"><span class="string">                                           defined message reader.              </span></span><br><span class="line"><span class="string">--request-required-acks &lt;String:         The required acks of the producer      </span></span><br><span class="line"><span class="string">  request required acks&gt;                   requests (default: 1)                </span></span><br><span class="line"><span class="string">--request-timeout-ms &lt;Integer: request   The ack timeout of the producer        </span></span><br><span class="line"><span class="string">  timeout ms&gt;                              requests. Value must be non-negative </span></span><br><span class="line"><span class="string">                                           and non-zero (default: 1500)         </span></span><br><span class="line"><span class="string">--retry-backoff-ms &lt;Integer&gt;             Before each retry, the producer        </span></span><br><span class="line"><span class="string">                                           refreshes the metadata of relevant   </span></span><br><span class="line"><span class="string">                                           topics. Since leader election takes  </span></span><br><span class="line"><span class="string">                                           a bit of time, this property         </span></span><br><span class="line"><span class="string">                                           specifies the amount of time that    </span></span><br><span class="line"><span class="string">                                           the producer waits before refreshing </span></span><br><span class="line"><span class="string">                                           the metadata. (default: 100)         </span></span><br><span class="line"><span class="string">--socket-buffer-size &lt;Integer: size&gt;     The size of the tcp RECV size.         </span></span><br><span class="line"><span class="string">                                           (default: 102400)                    </span></span><br><span class="line"><span class="string">--sync                                   If set message send requests to the    </span></span><br><span class="line"><span class="string">                                           brokers are synchronously, one at a  </span></span><br><span class="line"><span class="string">                                           time as they arrive.                 </span></span><br><span class="line"><span class="string">--timeout &lt;Integer: timeout_ms&gt;          If set and the producer is running in  </span></span><br><span class="line"><span class="string">                                           asynchronous mode, this gives the    </span></span><br><span class="line"><span class="string">                                           maximum amount of time a message     </span></span><br><span class="line"><span class="string">                                           will queue awaiting sufficient batch </span></span><br><span class="line"><span class="string">                                           size. The value is given in ms.      </span></span><br><span class="line"><span class="string">                                           (default: 1000)                      </span></span><br><span class="line"><span class="string">--topic &lt;String: topic&gt;                  REQUIRED: The topic id to produce      </span></span><br><span class="line"><span class="string">                                           messages to.</span></span><br></pre></td></tr></table></figure>

<p>首先打开两个消费者，属于同一个组；</p>
<p>打开两个SSH终端，分别执行下面命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /data/knner</span><br><span class="line">$ ./kafka01/bin/kafka-console-consumer.sh --bootstrap-server 172.17.0.87:9092 --from-beginning --topic <span class="built_in">test</span> --group <span class="built_in">test</span>-group</span><br></pre></td></tr></table></figure>

<p>然后在另打开一个SSH终端，执行下面命令充当生产者：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-console-producer.sh --broker-list 172.17.0.87:9092,172.17.0.87:9093,172.17.0.87:9094 --topic <span class="built_in">test</span></span><br><span class="line">&gt;test001 <span class="comment"># 输入两条消息</span></span><br><span class="line">&gt;test002</span><br></pre></td></tr></table></figure>

<p>我们查看两个消费者中只有其中一个消费者消费了这两条消息，另一个没有消费。因为这两个消费者同属于一个组的。一条消息只能被组内个一个消费者消费。</p>
<p>我们继续测试，停掉刚才的两个消费者，然后分别执行下面命令，来运行两个消费者，但是，属于不同的组</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 消费者1：消费者组test-group01</span></span><br><span class="line">$ ./kafka01/bin/kafka-console-consumer.sh --bootstrap-server 172.17.0.87:9092 --from-beginning --topic <span class="built_in">test</span> --group <span class="built_in">test</span>-group</span><br><span class="line"><span class="comment"># 消费者2：消费者组test-group02</span></span><br><span class="line">$ ./kafka01/bin/kafka-console-consumer.sh --bootstrap-server 172.17.0.87:9092 --from-beginning --topic <span class="built_in">test</span> --group <span class="built_in">test</span>-group-01</span><br></pre></td></tr></table></figure>

<p>然后在通过刚才的消费者再输入另外两条消息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">test003</span><br><span class="line">test004</span><br></pre></td></tr></table></figure>

<p>我们发现这两个消费者都消费了这两条消息；因为这两个消费者属于不同的组。一个消息被多个消费者组多次消费，但是只能被组内个一个消费者消费。</p>
<h4 id="检查消费者位置以及分区偏移"><a href="#检查消费者位置以及分区偏移" class="headerlink" title="检查消费者位置以及分区偏移"></a>检查消费者位置以及分区偏移</h4><p><code>kafka-consumer-groups.sh</code> 命令帮助：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-consumer-groups.sh</span><br><span class="line">List all consumer groups, describe a consumer group, delete consumer group info, or reset consumer group offsets.</span><br><span class="line">Option                                  Description                            </span><br><span class="line">------                                  -----------                            </span><br><span class="line">--all-topics                            Consider all topics assigned to a      </span><br><span class="line">                                          group <span class="keyword">in</span> the `reset-offsets` process.</span><br><span class="line">--bootstrap-server &lt;String: server to   REQUIRED: The server(s) to connect to. </span><br><span class="line">  connect to&gt;    <span class="comment"># 指定kafka 服务器                                                              </span></span><br><span class="line">--by-duration &lt;String: duration&gt;        Reset offsets to offset by duration    </span><br><span class="line">                                          from current timestamp. Format:      </span><br><span class="line">                                          <span class="string">'PnDTnHnMnS'</span>                         </span><br><span class="line">--<span class="built_in">command</span>-config &lt;String: <span class="built_in">command</span>       Property file containing configs to be </span><br><span class="line">  config property file&gt;                   passed to Admin Client and Consumer. </span><br><span class="line">--delete                                Pass <span class="keyword">in</span> groups to delete topic         </span><br><span class="line">                                          partition offsets and ownership      </span><br><span class="line">                                          information over the entire consumer </span><br><span class="line">                                          group. For instance --group g1 --    </span><br><span class="line">                                          group g2     <span class="comment"># 清除消费者信息，包括分片偏移量，所有者信息                        </span></span><br><span class="line">--describe                              Describe consumer group and list       </span><br><span class="line">                                          offset lag (number of messages not   </span><br><span class="line">                                          yet processed) related to given      </span><br><span class="line">                                          group.        <span class="comment"># 查看描述信息                       </span></span><br><span class="line">--dry-run                               Only show results without executing    </span><br><span class="line">                                          changes on Consumer Groups.          </span><br><span class="line">                                          Supported operations: reset-offsets. </span><br><span class="line">--execute                               Execute operation. Supported           </span><br><span class="line">                                          operations: reset-offsets.           </span><br><span class="line">--<span class="built_in">export</span>                                Export operation execution to a CSV    </span><br><span class="line">                                          file. Supported operations: reset-   </span><br><span class="line">                                          offsets.      <span class="comment"># 导出                       </span></span><br><span class="line">--from-file &lt;String: path to CSV file&gt;  Reset offsets to values defined <span class="keyword">in</span> CSV </span><br><span class="line">                                          file.                                </span><br><span class="line">--group &lt;String: consumer group&gt;        The consumer group we wish to act on.  </span><br><span class="line">--list                                  List all consumer groups.   <span class="comment"># 指定消费者组           </span></span><br><span class="line">--members                               Describe members of the group. This    </span><br><span class="line">                                          option may be used with <span class="string">'--describe'</span> </span><br><span class="line">                                          and <span class="string">'--bootstrap-server'</span> options     </span><br><span class="line">                                          only.                                </span><br><span class="line">                                        Example: --bootstrap-server localhost: </span><br><span class="line">                                          9092 --describe --group group1 --    </span><br><span class="line">                                          members                              </span><br><span class="line">--offsets                               Describe the group and list all topic  </span><br><span class="line">                                          partitions <span class="keyword">in</span> the group along with   </span><br><span class="line">                                          their offset lag. This is the        </span><br><span class="line">                                          default sub-action of and may be     </span><br><span class="line">                                          used with <span class="string">'--describe'</span> and <span class="string">'--       </span></span><br><span class="line"><span class="string">                                          bootstrap-server'</span> options only.      </span><br><span class="line">                                        Example: --bootstrap-server localhost: </span><br><span class="line">                                          9092 --describe --group group1 --    </span><br><span class="line">                                          offsets                              </span><br><span class="line">--reset-offsets                         Reset offsets of consumer group.       </span><br><span class="line">                                          Supports one consumer group at the   </span><br><span class="line">                                          time, and instances should be        </span><br><span class="line">                                          inactive                             </span><br><span class="line">                                        Has 2 execution options: --dry-run     </span><br><span class="line">                                          (the default) to plan <span class="built_in">which</span> offsets  </span><br><span class="line">                                          to reset, and --execute to update    </span><br><span class="line">                                          the offsets. Additionally, the --    </span><br><span class="line">                                          <span class="built_in">export</span> option is used to <span class="built_in">export</span> the  </span><br><span class="line">                                          results to a CSV format.             </span><br><span class="line">                                        You must choose one of the following   </span><br><span class="line">                                          reset specifications: --to-datetime, </span><br><span class="line">                                          --by-period, --to-earliest, --to-    </span><br><span class="line">                                          latest, --<span class="built_in">shift</span>-by, --from-file, --  </span><br><span class="line">                                          to-current.                          </span><br><span class="line">                                        To define the scope use --all-topics   </span><br><span class="line">                                          or --topic. One scope must be        </span><br><span class="line">                                          specified unless you use <span class="string">'--from-    </span></span><br><span class="line"><span class="string">                                          file'</span>.                               </span><br><span class="line">--<span class="built_in">shift</span>-by &lt;Long: number-of-offsets&gt;    Reset offsets shifting current offset  </span><br><span class="line">                                          by <span class="string">'n'</span>, <span class="built_in">where</span> <span class="string">'n'</span> can be positive or </span><br><span class="line">                                          negative.                            </span><br><span class="line">--state                                 Describe the group state. This option  </span><br><span class="line">                                          may be used with <span class="string">'--describe'</span> and <span class="string">'--</span></span><br><span class="line"><span class="string">                                          bootstrap-server'</span> options only.      </span><br><span class="line">                                        Example: --bootstrap-server localhost: </span><br><span class="line">                                          9092 --describe --group group1 --    </span><br><span class="line">                                          state                                </span><br><span class="line">--timeout &lt;Long: timeout (ms)&gt;          The timeout that can be <span class="built_in">set</span> <span class="keyword">for</span> some   </span><br><span class="line">                                          use cases. For example, it can be    </span><br><span class="line">                                          used when describing the group to    </span><br><span class="line">                                          specify the maximum amount of time   </span><br><span class="line">                                          <span class="keyword">in</span> milliseconds to <span class="built_in">wait</span> before the   </span><br><span class="line">                                          group stabilizes (when the group is  </span><br><span class="line">                                          just created, or is going through    </span><br><span class="line">                                          some changes). (default: 5000)       </span><br><span class="line">--to-current                            Reset offsets to current offset.       </span><br><span class="line">--to-datetime &lt;String: datetime&gt;        Reset offsets to offset from datetime. </span><br><span class="line">                                          Format: <span class="string">'YYYY-MM-DDTHH:mm:SS.sss'</span>    </span><br><span class="line">--to-earliest                           Reset offsets to earliest offset.      </span><br><span class="line">--to-latest                             Reset offsets to latest offset.        </span><br><span class="line">--to-offset &lt;Long: offset&gt;              Reset offsets to a specific offset.    </span><br><span class="line">--topic &lt;String: topic&gt;                 The topic whose consumer group         </span><br><span class="line">                                          information should be deleted or     </span><br><span class="line">                                          topic whose should be included <span class="keyword">in</span>    </span><br><span class="line">                                          the reset offset process. In `reset- </span><br><span class="line">                                          offsets` <span class="keyword">case</span>, partitions can be     </span><br><span class="line">                                          specified using this format: `topic1:</span><br><span class="line">                                          0,1,2`, <span class="built_in">where</span> 0,1,2 are the          </span><br><span class="line">                                          partition to be included <span class="keyword">in</span> the      </span><br><span class="line">                                          process. Reset-offsets also supports </span><br><span class="line">                                          multiple topic inputs.               </span><br><span class="line">--verbose                               Provide additional information, <span class="keyword">if</span>     </span><br><span class="line">                                          any, when describing the group. This </span><br><span class="line">                                          option may be used with <span class="string">'--          </span></span><br><span class="line"><span class="string">                                          offsets'</span>/<span class="string">'--members'</span>/<span class="string">'--state'</span> and   </span><br><span class="line">                                          <span class="string">'--bootstrap-server'</span> options only.   </span><br><span class="line">                                        Example: --bootstrap-server localhost: </span><br><span class="line">                                          9092 --describe --group group1 --    </span><br><span class="line">                                          members --verbose</span><br></pre></td></tr></table></figure>

<p>列出消费者组：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --list</span><br><span class="line"><span class="built_in">test</span>-group-01</span><br><span class="line"><span class="built_in">test</span>-group</span><br></pre></td></tr></table></figure>

<p>查看具体消费者组的描述信息：</p>
<p>包括分区偏移量，最后的分区偏移量，所在host，client id</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --describe --group <span class="built_in">test</span>-group-01</span><br><span class="line"></span><br><span class="line">TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                     HOST            CLIENT-ID</span><br><span class="line"><span class="built_in">test</span>            0          5               5               0               consumer-1-1f2cc552-4df7-47ad-b112-52e2a87ee697 /172.17.0.87    consumer-1</span><br></pre></td></tr></table></figure>

<p>如何指定CLIENT-ID呢？</p>
<p>我们关闭之前的所有消费者，另外启动两个，如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./kafka01/bin/kafka-console-consumer.sh --bootstrap-server kafka01:9092,kafka02:9093,kafka03:9094 --from-beginning --topic <span class="built_in">test</span> --group <span class="built_in">test</span>-group-01 --consumer-property client.id=001</span><br><span class="line"></span><br><span class="line">$ ./kafka01/bin/kafka-console-consumer.sh --bootstrap-server kafka01:9092,kafka02:9093,kafka03:9094 --from-beginning --topic <span class="built_in">test</span> --group <span class="built_in">test</span>-group-01 --consumer-property client.id=002</span><br></pre></td></tr></table></figure>

<p>此时查看消费者组：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --describe --group <span class="built_in">test</span>-group-01</span><br><span class="line"></span><br><span class="line">TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                              HOST            CLIENT-ID</span><br><span class="line"><span class="built_in">test</span>            0          5               5               0               001-3d946a1e-8cb3-4d4d-8395-4932a342fc89 /172.17.0.87    001</span><br></pre></td></tr></table></figure>

<p>发现还是只有一个，加上<code>--members</code>：此选项提供使用者组中所有活动成员的列表</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --describe --group <span class="built_in">test</span>-group-01 --members</span><br><span class="line"></span><br><span class="line">CONSUMER-ID                              HOST            CLIENT-ID       <span class="comment">#PARTITIONS     </span></span><br><span class="line">001-3d946a1e-8cb3-4d4d-8395-4932a342fc89 /172.17.0.87    001             1               </span><br><span class="line">002-303d3b20-d08c-4da6-a492-913fb2d3a762 /172.17.0.87    002             0</span><br></pre></td></tr></table></figure>

<p><code>--members --verbose</code>：除了上述“ –members”选项报告的信息之外，此选项还提供分配给每个成员的分区</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --describe --group <span class="built_in">test</span>-group-01 --members --verbose</span><br><span class="line"></span><br><span class="line">CONSUMER-ID                              HOST            CLIENT-ID       <span class="comment">#PARTITIONS     ASSIGNMENT</span></span><br><span class="line">001-3d946a1e-8cb3-4d4d-8395-4932a342fc89 /172.17.0.87    001             1               <span class="built_in">test</span>(0)</span><br><span class="line">002-303d3b20-d08c-4da6-a492-913fb2d3a762 /172.17.0.87    002             0               -</span><br></pre></td></tr></table></figure>

<p><code>--offsets</code>：这是默认的describe选项，并提供与“ –describe”选项相同的输出</p>
<p><code>--state</code>：此选项提供有用的组级别信息。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --describe --group <span class="built_in">test</span>-group-01 --state</span><br><span class="line"></span><br><span class="line">COORDINATOR (ID)          ASSIGNMENT-STRATEGY       STATE                <span class="comment">#MEMBERS</span></span><br><span class="line">172.17.0.87:9092 (0)      range                     Stable               2</span><br></pre></td></tr></table></figure>

<p>要手动删除一个或多个使用者组，可以使用“ –delete”选项：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注意，需要先停止消费者</span></span><br><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --delete --group <span class="built_in">test</span>-group-01</span><br><span class="line">Error: Deletion of some consumer groups failed:</span><br><span class="line">* Group <span class="string">'test-group-01'</span> could not be deleted due to: NON_EMPTY_GROUP</span><br><span class="line"></span><br><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --delete --group <span class="built_in">test</span>-group-01</span><br><span class="line">Deletion of requested consumer groups (<span class="string">'test-group-01'</span>) was successful.</span><br><span class="line"></span><br><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --list</span><br><span class="line"><span class="built_in">test</span></span><br><span class="line"><span class="built_in">test</span>-group</span><br></pre></td></tr></table></figure>

<h4 id="重置分区偏移"><a href="#重置分区偏移" class="headerlink" title="重置分区偏移"></a>重置分区偏移</h4><p>要重置使用者组的偏移，可以使用“ –reset-offsets”选项。此选项当时支持一个消费者组。它需要定义以下范围：–all-topics或–topic。除非您使用“ –from-file”方案，否则必须选择一个范围。另外，首先请确保使用者实例处于非活动状态。</p>
<p>它具有3个执行选项：</p>
<ul>
<li>（默认）以显示要重置的偏移量。</li>
<li>–execute：执行–reset-offsets过程。</li>
<li>–export：将结果导出为CSV格式。</li>
</ul>
<p>–reset-offsets还具有以下场景可供选择（必须选择至少一个场景）：</p>
<ul>
<li>–to-datetime &lt;String：datetime&gt;：将偏移量重置为与datetime的偏移量。格式：“ YYYY-MM-DDTHH:mm:SS.sss”</li>
<li>–to-earliest ：将偏移量重置为最早的偏移量。</li>
<li>–to-latest：将偏移量重置为最新偏移量。</li>
<li>–shift-by &lt;Long: number-of-offsets&gt;：重置偏移量，将当前偏移量偏移“ n”，其中“ n”可以为正或负。</li>
<li>–from-file：将偏移量重置为CSV文件中定义的值。</li>
<li>–to-current：将偏移量重置为当前偏移量。</li>
<li>–by-duration &lt;String：duration&gt;：将偏移量重置为从当前时间戳记的持续时间偏移量。格式：“ PnDTnHnMnS”</li>
<li>–to-offset：将偏移量重置为特定偏移量。</li>
</ul>
<p>请注意，超出范围的偏移量将调整为可用的偏移量结束。例如，如果偏移量结束为10，偏移量请求为15，则实际上将选择偏移量为10。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看：CURRENT-OFFSET=LOG-END-OFFSET 也就是全部消费完了</span></span><br><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --describe --group <span class="built_in">test</span>-group</span><br><span class="line">Consumer group <span class="string">'test-group'</span> has no active members.</span><br><span class="line"></span><br><span class="line">TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID</span><br><span class="line"><span class="built_in">test</span>            0          5               5               0               -               -               -</span><br><span class="line"><span class="comment"># 重置偏移量到最早的</span></span><br><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --reset-offsets --group <span class="built_in">test</span>-group --topic <span class="built_in">test</span> --to-earliest </span><br><span class="line">WARN: No action will be performed as the --execute option is missing.In a future major release, the default behavior of this <span class="built_in">command</span> will be to prompt the user before executing the reset rather than doing a dry run. You should add the --dry-run option explicitly <span class="keyword">if</span> you are scripting this <span class="built_in">command</span> and want to keep the current default behavior without prompting.</span><br><span class="line"></span><br><span class="line">TOPIC                          PARTITION  NEW-OFFSET     </span><br><span class="line"><span class="built_in">test</span>                           0          0              </span><br><span class="line"><span class="comment"># 发现并没有生效，</span></span><br><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --describe --group <span class="built_in">test</span>-group</span><br><span class="line">Consumer group <span class="string">'test-group'</span> has no active members.</span><br><span class="line"></span><br><span class="line">TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID</span><br><span class="line"><span class="built_in">test</span>            0          5               5               0               -               -               -</span><br><span class="line"><span class="comment"># 根据上面的提示，需要加上--execute参数来执行：</span></span><br><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --reset-offsets --group <span class="built_in">test</span>-group --topic <span class="built_in">test</span> --to-earliest --execute</span><br><span class="line"></span><br><span class="line">TOPIC                          PARTITION  NEW-OFFSET     </span><br><span class="line"><span class="built_in">test</span>                           0          0              </span><br><span class="line"></span><br><span class="line"><span class="comment"># CURRENT-OFFSET此时为0了。</span></span><br><span class="line">$ ./bin/kafka-consumer-groups.sh --bootstrap-server kafka01:9092 --describe --group <span class="built_in">test</span>-group</span><br><span class="line">Consumer group <span class="string">'test-group'</span> has no active members.</span><br><span class="line"></span><br><span class="line">TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID</span><br><span class="line"><span class="built_in">test</span>            0          0               5               5               -               -               -</span><br></pre></td></tr></table></figure>



<h3 id="扩展集群"><a href="#扩展集群" class="headerlink" title="扩展集群"></a>扩展集群</h3><p>将服务器添加到Kafka集群很容易，只需为其分配唯一的代理ID，然后在新服务器上启动Kafka。但是，不会为这些新服务器自动分配任何数据分区，因此，除非将分区移至它们，否则在创建新主题之前它们将不会做任何工作。因此，通常在将计算机添加到群集时，您将需要将一些现有数据迁移到这些计算机。</p>
<p>数据迁移过程是手动启动的，但是是完全自动化的。在幕后，Kafka会将新服务器添加为要迁移的分区的跟随者，并允许它完全复制该分区中的现有数据。新服务器完全复制该分区的内容并加入同步副本后，现有副本之一将删除其分区的数据。</p>
<p>分区重新分配工具可用于在代理之间移动分区。理想的分区分布将确保所有代理之间的数据负载和分区大小均匀。分区重新分配工具没有能力自动研究Kafka群集中的数据分布，并四处移动分区以实现均匀的负载分布。因此，管理员必须弄清楚应该移动哪些主题或分区。</p>
<p>分区重新分配工具可以在3种互斥模式下运行：</p>
<ul>
<li>–generate：在此模式下，给定主题列表和代理列表，该工具会生成候选重新分配，以将指定主题的所有分区移至新的代理。给定主题和目标代理的列表，此选项仅提供一种方便的方法来生成分区重新分配计划。</li>
<li>–execute：在此模式下，该工具将根据用户提供的重新分配计划启动分区的重新分配。（使用–reassignment-json-file选项）。这可以是管理员手工制作的自定义重新分配计划，也可以使用–generate选项提供</li>
<li>–verify：在此模式下，该工具会验证上一次–execute期间列出的所有分区的重新分配状态。状态可以是成功完成，失败或进行中</li>
</ul>
<h4 id="自动将数据迁移到新计算机"><a href="#自动将数据迁移到新计算机" class="headerlink" title="自动将数据迁移到新计算机"></a>自动将数据迁移到新计算机</h4><p>分区重新分配工具可用于将某些主题从当前代理集移到新添加的代理。这在扩展现有集群时通常很有用，因为与一次移动一个分区相比，将整个主题移至新的一组代理更容易。用于执行此操作时，用户应提供应移至新的一组代理的主题列表和新代理的目标列表。然后，该工具将给定主题列表中的所有分区平均分配到新的一组代理中。在此过程中，主题的复制因子保持不变。实际上，主题输入列表的所有分区的副本都从旧的代理集移到了新添加的代理。</p>
<p>例如，以下示例将主题foo1，foo2的所有分区移动到新的代理集5,6。在此步骤结束时，主题foo1和foo2的所有分区<em>仅存</em>在于代理5,6上。</p>
<p>由于该工具将主题的输入列表作为json文件接受，因此您首先需要确定要移动的主题并按以下方式创建json文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; cat topics-to-move.json</span><br><span class="line">&#123;&quot;topics&quot;: [&#123;&quot;topic&quot;: &quot;foo1&quot;&#125;,</span><br><span class="line">            &#123;&quot;topic&quot;: &quot;foo2&quot;&#125;],</span><br><span class="line">&quot;version&quot;:1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>kafka-reassign-partitions.sh</code> 命令帮助：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-reassign-partitions.sh</span><br><span class="line">This <span class="built_in">command</span> moves topic partitions between replicas.</span><br><span class="line">Option                                 Description                           </span><br><span class="line">------                                 -----------                           </span><br><span class="line">--bootstrap-server &lt;String: Server(s)  the server(s) to use <span class="keyword">for</span>              </span><br><span class="line">  to use <span class="keyword">for</span> bootstrapping&gt;              bootstrapping. REQUIRED <span class="keyword">if</span> an       </span><br><span class="line">                                         absolution path of the <span class="built_in">log</span> directory</span><br><span class="line">                                         is specified <span class="keyword">for</span> any replica <span class="keyword">in</span> the </span><br><span class="line">                                         reassignment json file              </span><br><span class="line">--broker-list &lt;String: brokerlist&gt;     The list of brokers to <span class="built_in">which</span> the      </span><br><span class="line">                                         partitions need to be reassigned <span class="keyword">in</span> </span><br><span class="line">                                         the form <span class="string">"0,1,2"</span>. This is required  </span><br><span class="line">                                         <span class="keyword">if</span> --topics-to-move-json-file is    </span><br><span class="line">                                         used to generate reassignment       </span><br><span class="line">                                         configuration                       </span><br><span class="line">--<span class="built_in">disable</span>-rack-aware                   Disable rack aware replica assignment </span><br><span class="line">--execute                              Kick off the reassignment as specified</span><br><span class="line">                                         by the --reassignment-json-file     </span><br><span class="line">                                         option.                             </span><br><span class="line">--generate                             Generate a candidate partition        </span><br><span class="line">                                         reassignment configuration. Note    </span><br><span class="line">                                         that this only generates a candidate</span><br><span class="line">                                         assignment, it does not execute it. </span><br><span class="line">--reassignment-json-file &lt;String:      The JSON file with the partition      </span><br><span class="line">  manual assignment json file path&gt;      reassignment configurationThe format</span><br><span class="line">                                         to use is -                         </span><br><span class="line">                                       &#123;<span class="string">"partitions"</span>:                        </span><br><span class="line">                                       	[&#123;<span class="string">"topic"</span>: <span class="string">"foo"</span>,                    </span><br><span class="line">                                       	  <span class="string">"partition"</span>: 1,                    </span><br><span class="line">                                       	  <span class="string">"replicas"</span>: [1,2,3],               </span><br><span class="line">                                       	  <span class="string">"log_dirs"</span>: [<span class="string">"dir1"</span>,<span class="string">"dir2"</span>,<span class="string">"dir3"</span>] </span><br><span class="line">                                         &#125;],                                 </span><br><span class="line">                                       <span class="string">"version"</span>:1                           </span><br><span class="line">                                       &#125;                                     </span><br><span class="line">                                       Note that <span class="string">"log_dirs"</span> is optional. When</span><br><span class="line">                                         it is specified, its length must    </span><br><span class="line">                                         equal the length of the replicas    </span><br><span class="line">                                         list. The value <span class="keyword">in</span> this list can be </span><br><span class="line">                                         either <span class="string">"any"</span> or the absolution path </span><br><span class="line">                                         of the <span class="built_in">log</span> directory on the broker. </span><br><span class="line">                                         If absolute <span class="built_in">log</span> directory path is   </span><br><span class="line">                                         specified, it is currently required </span><br><span class="line">                                         that the replica has not already    </span><br><span class="line">                                         been created on that broker. The    </span><br><span class="line">                                         replica will <span class="keyword">then</span> be created <span class="keyword">in</span> the </span><br><span class="line">                                         specified <span class="built_in">log</span> directory on the      </span><br><span class="line">                                         broker later.                       </span><br><span class="line">--replica-alter-log-dirs-throttle      The movement of replicas between <span class="built_in">log</span>  </span><br><span class="line">  &lt;Long: replicaAlterLogDirsThrottle&gt;    directories on the same broker will </span><br><span class="line">                                         be throttled to this value          </span><br><span class="line">                                         (bytes/sec). Rerunning with this    </span><br><span class="line">                                         option, whilst a rebalance is <span class="keyword">in</span>    </span><br><span class="line">                                         progress, will alter the throttle   </span><br><span class="line">                                         value. The throttle rate should be  </span><br><span class="line">                                         at least 1 KB/s. (default: -1)      </span><br><span class="line">--throttle &lt;Long: throttle&gt;            The movement of partitions between    </span><br><span class="line">                                         brokers will be throttled to this   </span><br><span class="line">                                         value (bytes/sec). Rerunning with   </span><br><span class="line">                                         this option, whilst a rebalance is  </span><br><span class="line">                                         <span class="keyword">in</span> progress, will alter the throttle</span><br><span class="line">                                         value. The throttle rate should be  </span><br><span class="line">                                         at least 1 KB/s. (default: -1)      </span><br><span class="line">--timeout &lt;Long: timeout&gt;              The maximum time <span class="keyword">in</span> ms allowed to <span class="built_in">wait</span></span><br><span class="line">                                         <span class="keyword">for</span> partition reassignment execution</span><br><span class="line">                                         to be successfully initiated        </span><br><span class="line">                                         (default: 10000)                    </span><br><span class="line">--topics-to-move-json-file &lt;String:    Generate a reassignment configuration </span><br><span class="line">  topics to reassign json file path&gt;     to move the partitions of the       </span><br><span class="line">                                         specified topics to the list of     </span><br><span class="line">                                         brokers specified by the --broker-  </span><br><span class="line">                                         list option. The format to use is - </span><br><span class="line">                                       &#123;<span class="string">"topics"</span>:                            </span><br><span class="line">                                       	[&#123;<span class="string">"topic"</span>: <span class="string">"foo"</span>&#125;,&#123;<span class="string">"topic"</span>: <span class="string">"foo1"</span>&#125;],</span><br><span class="line">                                       <span class="string">"version"</span>:1                           </span><br><span class="line">                                       &#125;                                     </span><br><span class="line">--verify                               Verify <span class="keyword">if</span> the reassignment completed  </span><br><span class="line">                                         as specified by the --reassignment- </span><br><span class="line">                                         json-file option. If there is a     </span><br><span class="line">                                         throttle engaged <span class="keyword">for</span> the replicas   </span><br><span class="line">                                         specified, and the rebalance has    </span><br><span class="line">                                         completed, the throttle will be     </span><br><span class="line">                                         removed                             </span><br><span class="line">--zookeeper &lt;String: urls&gt;             REQUIRED: The connection string <span class="keyword">for</span>   </span><br><span class="line">                                         the zookeeper connection <span class="keyword">in</span> the form</span><br><span class="line">                                         host:port. Multiple URLS can be     </span><br><span class="line">                                         given to allow fail-over.</span><br></pre></td></tr></table></figure>



<p>JSON文件准备好后，请使用分区重新分配工具生成候选分配：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file topics-to-move.json --broker-list &quot;5,6&quot; --generate</span><br><span class="line">Current partition replica assignment</span><br><span class="line"> </span><br><span class="line">&#123;&quot;version&quot;:1,</span><br><span class="line">&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,4]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,4]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line"> </span><br><span class="line">&#123;&quot;version&quot;:1,</span><br><span class="line">&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[5,6]&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该工具生成候选分配，该分配会将所有分区从主题foo1，foo2移至代理5,6。但是请注意，此时分区移动尚未开始，它仅告诉您当前分配和建议的新分配。如果您想回滚到当前分配，则应将其保存。新的赋值应保存在json文件（例如，expand-cluster-reassignment.json）中，然后使用–execute选项输入到工具中，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"> </span><br><span class="line">&#123;&quot;version&quot;:1,</span><br><span class="line">&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,4]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,4]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions</span><br><span class="line">&#123;&quot;version&quot;:1,</span><br><span class="line">&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[5,6]&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>最后，–verify选项可与该工具一起使用，以检查分区重新分配的状态。请注意，应将相同的expand-cluster-reassignment.json（与–execute选项一起使用）与–verify选项一起使用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --verify</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition [foo1,0] completed successfully</span><br><span class="line">Reassignment of partition [foo1,1] is in progress</span><br><span class="line">Reassignment of partition [foo1,2] is in progress</span><br><span class="line">Reassignment of partition [foo2,0] completed successfully</span><br><span class="line">Reassignment of partition [foo2,1] completed successfully</span><br><span class="line">Reassignment of partition [foo2,2] completed successfully</span><br></pre></td></tr></table></figure>

<h4 id="自定义分区分配和迁移"><a href="#自定义分区分配和迁移" class="headerlink" title="自定义分区分配和迁移"></a>自定义分区分配和迁移</h4><p>分区重新分配工具还可以用于将分区的副本选择性地移动到一组特定的代理。以这种方式使用时，假定用户知道重新分配计划，并且不需要工具生成候选重新分配，从而有效地跳过了–generate步骤并直接进入–execute步骤</p>
<p>例如，以下示例将主题foo1的分区0移动到代理5,6，将主题foo2的分区1移动到代理2,3：</p>
<p>第一步是在json文件中手工制作自定义重新分配计划：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; cat custom-reassignment.json</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]&#125;,&#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;]&#125;</span><br></pre></td></tr></table></figure>

<p>然后，将json文件与–execute选项一起使用以开始重新分配过程：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"> </span><br><span class="line">&#123;&quot;version&quot;:1,</span><br><span class="line">&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[1,2]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[3,4]&#125;]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions</span><br><span class="line">&#123;&quot;version&quot;:1,</span><br><span class="line">&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">              &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>–verify选项可与该工具一起使用，以检查分区重新分配的状态。请注意，应将相同的expand-cluster-reassignment.json（与–execute选项一起使用）与–verify选项一起使用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --verify</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition [foo1,0] completed successfully</span><br><span class="line">Reassignment of partition [foo2,1] completed successfully</span><br></pre></td></tr></table></figure>



<h3 id="复制因子增加"><a href="#复制因子增加" class="headerlink" title="复制因子增加"></a>复制因子增加</h3><p>增加现有分区的复制因子很容易。只需在自定义重新分配json文件中指定额外的副本，然后将其与–execute选项一起使用即可增加指定分区的复制因子。</p>
<p>例如，以下示例将主题foo的分区0的复制因子从1增加到3。在增加复制因子之前，该分区的唯一副本存在于代理5上。作为增加复制因子的一部分，我们将在经纪人6和7。</p>
<p>第一步是在json文件中手工制作自定义重新分配计划：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&gt; cat increase-replication-factor.json</span><br><span class="line">&#123;<span class="attr">"version"</span>:<span class="number">1</span>,</span><br><span class="line"><span class="attr">"partitions"</span>:[&#123;<span class="attr">"topic"</span>:<span class="string">"foo"</span>,<span class="attr">"partition"</span>:<span class="number">0</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]&#125;]&#125;</span><br></pre></td></tr></table></figure>

<p>然后，将json文件与–execute选项一起使用以开始重新分配过程：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"> </span><br><span class="line">&#123;<span class="attr">"version"</span>:<span class="number">1</span>,</span><br><span class="line"><span class="attr">"partitions"</span>:[&#123;<span class="attr">"topic"</span>:<span class="string">"foo"</span>,<span class="attr">"partition"</span>:<span class="number">0</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>]&#125;]&#125;</span><br><span class="line"> </span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions</span><br><span class="line">&#123;<span class="attr">"version"</span>:<span class="number">1</span>,</span><br><span class="line"><span class="attr">"partitions"</span>:[&#123;<span class="attr">"topic"</span>:<span class="string">"foo"</span>,<span class="attr">"partition"</span>:<span class="number">0</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]&#125;]&#125;</span><br></pre></td></tr></table></figure>

<p>–verify选项可与该工具一起使用，以检查分区重新分配的状态。请注意，应将–verify选项与同一crement-replication-factor.json（与–execute选项一起使用）一起使用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --verify</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition [foo,0] completed successfully</span><br></pre></td></tr></table></figure>

<p>您还可以使用kafka-topics工具验证复制因子的增加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic foo --describe</span><br><span class="line">Topic:foo   PartitionCount:1    ReplicationFactor:3 Configs:</span><br><span class="line">  Topic: foo    Partition: 0    Leader: 5   Replicas: 5,6,7 Isr: 5,6,7</span><br></pre></td></tr></table></figure>



<p>我们已之前创建的test Topic为例：</p>
<p>这里的replicationFactor是1，也就是没有副本的。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-topics.sh --zookeeper zk01:2181 --describe --topic <span class="built_in">test</span></span><br><span class="line">Topic:<span class="built_in">test</span>	PartitionCount:1	ReplicationFactor:1	Configs:</span><br><span class="line">4Topic: <span class="built_in">test</span>	Partition: 0	Leader: 1	Replicas: 1	Isr: 1</span><br></pre></td></tr></table></figure>

<p>创建json文件：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">$ cat jsonfiles/increase-replication-factor.json </span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">"partitions"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"partition"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"replicas"</span>: [</span><br><span class="line">                <span class="number">0</span>,</span><br><span class="line">                <span class="number">1</span>,</span><br><span class="line">                <span class="number">2</span></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"topic"</span>: <span class="string">"test"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"version"</span>: <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>应用：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-reassign-partitions.sh --zookeeper zk01:2181 --reassignment-json-file jsonfiles/increase-replication-factor.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">"version"</span>:1,<span class="string">"partitions"</span>:[&#123;<span class="string">"topic"</span>:<span class="string">"test"</span>,<span class="string">"partition"</span>:0,<span class="string">"replicas"</span>:[1],<span class="string">"log_dirs"</span>:[<span class="string">"any"</span>]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br></pre></td></tr></table></figure>

<p>检查一下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ./bin/kafka-topics.sh --zookeeper zk01:2181 --describe --topic <span class="built_in">test</span></span><br><span class="line">Topic:<span class="built_in">test</span>	PartitionCount:1	ReplicationFactor:3	Configs:</span><br><span class="line">4Topic: <span class="built_in">test</span>	Partition: 0	Leader: 1	Replicas: 0,1,2	Isr: 1,0,2</span><br></pre></td></tr></table></figure>



<h3 id="限制数据迁移期间的带宽使用"><a href="#限制数据迁移期间的带宽使用" class="headerlink" title="限制数据迁移期间的带宽使用"></a>限制数据迁移期间的带宽使用</h3><p>Kafka允许您对复制流量应用限制，在用于将副本在计算机之间移动的带宽上设置上限。当重新平衡群集，引导新的代理或添加或删除代理时，这很有用，因为它限制了这些数据密集型操作对用户的影响。</p>
<p>有两个接口可用于接合油门。最简单，最安全的方法是在调用kafka-reassign-partitions.sh时应用油门，但kafka-configs.sh也可用于直接查看和更改油门值。</p>
<p>因此，例如，如果您要执行重新平衡，则使用以下命令，它将以不超过50MB/s的速度移动分区。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file bigger-cluster.json —throttle 50000000</span><br></pre></td></tr></table></figure>

<p>当您执行此脚本时，您将看到油门启动：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The throttle limit was set to 50000000 B/s</span><br><span class="line">Successfully started reassignment of partitions.</span><br></pre></td></tr></table></figure>

<p>如果您希望在重新平衡期间更改油门，比如说要增加吞吐量以使其更快地完成，则可以通过重新运行传递相同的reassignment-json-file的execute命令来执行此操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ bin/kafka-reassign-partitions.sh --zookeeper localhost:2181  --execute --reassignment-json-file bigger-cluster.json --throttle 700000000</span><br><span class="line">  There is an existing assignment running.</span><br><span class="line">  The throttle limit was set to 700000000 B/s</span><br></pre></td></tr></table></figure>

<p>重新平衡完成后，管理员可以使用–verify选项检查重新平衡的状态。如果重新平衡完成，则将通过–verify命令删除油门。重要的是，一旦重新平衡完成，管理员可以通过使用–verify选项运行命令来及时删除限制。否则，可能会限制常规复制流量。</p>
<p>当执行–verify选项并完成重新分配后，脚本将确认已取消节流阀：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181  --verify --reassignment-json-file bigger-cluster.json</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition [my-topic,1] completed successfully</span><br><span class="line">Reassignment of partition [mytopic,0] completed successfully</span><br><span class="line">Throttle was removed.</span><br></pre></td></tr></table></figure>

<p>管理员还可以使用kafka-configs.sh验证分配的配置。有两对节气门配置，用于管理节流过程。油门值本身。这是使用动态属性在代理级别配置的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">leader.replication.throttled.rate</span><br><span class="line">  follower.replication.throttled.rate</span><br></pre></td></tr></table></figure>

<p>还有一组枚举的受限制的副本：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">leader.replication.throttled.replicas</span><br><span class="line">  follower.replication.throttled.replicas</span><br></pre></td></tr></table></figure>

<p>每个主题都配置了哪些。所有四个配置值都由kafka-reassign-partitions.sh自动分配（在下面讨论）。</p>
<p>要查看油门极限配置：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-configs.sh --describe --zookeeper localhost:2181 --entity-type brokers</span><br><span class="line">Configs <span class="keyword">for</span> brokers <span class="string">'2'</span> are leader.replication.throttled.rate=700000000,follower.replication.throttled.rate=700000000</span><br><span class="line">Configs <span class="keyword">for</span> brokers <span class="string">'1'</span> are leader.replication.throttled.rate=700000000,follower.replication.throttled.rate=700000000</span><br></pre></td></tr></table></figure>

<p>这显示了应用于复制协议的引导方和跟随方的限制。默认情况下，双方都分配有相同的限制吞吐量值。</p>
<p>要查看限制副本的列表：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-configs.sh --describe --zookeeper localhost:2181 --entity-type topics</span><br><span class="line">Configs <span class="keyword">for</span> topic <span class="string">'my-topic'</span> are leader.replication.throttled.replicas=1:102,0:101,</span><br><span class="line">    follower.replication.throttled.replicas=1:101,0:102</span><br></pre></td></tr></table></figure>

<p>在这里，我们看到引导者限制应用于代理102上的分区1和代理101的分区0。同样，跟随者限制应用于代理101上的分区1和代理102上的分区0。</p>
<p>默认情况下，kafka-reassign-partitions.sh会将引导者限制应用于重新平衡之前存在的所有副本，其中任何一个都可能是领导者。它将从动油门应用于所有移动目的地。因此，如果在代理程序101,102上存在一个具有副本的分区，并将其重新分配给102,103，则该分区的前导调节器将应用于101,102，而后继调节器将仅应用于103。</p>
<p>如果需要，还可以使用kafka-configs.sh上的–alter开关手动更改油门配置。</p>
<h3 id="Kafka-python"><a href="#Kafka-python" class="headerlink" title="Kafka python"></a>Kafka python</h3><h4 id="Kafka-python-3-5k※"><a href="#Kafka-python-3-5k※" class="headerlink" title="Kafka-python 3.5k※"></a>Kafka-python 3.5k※</h4><p>GitHub：</p>
<p><a href="https://github.com/dpkp/kafka-python" target="_blank" rel="noopener">https://github.com/dpkp/kafka-python</a></p>
<p>Docs：</p>
<p><a href="https://kafka-python.readthedocs.io/en/master/index.html" target="_blank" rel="noopener">https://kafka-python.readthedocs.io/en/master/index.html</a></p>
<h4 id="confluent-kafka-python-1-6k※"><a href="#confluent-kafka-python-1-6k※" class="headerlink" title="confluent-kafka-python 1.6k※"></a>confluent-kafka-python 1.6k※</h4><p>GitHub：</p>
<p><a href="https://github.com/confluentinc/confluent-kafka-python" target="_blank" rel="noopener">https://github.com/confluentinc/confluent-kafka-python</a></p>
<p>Docs：</p>
<p><a href="http://docs.confluent.io/current/clients/index.html" target="_blank" rel="noopener">http://docs.confluent.io/current/clients/index.html</a></p>
<h4 id="pykafka-983※"><a href="#pykafka-983※" class="headerlink" title="pykafka 983※"></a>pykafka 983※</h4><p>GitHub:</p>
<p><a href="https://github.com/Parsely/pykafka" target="_blank" rel="noopener">https://github.com/Parsely/pykafka</a></p>
<p>Docs:</p>
<p><a href="http://pykafka.readthedocs.org/" target="_blank" rel="noopener">http://pykafka.readthedocs.org/</a></p>
<h3 id="Kafka生态"><a href="#Kafka生态" class="headerlink" title="Kafka生态"></a>Kafka生态</h3><blockquote>
<p>参考：</p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem</a></p>
</blockquote>
<blockquote>
<p>本文到这里就结束了，欢迎期待后面的文章。您可以关注下方的公众号二维码，在第一时间查看新文章。</p>
<p><img src="https://imgs.knner.wang/images/knner/wx.jpg" alt="公众号"></p>
</blockquote>

    </div>

    
    
    
      
        <div class="reward-container">
  <div>如有疏忽错误欢迎在留言区评论指正，如果对您有所帮助欢迎点击下方进行打赏。</div>
  <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/knner/wechatpay.jpg" alt="Knner.Wang 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/knner/alipay.jpg" alt="Knner.Wang 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Knner.Wang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://knner.wang/2019/12/05/install-and-config-zookeeper-and-kafka-cluster.html" title="安装配置Zookeeper和Kafka集群">https://knner.wang/2019/12/05/install-and-config-zookeeper-and-kafka-cluster.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2019/12/02/install-nacos-cluster-on-kubernetes.html" rel="next" title="在Kubernetes上安装Nacos集群">
                  <i class="fa fa-chevron-left"></i> 在Kubernetes上安装Nacos集群
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2019/12/15/adding-filebeat-and-logstash-to-collect-and-processing-logs-in-kubernetes.html" rel="prev" title="使用filebeat + kafka + logstash收集处理kubernetes日志，配置收集处理nginx-ingress json日志">
                  使用filebeat + kafka + logstash收集处理kubernetes日志，配置收集处理nginx-ingress json日志 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
  <div class="comments" id="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Zookeeper："><span class="nav-number">1.</span> <span class="nav-text">Zookeeper：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装："><span class="nav-number">1.1.</span> <span class="nav-text">安装：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#下载"><span class="nav-number">1.1.1.</span> <span class="nav-text">下载</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#目录结构"><span class="nav-number">1.1.2.</span> <span class="nav-text">目录结构</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Zookeeper-配置详解"><span class="nav-number">1.2.</span> <span class="nav-text">Zookeeper 配置详解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#最低配置："><span class="nav-number">1.2.1.</span> <span class="nav-text">最低配置：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#进阶配置："><span class="nav-number">1.2.2.</span> <span class="nav-text">进阶配置：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#集群选项配置"><span class="nav-number">1.2.3.</span> <span class="nav-text">集群选项配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#加密，身份验证，授权选项"><span class="nav-number">1.2.4.</span> <span class="nav-text">加密，身份验证，授权选项</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#性能调整选项"><span class="nav-number">1.2.5.</span> <span class="nav-text">性能调整选项</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AdminServer配置"><span class="nav-number">1.2.6.</span> <span class="nav-text">AdminServer配置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Zookeeper-集群配置"><span class="nav-number">1.3.</span> <span class="nav-text">Zookeeper 集群配置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#zk01-配置"><span class="nav-number">1.3.1.</span> <span class="nav-text">zk01 配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#zk02-配置"><span class="nav-number">1.3.2.</span> <span class="nav-text">zk02 配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#zk03-配置"><span class="nav-number">1.3.3.</span> <span class="nav-text">zk03 配置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#启动"><span class="nav-number">1.4.</span> <span class="nav-text">启动</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#集群可用性测试"><span class="nav-number">1.4.1.</span> <span class="nav-text">集群可用性测试</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#监管Zookeeper进程"><span class="nav-number">1.4.2.</span> <span class="nav-text">监管Zookeeper进程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ZooKeeper命令"><span class="nav-number">1.5.</span> <span class="nav-text">ZooKeeper命令</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#四小写字母命令"><span class="nav-number">1.5.1.</span> <span class="nav-text">四小写字母命令</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#四小写字母命令测试"><span class="nav-number">1.5.2.</span> <span class="nav-text">四小写字母命令测试</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AdminServer"><span class="nav-number">1.5.3.</span> <span class="nav-text">AdminServer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AdminServer-测试"><span class="nav-number">1.5.4.</span> <span class="nav-text">AdminServer 测试</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Troubleshooting-故障排查"><span class="nav-number">1.6.</span> <span class="nav-text">Troubleshooting 故障排查</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#恢复-TxnLogToolkit-恢复带有损坏CRC的事务日志条目"><span class="nav-number">1.6.1.</span> <span class="nav-text">恢复-TxnLogToolkit 恢复带有损坏CRC的事务日志条目</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ZK-datadir-datalogdir-目录结构"><span class="nav-number">1.7.</span> <span class="nav-text">ZK datadir datalogdir 目录结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zkCli-使用测试"><span class="nav-number">1.8.</span> <span class="nav-text">zkCli 使用测试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka："><span class="nav-number">2.</span> <span class="nav-text">Kafka：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#介绍"><span class="nav-number">2.1.</span> <span class="nav-text">介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#主题和日志-Topics-and-Logs"><span class="nav-number">2.1.1.</span> <span class="nav-text">主题和日志 Topics and Logs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分布"><span class="nav-number">2.1.2.</span> <span class="nav-text">分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#地址复制"><span class="nav-number">2.1.3.</span> <span class="nav-text">地址复制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#生产者-Producers"><span class="nav-number">2.1.4.</span> <span class="nav-text">生产者 Producers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#消费者-Consumers"><span class="nav-number">2.1.5.</span> <span class="nav-text">消费者 Consumers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#消息复制与分区"><span class="nav-number">2.1.6.</span> <span class="nav-text">消息复制与分区</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#名词解释"><span class="nav-number">2.1.7.</span> <span class="nav-text">名词解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#保证"><span class="nav-number">2.1.8.</span> <span class="nav-text">保证</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka作为消息传递系统"><span class="nav-number">2.1.9.</span> <span class="nav-text">Kafka作为消息传递系统</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka用于流处理"><span class="nav-number">2.1.10.</span> <span class="nav-text">Kafka用于流处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#拼凑在一起"><span class="nav-number">2.1.11.</span> <span class="nav-text">拼凑在一起</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装"><span class="nav-number">2.2.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-配置详解"><span class="nav-number">2.3.</span> <span class="nav-text">Kafka 配置详解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka01-配置："><span class="nav-number">2.3.1.</span> <span class="nav-text">Kafka01 配置：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka02-配置："><span class="nav-number">2.3.2.</span> <span class="nav-text">Kafka02 配置：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka03-配置："><span class="nav-number">2.3.3.</span> <span class="nav-text">Kafka03 配置：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#启动-1"><span class="nav-number">2.4.</span> <span class="nav-text">启动</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#修改堆内存"><span class="nav-number">2.4.1.</span> <span class="nav-text">修改堆内存</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优雅关机"><span class="nav-number">2.5.</span> <span class="nav-text">优雅关机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Topic的增删改查"><span class="nav-number">2.6.</span> <span class="nav-text">Topic的增删改查</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Topic-的创建"><span class="nav-number">2.6.1.</span> <span class="nav-text">Topic 的创建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Topic-的查看"><span class="nav-number">2.6.2.</span> <span class="nav-text">Topic 的查看</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Topic-的创建，手动指定Partitions和Broker的对应关系："><span class="nav-number">2.6.3.</span> <span class="nav-text">Topic 的创建，手动指定Partitions和Broker的对应关系：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Topic-的修改"><span class="nav-number">2.6.4.</span> <span class="nav-text">Topic 的修改</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Topic-的删除"><span class="nav-number">2.6.5.</span> <span class="nav-text">Topic 的删除</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Topic-的配置的增删改查"><span class="nav-number">2.7.</span> <span class="nav-text">Topic 的配置的增删改查</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#列出Topic的配置："><span class="nav-number">2.7.1.</span> <span class="nav-text">列出Topic的配置：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#增加Topic的配置："><span class="nav-number">2.7.2.</span> <span class="nav-text">增加Topic的配置：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#修改Topic的现有配置"><span class="nav-number">2.7.3.</span> <span class="nav-text">修改Topic的现有配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#删除Topic配置"><span class="nav-number">2.7.4.</span> <span class="nav-text">删除Topic配置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-的生产者和消费者测试"><span class="nav-number">2.8.</span> <span class="nav-text">Kafka 的生产者和消费者测试</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#检查消费者位置以及分区偏移"><span class="nav-number">2.8.1.</span> <span class="nav-text">检查消费者位置以及分区偏移</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#重置分区偏移"><span class="nav-number">2.8.2.</span> <span class="nav-text">重置分区偏移</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#扩展集群"><span class="nav-number">2.9.</span> <span class="nav-text">扩展集群</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#自动将数据迁移到新计算机"><span class="nav-number">2.9.1.</span> <span class="nav-text">自动将数据迁移到新计算机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#自定义分区分配和迁移"><span class="nav-number">2.9.2.</span> <span class="nav-text">自定义分区分配和迁移</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#复制因子增加"><span class="nav-number">2.10.</span> <span class="nav-text">复制因子增加</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#限制数据迁移期间的带宽使用"><span class="nav-number">2.11.</span> <span class="nav-text">限制数据迁移期间的带宽使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-python"><span class="nav-number">2.12.</span> <span class="nav-text">Kafka python</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka-python-3-5k※"><span class="nav-number">2.12.1.</span> <span class="nav-text">Kafka-python 3.5k※</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#confluent-kafka-python-1-6k※"><span class="nav-number">2.12.2.</span> <span class="nav-text">confluent-kafka-python 1.6k※</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pykafka-983※"><span class="nav-number">2.12.3.</span> <span class="nav-text">pykafka 983※</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka生态"><span class="nav-number">2.13.</span> <span class="nav-text">Kafka生态</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="Knner.Wang"
    src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Knner.Wang</p>
  <div class="site-description" itemprop="description">Knner.Wang's Blog,for sharing IT technology and also my study notes.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">25</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/wanghkkk" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;wanghkkk" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://hub.docker.com/u/wanghkkk" title="DockerHub &amp;rarr; https:&#x2F;&#x2F;hub.docker.com&#x2F;u&#x2F;wanghkkk" rel="noopener" target="_blank"><i class="fa fa-fw fa-docker"></i>DockerHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wanghk@live.cn" title="E-Mail &amp;rarr; mailto:wanghk@live.cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">京ICP备19058691号-1 </a>
      <img src="/images/knner/ga-icon.png" style="display: inline-block;"><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010802030660" rel="noopener" target="_blank">11010802030660 </a>
  </div>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Knner.Wang 版权所有</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
    和<a href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral" target="_blank" rel="noopener"><img width="60" style="display: inline;" src="/images/logos/upyun_logo5.png"></a>提供CDN加速/云存储服务
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0
  </div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>



  











<script>
if (document.querySelectorAll('div.pdf').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js', () => {
    document.querySelectorAll('div.pdf').forEach(element => {
      PDFObject.embed(element.getAttribute('target'), element, {
        pdfOpenParams: {
          navpanes: 0,
          toolbar: 0,
          statusbar: 0,
          pagemode: 'thumbs',
          view: 'FitH'
        },
        PDFJS_URL: '/lib/pdf/web/viewer.html',
        height: element.getAttribute('height') || '500px'
      });
    });
  }, window.PDFObject);
}
</script>


<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme: 'forest',
      logLevel: 3,
      flowchart: { curve: 'linear' },
      gantt: { axisFormat: '%m/%d/%Y' },
      sequence: { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>



  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://knner.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  function loadComments() {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: {page: {
            url: "https://knner.wang/2019/12/05/install-and-config-zookeeper-and-kafka-cluster.html",
            identifier: "2019/12/05/install-and-config-zookeeper-and-kafka-cluster.html",
            title: "安装配置Zookeeper和Kafka集群"
          }
        }
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://knner.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  }
    (function() {
      var offsetTop = document.getElementById('comments').offsetTop - window.innerHeight;
      if (offsetTop <= 0) {
        // load directly when there's no a scrollbar
        window.addEventListener('load', loadComments, false);
      } else {
        var disqus_scroll = () => {
          // offsetTop may changes because of manually resizing browser window or lazy loading images.
          var offsetTop = document.getElementById('comments').offsetTop - window.innerHeight;
          var scrollTop = window.scrollY;

          // pre-load comments a bit? (margin or anything else)
          if (offsetTop - scrollTop < 60) {
            window.removeEventListener('scroll', disqus_scroll);
            loadComments();
          }
        };
        window.addEventListener('scroll', disqus_scroll);
      }
    })();
  
</script>

</body>
</html>
